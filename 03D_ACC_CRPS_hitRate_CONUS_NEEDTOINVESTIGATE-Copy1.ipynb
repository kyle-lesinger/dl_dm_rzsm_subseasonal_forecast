{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7302f239-5aba-4164-8125-86df134c9bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/klesinger/conda-envs/tf212gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from numpy import meshgrid\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter, LatitudeLocator\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, TwoSlopeNorm\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "from ridgeplot import ridgeplot\n",
    "import joypy\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "import climpred\n",
    "from xclim import sdba\n",
    "from climpred.options import OPTIONS\n",
    "import json\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from matplotlib.lines import Line2D  # For custom legend entries\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import matplotlib.gridspec as gridspec\n",
    "import hydroeval as he\n",
    "\n",
    "from function import preprocessUtils as putils\n",
    "from function import masks\n",
    "from function import verifications\n",
    "from function import funs as f\n",
    "from function import conf\n",
    "from function import loadbias\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "global dim_order, region_name, test_year, leads_\n",
    "dim_order = conf.dim_order\n",
    "region_name = 'CONUS'\n",
    "test_year = 2019\n",
    "leads_ = [6,13,20,27]\n",
    "\n",
    "dir = '/glade/work/klesinger/FD_RZSM_deep_learning'\n",
    "assert test_year == 2019, 'This is only the script for when the testing years are 2018-2019. Test year must = 2019.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f21f8ca-ca67-4f9d-ac41-7018e915bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "global final_experiment\n",
    "final_experiment = 'EX29'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49b3c50b-ae23-42d5-83d7-fb6d15d55902",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = masks.load_mask(region_name)\n",
    "#Mask with np.nan for non-CONUS land values\n",
    "try:\n",
    "    mask_anom = mask[putils.xarray_varname(mask)][0,:,:].values\n",
    "except IndexError:\n",
    "    mask_anom = mask[putils.xarray_varname(mask)][:,:].values\n",
    "\n",
    "if region_name == 'CONUS':\n",
    "    region_mask = masks.load_region_mask(region_name).rename({'latitude':'lat','longitude':'lon'}).isel(time=0)\n",
    "    CONUS_region_names = {1:'Northeast',2:'Southeast',3:'Midwest',4:'Great Plains',5:'Northwest',6:'Southwest'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f38e61-2f06-4ffc-baf6-999766e5d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global custom_names\n",
    "custom_names = {\n",
    "    'GEFSv12': 'NWP (2)', 'EMOS': 'DM-BC_EMOS (4)', \n",
    "    'DM-BC_DL': 'DM-BC-DL (2)', 'DL': 'DL (16)',\n",
    "    'DL-DM': 'DL-DM (20)', 'ML_NWP_OBS': 'ML_NWP_OBS (2)',\n",
    "    'ECMWF':'NWP (2)', 'NWP_BC': 'NWP-BC (2)',\n",
    "}\n",
    "\n",
    "        \n",
    "def return_name(name):\n",
    "    if 'XGBOOST' in name:\n",
    "        name_out = 'ML_NWP_OBS'\n",
    "    else:\n",
    "        name_out = name\n",
    "    custom_names = {name: name_out}\n",
    "\n",
    "    return(custom_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc46ab7f-c07d-4455-a5f4-608adcf2b6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading soil data all the baseline files for observations, GEFSv12, and ECMWF for region CONUS\n",
      "Loading GLEAM observations\n",
      "Loading GEFS data\n",
      "Loading ECMWF data\n"
     ]
    }
   ],
   "source": [
    "'''Testing and validation dates only for the year 2019'''\n",
    "\n",
    "test_start = '2018-01-01'\n",
    "test_end = '2019-12-31'\n",
    "val_start = '2015-01-01'\n",
    "val_end = '2017-12-31'\n",
    "train_start  = '2000-01-01'\n",
    "\n",
    "\n",
    "'''Test subsets of obs, ecmwf raw, gefsv12 raw'''\n",
    "global obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only\n",
    "\n",
    "obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only = verifications.open_obs_and_baseline_files_multiple_leads(region_name, leads_, test_start, test_end, mask_anom)\n",
    "\n",
    "#Open the gleam percentile/anom files\n",
    "init_dates = putils.get_init_date_list(f'{conf.gefsv12_data}/{region_name}/soilw_bgrnd')\n",
    "dt_dates = [pd.to_datetime(i) for i in init_dates]\n",
    "\n",
    "only_testing_dates = [i for i in dt_dates if i >= pd.to_datetime('2018-01-01')]\n",
    "obs_anom_percentile = xr.open_mfdataset(f'{conf.gleam_data}/{region_name}/RZSM_anom_and_percentile_reformat/*').sel(S=slice(test_start,test_end))\n",
    "obs_anom_percentile['S'] = only_testing_dates\n",
    "obs_anom_percentile\n",
    "\n",
    "''' Load EMOS results (only for CONUS). We have not completed EMOS on any other region except CONUS'''\n",
    "if region_name == 'CONUS':\n",
    "    emos_ = xr.open_dataset(f'Data/EMOS/{region_name}/EMOS_11_test_predictions_12_weeks_before.nc')\n",
    "    emos_ = emos_.rename({'idate':'S', 'model': 'M','vdate': 'L', 'latitude': 'Y', 'longitude': 'X'})\n",
    "    emos_testing = emos_.sel(S=slice(test_start,test_end))\n",
    "\n",
    "test_dates_subx = only_testing_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617588b2-6c1d-433c-9b34-431ef9efe672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Experiment info for plotting\n",
    "# black = ['EX0','EX13'] # bias-correction\n",
    "# red = ['EX14','EX15','EX16','EX17','EX22','EX23','EX24','EX25'] # obs.-driven\n",
    "# blue = ['EX1','EX2','EX3','EX4','EX5','EX6','EX7','EX8','EX9','EX10','EX11','EX12',\n",
    "#        'EX18','EX19','EX20','EX21','EX27','EX28'] # hybrid\n",
    "\n",
    "global obs_original,obs_raw\n",
    "obs_original = xr.open_dataset(f'{conf.gleam_data}/{region_name}/RZSM_anomaly.nc').rename({'SMsurf':'RZSM'}).drop('season').load()\n",
    "obs_raw = xr.open_dataset(f'{conf.gleam_data}/{region_name}/RZSM_weighted_mean_0_100cm.nc4').rename({'SMsurf':'RZSM'}).load()\n",
    "\n",
    "init_dates = putils.get_init_date_list(f'{conf.gefsv12_data}/{region_name}/soilw_bgrnd')\n",
    "dt_dates = [pd.to_datetime(i) for i in init_dates]\n",
    "\n",
    "#Load previously created data (percentiles of the anomaly)\n",
    "global emcwf_perc, gefs_perc\n",
    "# ecmwf_perc = verifications.load_ECMWF_percentile_anomaly(region_name).sel(S=slice(test_start,test_end)).load()\n",
    "# gefs_perc = verifications.load_GEFSv12_percentile_anomaly(region_name).sel(S=slice(test_start,test_end)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee07c13-dc61-428a-8755-8f5f6c09da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now bias correct and then take the anomaly\n",
    "    \n",
    "climpred.set_options(seasonality=\"season\") \n",
    "seasonality_str = OPTIONS[\"seasonality\"]\n",
    "\n",
    "def return_seasonal_mean(file,train_end):\n",
    "    climatology_season = file.sel(init=(file['init.year'] <= train_end)).groupby(f\"init.{seasonality_str}\").mean()\n",
    "    return(climatology_season)\n",
    "\n",
    "def compute_anomaly_on_testing(full_file, train_end, test_file):\n",
    "    climatology_season = return_seasonal_mean(full_file,train_end)\n",
    "    climatology_season = climatology_season.rename({'member':'M','lead':'L','lat':'Y','lon':'X'})\n",
    "    \n",
    "    summer_= test_file.sel(S=(test_file['S.season']=='JJA')) - climatology_season.sel(season='JJA')\n",
    "    fall_= test_file.sel(S=(test_file['S.season']=='SON'))- climatology_season.sel(season='SON')\n",
    "    winter_= test_file.sel(S=(test_file['S.season']=='DJF'))- climatology_season.sel(season='DJF')\n",
    "    spring_= test_file.sel(S=(test_file['S.season']=='MAM'))- climatology_season.sel(season='MAM')\n",
    "\n",
    "    combined_files = xr.concat([summer_,fall_,winter_,spring_],dim='S').sortby('S')\n",
    "    combined_files = combined_files.drop('season')\n",
    "    return(combined_files)\n",
    "\n",
    "def expand_dims_by_lead(file):\n",
    "    return(file.expand_dims({'L':1}).transpose(*dim_order))\n",
    "\n",
    "def select_data_by_lead(obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only, day_num):\n",
    "    obs_lead = expand_dims_by_lead(obs_anomaly_SubX_format.sel(L=day_num))\n",
    "    baseline_gefs =  expand_dims_by_lead(baseline_anomaly.sel(L=day_num))\n",
    "    baseline_ecm =  expand_dims_by_lead(baseline_ecmwf.sel(L=day_num))\n",
    "    var_OUT = var_OUT\n",
    "    template =  expand_dims_by_lead(template_testing_only.sel(L=day_num))\n",
    "    return(obs_lead,baseline_gefs,baseline_ecm,var_OUT,template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a20f0-8be7-48d9-9ad8-f6ea300f6ff8",
   "metadata": {},
   "source": [
    "# Lineplot ACC and CRPS all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698a07f-8836-4613-bd8f-465faae51593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_lineplot_to_dataframe(df,fcst_vals,name_of_fcst, metric,week):\n",
    "    # df = pd.DataFrame()\n",
    "    def return_color(name_of_fcst):\n",
    "        black = ['EX0','EX13'] # bias-corrected DL\n",
    "        red = ['EX14','EX15','EX16','EX17','EX22','EX23','EX24','EX25'] #Observation driven\n",
    "        blue = ['EX1','EX2','EX3','EX4','EX5','EX6','EX7','EX8','EX9','EX10','EX11','EX12',\n",
    "               'EX18','EX19','EX20','EX21','EX27','EX28','EX29'] #Hybrid\n",
    "\n",
    "        black2 = ['DM-BC_DL']\n",
    "        red2 = ['DL']\n",
    "        blue2 = ['DL-DM']\n",
    "        \n",
    "        green = ['ECMWF','GEFSv12']\n",
    "        \n",
    "        purple = 'NWP_BC'\n",
    "\n",
    "        yellow = 'EMOS'\n",
    "\n",
    "        if (name_of_fcst in black) or (name_of_fcst in black2):\n",
    "            color = 'black'\n",
    "        elif (name_of_fcst in red) or (name_of_fcst in red2):\n",
    "            color = 'red'\n",
    "        elif (name_of_fcst in blue) or (name_of_fcst in blue2):\n",
    "            color = 'blue'\n",
    "        elif name_of_fcst in green:\n",
    "            color = 'green'\n",
    "        elif purple in name_of_fcst:\n",
    "            color = 'purple'\n",
    "        elif yellow in name_of_fcst:\n",
    "            color = 'yellow'\n",
    "        return(color)\n",
    "\n",
    "    if week==10:\n",
    "        for idx,lead in enumerate([6,13,20,27]):\n",
    "            try:\n",
    "                data = fcst_vals.sel(lead=lead).mean()[putils.xarray_varname(fcst_vals)].values\n",
    "            except KeyError:\n",
    "                data = fcst_vals.mean()[putils.xarray_varname(fcst_vals)].values\n",
    "            dict_ = {'Forecast':[name_of_fcst], 'Week':[idx+1], f'{metric}': [data], 'Color':return_color(name_of_fcst)}\n",
    "            df = pd.concat([df,pd.DataFrame.from_dict(dict_)])\n",
    "    else:\n",
    "        data = fcst_vals.mean()[putils.xarray_varname(fcst_vals)].values\n",
    "        dict_ = {'Forecast':[name_of_fcst], 'Week':[week], f'{metric}': [data], 'Color':return_color(name_of_fcst)}\n",
    "        df = pd.concat([df,pd.DataFrame.from_dict(dict_)])\n",
    "\n",
    "    return(df)\n",
    "\n",
    "\n",
    "'''Only the single value for all the experiments'''\n",
    "\n",
    "def add_lineplot_to_dataframe_average(df,fcst_vals,name_of_fcst, metric,week):\n",
    "    # df = pd.DataFrame()\n",
    "\n",
    "    def return_color(name_of_fcst):\n",
    "        black = ['DM-BC_DL']\n",
    "        red = ['DL']\n",
    "        blue = ['DL-DM']\n",
    "        \n",
    "        green = ['ECMWF','GEFSv12']\n",
    "        \n",
    "        purple = 'XGBOOST'\n",
    "\n",
    "        yellow = 'EMOS'\n",
    "\n",
    "        if name_of_fcst in black:\n",
    "            color = 'black'\n",
    "        elif name_of_fcst in red:\n",
    "            color = 'red'\n",
    "        elif name_of_fcst in blue:\n",
    "            color = 'blue'\n",
    "        elif name_of_fcst in green:\n",
    "            color = 'green'\n",
    "        elif purple in name_of_fcst:\n",
    "            color = 'purple'\n",
    "        elif yellow in name_of_fcst:\n",
    "            color = 'yellow'\n",
    "            \n",
    "        return(color)\n",
    "\n",
    "    if week==10:\n",
    "        for idx,lead in enumerate([6,13,20,27]):\n",
    "            \n",
    "            data = fcst_vals.sel(lead=lead).mean()[putils.xarray_varname(fcst_vals)].values\n",
    "            dict_ = {'Forecast':[name_of_fcst], 'Week':[idx+1], f'{metric}': [data], 'Color':return_color(name_of_fcst)}\n",
    "            df = pd.concat([df,pd.DataFrame.from_dict(dict_)])\n",
    "    else:\n",
    "        data = fcst_vals.mean()[putils.xarray_varname(fcst_vals)].values\n",
    "        dict_ = {'Forecast':[name_of_fcst], 'Week':[week], f'{metric}': [data], 'Color':return_color(name_of_fcst)}\n",
    "        df = pd.concat([df,pd.DataFrame.from_dict(dict_)])\n",
    "    \n",
    "    return(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6fa9e0-a53c-4795-b982-17ed6d4ee491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_UNET_experiments(correct_experiments):\n",
    "    only_RZSM = [j for j in correct_experiments if 'RZSM' in j] \n",
    "    only_ensemble= [j for j in only_RZSM if 'final' not in j]\n",
    "    only_ensemble = [j for j in only_ensemble if 'Residual' not in j]\n",
    "    only_2019 = [j for j in only_ensemble if '2012' not in j]\n",
    "    return(only_2019)\n",
    "\n",
    "def common_UNET_no_regular_experiments(correct_experiments):\n",
    "    only_RZSM = [j for j in correct_experiments if 'RZSM' in j] \n",
    "    only_ensemble= [j for j in only_RZSM if 'final' not in j]\n",
    "    only_ensemble = [j for j in only_ensemble if 'Residual' not in j]\n",
    "    only_ensemble = [j for j in only_ensemble if 'regular' not in j]\n",
    "    only_2019 = [j for j in only_ensemble if '2012' not in j]\n",
    "    return(only_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f77e0-35eb-46ef-ac02-54abfb09bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_EMOS_average(region_name,test_start,test_end,obs_original):\n",
    "    print('Adding EMOS results')\n",
    "    '''This is for adding all 4 EMOS experiments, but for some ready it is breaking right now and I'm not sure why (it kills the kernel)'''\n",
    "    emos_files_full = sorted(glob(f'Data/EMOS/{region_name}/EMOS_11*test_predictions*.nc'))\n",
    "    \n",
    "    # Loop through all the EMOS file experiments\n",
    "    e_acc, e_crps = ecmwf_acc.copy(deep=True), ecmwf_acc.copy(deep=True)\n",
    "\n",
    "    e_acc[putils.xarray_varname(e_acc)][:,:,:],  e_crps[putils.xarray_varname(e_crps)][:,:,:] = 0, 0\n",
    "\n",
    "    # Loop through all the EMOS file experiments\n",
    "    for idx,file in enumerate(emos_files_full):\n",
    "        # break\n",
    "        emos_ = xr.open_dataset(file).rename({'idate':'S', 'model': 'M','vdate': 'L', 'latitude': 'Y', 'longitude': 'X'}).sel(S=slice(test_start,test_end))\n",
    "        #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "        emos_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "        add_  = (e_acc[putils.xarray_varname(e_acc)].values + emos_acc[putils.xarray_varname(emos_acc)].sel(lead=[6,13,20,27]).values)\n",
    "        e_acc[putils.xarray_varname(e_acc)][:,:,:] = add_\n",
    "\n",
    "        emos_crps = verifications.create_climpred_CRPSS(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "        emos_crps = emos_crps.mean(dim='init')\n",
    "        add_  = (e_crps[putils.xarray_varname(e_crps)].values + emos_crps[putils.xarray_varname(emos_crps)].sel(lead=[6,13,20,27]).values)\n",
    "        e_crps[putils.xarray_varname(e_crps)][:,:,:] = add_\n",
    "\n",
    "    #Divide\n",
    "    e_acc = e_acc /len(emos_files_full)\n",
    "    e_crps = e_crps /len(emos_files_full)\n",
    "    #RE ADD MASK\n",
    "    mm = ~np.isnan(emos_.isel(M=0,S=0).sel(L=[6,13,20,27]))[putils.xarray_varname(emos_)].values\n",
    "    e_acc = xr.where(mm==True,e_acc,np.nan)\n",
    "    e_crps = xr.where(mm==True,e_crps,np.nan)\n",
    "    \n",
    "    df_acc = add_lineplot_to_dataframe(df_acc,e_acc,'EMOS', 'ACC',10)\n",
    "    df_crps = add_lineplot_to_dataframe(df_crps,e_crps,'EMOS', 'CRPSS',10)\n",
    "    return df_acc, df_crps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c33b4f-fb6c-4a34-9686-c2258a4bc6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t\n",
    "def lineplot_ACC_CRPSS_climpred_average_by_type(region_name, test_start, test_end, leads=[6,13,20,27]):\n",
    "    '''This script will take an average of each type of model configuration'''\n",
    "\n",
    "    print(f'Calculating ACC and CRPS on GEFS and ECMWF')\n",
    "    obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=leads), baseline_anomaly.sel(L=leads), baseline_ecmwf.sel(L=leads)\n",
    "\n",
    "    gefs_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "    ecmwf_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    gefs_crps = verifications.create_climpred_CRPSS(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "    ecmwf_crps = verifications.create_climpred_CRPSS(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    df_acc = pd.DataFrame()\n",
    "    df_crps = pd.DataFrame()\n",
    "\n",
    "    diff_nwp = (gefs_acc + ecmwf_acc)/2\n",
    "    df_acc = add_lineplot_to_dataframe(df_acc,diff_nwp,'GEFSv12', 'ACC',10) #Just keep the name the same for later\n",
    "\n",
    "    diff_nwp = (gefs_crps + ecmwf_crps)/2\n",
    "    df_crps = add_lineplot_to_dataframe(df_crps,diff_nwp,'GEFSv12', 'CRPSS',10) #Just keep the name the same for later\n",
    "    \n",
    "    #Bias corrected data ACC\n",
    "    print('Loading additive bias correction results')\n",
    "    gef_BC, ecm_BC = loadbias.load_additive_bias_corrected_data_ACC(leads,region_name)\n",
    "    diff_nwp_BC = (gef_BC + ecm_BC)/2\n",
    "    df_acc = add_lineplot_to_dataframe(df=df_acc,fcst_vals=diff_nwp_BC,name_of_fcst='NWP_BC', metric='ACC',week=10) #Just keep the name the same for later   \n",
    "\n",
    "    #bias corrected data CRPSS\n",
    "    print('Loading additive bias correction results')\n",
    "    gef_BC, ecm_BC = loadbias.load_additive_bias_corrected_data_CRPSS(leads,region_name)\n",
    "    gef_BC = gef_BC.crps.values\n",
    "    ecm_BC = ecm_BC.crps.values\n",
    "    diff_nwp_BC = (gef_BC + ecm_BC)/2\n",
    "    #Just replace the values\n",
    "    gefs_crps.crps[:,:,:,:] = diff_nwp_BC\n",
    "    df_crps = add_lineplot_to_dataframe(df=df_crps,fcst_vals=gefs_crps,name_of_fcst='NWP_BC', metric='CRPSS',week=10) #Just keep the name the same for later   \n",
    "\n",
    "\n",
    "    \n",
    "    #########################################   EMOS   ################################################################################################################\n",
    "    df_acc, df_crps = return_EMOS_average(region_name,test_start,test_end,obs_original)\n",
    "    ####################################################################################################################################\n",
    "    # x_acc = ecmwf_acc.copy(deep=True)\n",
    "    # x_crps = ecmwf_acc.copy(deep=True)\n",
    "    \n",
    "    # x_acc[putils.xarray_varname(x_acc)][:,:,:] = 0\n",
    "    # x_crps[putils.xarray_varname(x_crps)][:,:,:] = 0\n",
    "    \n",
    "    # #Load the XGBoost data\n",
    "    # for lead in [1,2,3,4]:\n",
    "    #     day_num = (lead*7) -1\n",
    "    #     print('Working on XGBoost experiments')\n",
    "    #     xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{lead}_testing/*'))\n",
    "    #     len_xg = len(xgboost_files)\n",
    "    #     add_to_file_OG = ecmwf_acc.sel(lead=day_num).copy(deep=True)\n",
    "    #     add_to_file_OG.acc[:,:] = 0\n",
    "    #     #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    #     for i in xgboost_files:\n",
    "    #         # break\n",
    "    #         test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "    #         add_to_file = verifications.load_XGBoost_file_and_make_ensemble_spread(gefs=gefs, file=i, source_of_XGBoost_reforecast='GEFSv12',day_num=day_num, region_name=region_name, test_year=test_year)\n",
    "    #         xg_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "    #         add_  = (x_acc[putils.xarray_varname(x_acc)].values + xg_acc[putils.xarray_varname(xg_acc)].sel(lead=[day_num]).values)\n",
    "            \n",
    "    #         xg_crps = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "    #         e_crps[putils.xarray_varname(e_crps)][:,:,:]  =\n",
    "\n",
    "    #     add_to_file_OG = add_to_file_OG /len_xg\n",
    "    #     #RE ADD MASK\n",
    "    #     add_to_file_OG = xr.where(~np.isnan(ecmwf_acc.sel(lead=day_num)),add_to_file_OG,np.nan)\n",
    "    #     df = add_lineplot_to_dataframe(df,add_to_file_OG,'XGBOOST', 'ACC',lead)\n",
    "    ####################################################################################################################################\n",
    "    black = ['EX0','EX13']\n",
    "    red = ['EX14','EX15','EX16','EX17','EX22','EX23','EX24','EX25']\n",
    "    blue = ['EX1','EX2','EX3','EX4','EX5','EX6','EX7','EX8','EX9','EX10','EX11','EX12',\n",
    "           'EX18','EX19','EX20','EX21','EX27','EX28','EX29']\n",
    "\n",
    "    obs_forecast_boolean = np.isnan(obs)\n",
    "    \n",
    "    #Add to a new gefs template for masking\n",
    "    obs_forecast_boolean2 = gefs.copy(deep=True)\n",
    "    obs_forecast_boolean2.RZSM[:,:,:,:,:] = obs_forecast_boolean['var']\n",
    "    \n",
    "    for idLead,week_ in enumerate([1,2,3,4]):\n",
    "        # break\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_}_testing/*')) #With a specific subset of data\n",
    "        day_num, keep_files = (week_*7) -1, []\n",
    "        ex = [f'EX{num}' for num in range(30)]\n",
    "        for num in ex:\n",
    "            if num not in keep_files:\n",
    "                correct_experiments = [j for j in unet_files if num in j]\n",
    "                if len(correct_experiments) != 0:\n",
    "                    '''This if statement helps to avoid experiments that may have been moved or deleted from the scratch space'''\n",
    "                    only_2019 = common_UNET_no_regular_experiments(correct_experiments)\n",
    "                    for i in only_2019:\n",
    "                        keep_files.append(i) \n",
    "        \n",
    "        bias_correction_black = [file for file in keep_files if any(black_item in file for black_item in black)]\n",
    "        hybrid_blue = [file for file in keep_files if any(black_item in file for black_item in blue)]\n",
    "        obs_red = [file for file in keep_files if any(black_item in file for black_item in red)]\n",
    "\n",
    "        \n",
    "        #Now loop through and open file, convert to anomaly and compute ACC score #Can add machine learning xgboost\n",
    "        for model,name in zip([bias_correction_black, hybrid_blue, obs_red], ['DM-BC_DL', 'DL-DM', 'DL']):\n",
    "            # break\n",
    "            print(f'Working on UNET experiments {name} WEEK {week_}.')\n",
    "            u_acc = ecmwf_acc.sel(lead=day_num).copy(deep=True)\n",
    "            u_crps = ecmwf_acc.sel(lead=day_num).copy(deep=True)\n",
    "            u_acc[putils.xarray_varname(u_acc)][:,:] = 0\n",
    "            u_crps[putils.xarray_varname(u_crps)][:,:] = 0\n",
    "    \n",
    "            # break\n",
    "            len_unet = len(model)\n",
    "            for i in model:\n",
    "                # break\n",
    "                new_source = 'ECMWF' if 'ECMWF' in i else 'GEFSv12'\n",
    "\n",
    "                test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "                ex_name = test_name.split('_')[0]\n",
    "                \n",
    "                add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "                #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "                \n",
    "                #Mask values where it should be 0 for ocean and water bodies\n",
    "                add_to_file = add_to_file.where(obs_forecast_boolean2==0,np.nan)\n",
    "    \n",
    "                unet_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original)).sel(lead=day_num)\n",
    "                unet_crps = verifications.create_climpred_CRPSS(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original)).sel(lead=day_num).mean(dim='init')\n",
    "\n",
    "                '''Add each experiment to the blank file'''\n",
    "                u_acc[putils.xarray_varname(u_acc)][:,:]  = u_acc[putils.xarray_varname(u_acc)][:,:].values + unet_acc[putils.xarray_varname(unet_acc)].values\n",
    "                u_crps[putils.xarray_varname(u_crps)][:,:]  = u_crps[putils.xarray_varname(u_crps)][:,:].values + unet_crps[putils.xarray_varname(unet_crps)].values\n",
    "        \n",
    "            u_acc = u_acc /len_unet\n",
    "            u_crps = u_crps /len_unet\n",
    "            #RE ADD MASK\n",
    "            u_acc = xr.where(~np.isnan(ecmwf_acc[putils.xarray_varname(ecmwf_acc)].sel(lead=day_num)),u_acc,np.nan)\n",
    "            u_crps = xr.where(~np.isnan(ecmwf_acc[putils.xarray_varname(ecmwf_acc)].sel(lead=day_num)),u_crps,np.nan)\n",
    "            \n",
    "            df_acc = add_lineplot_to_dataframe_average(df_acc,u_acc,name, 'ACC',week_)\n",
    "            df_crps = add_lineplot_to_dataframe_average(df_crps,u_crps,name, 'CRPSS',week_)\n",
    "    \n",
    "\n",
    "    return(df_acc,df_crps)\n",
    "    \n",
    "\n",
    "\n",
    "'''Call Function for LINE Plots across all leads 1-4 and experiments for the ACC values'''\n",
    "df_ACC,df_CRPS = lineplot_ACC_CRPSS_climpred_average_by_type(region_name, test_start, test_end,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf02ecd-54f2-46ff-850d-a995a8e84d74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_ACC_CRPS_LinePlot(df_ACC, df_CRPS):\n",
    "    # Ensure the 'Week' column is sorted to make the line plot correctly\n",
    "    # df_ACC.sort_values(by=['Forecast', 'Week'], inplace=True)\n",
    "    # df_CRPS.sort_values(by=['Forecast', 'Week'], inplace=True)\n",
    "    print('Plotting')\n",
    "    save_dir = f'Outputs/ACC_CRPS_line_plots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    #Plot\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15, 6),dpi=300)\n",
    "    plt.style.use('seaborn-v0_8-colorblind')\n",
    "    palette = plt.get_cmap('tab10')\n",
    "\n",
    "    for (idx,df),metric in zip(enumerate([df_ACC,df_CRPS]),['ACC','CRPSS']):\n",
    "        # Group by the 'Forecast' column\n",
    "        # break\n",
    "        grouped = df.groupby('Forecast')\n",
    "        color_tracker = {}\n",
    "        marker_style = ['o', 'v', '^', '<', '>', 's', 'p', '*', '+']\n",
    "        \n",
    "        for i, (name, group) in enumerate(grouped):\n",
    "            color = palette(i)\n",
    "            marker = marker_style[i % len(marker_style)]\n",
    "            \n",
    "            if color not in color_tracker:\n",
    "                axs[idx].plot(group['Week'], group[metric], label=custom_names[name], color=color, marker=marker, linestyle='-', markersize=6)\n",
    "                color_tracker[color] = custom_names[name]\n",
    "            else:\n",
    "                axs[idx].plot(group['Week'], group[metric], color=color, marker=marker, linestyle='-', markersize=6)\n",
    "                \n",
    "        # Add a horizontal line at y=0.5\n",
    "        if idx == 0:\n",
    "            axs[idx].axhline(y=0.5, color='gray', linestyle='--', linewidth=2)\n",
    "        \n",
    "        # Setting the title and labels with increased font sizes\n",
    "        # plt.title(metric, fontsize=30)\n",
    "        axs[idx].set_xlabel('Week Lead', fontsize=14)\n",
    "        axs[idx].set_ylabel(metric, fontsize=22, labelpad=10)\n",
    "\n",
    "        axs[idx].set_xticks([1,2,3,4])\n",
    "        axs[idx].tick_params(axis='both', which='major', labelsize=12)\n",
    "        \n",
    "        # Add a text box with the letter \"A\" in the upper left corner\n",
    "        if idx == 0:\n",
    "            axs[idx].text(-1.14, 1.1, \"A\", transform=fig.gca().transAxes, fontsize=25, fontweight='bold', va='top', ha='right')\n",
    "        else:\n",
    "            axs[idx].text(0, 1.1, \"B\", transform=fig.gca().transAxes, fontsize=25, fontweight='bold', va='top', ha='left')\n",
    "        \n",
    "        axs[idx].grid(True, which='both', linestyle='--', linewidth=0.5) # Add a grid for better readability\n",
    "        if idx == 0:\n",
    "            axs[idx].legend(title='Forecast (# models)', fontsize=12, title_fontsize=13) # Create the legend with a slightly larger font size\n",
    "            \n",
    "    plt.tight_layout() # Optionally, set a tight layout to ensure everything fits without overlap\n",
    "    plt.savefig(f'{save_dir}/Lineplot_ACC_CRPS_averaged_by_experiments.png', dpi=300)\n",
    "    return(0)\n",
    "\n",
    "\n",
    "plot_ACC_CRPS_LinePlot(df_ACC,df_CRPS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2beb52-dd23-46a4-b0aa-3a9f2c34f741",
   "metadata": {},
   "source": [
    "# Spatial ACC with CONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e20677a-ee03-42a6-8fac-46a1519ed23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_ACC_climpred_with_XGBOOST(week_lead, region_name, test_start, test_end, single_experiment_or_all_experiments):\n",
    "\n",
    "    save_dir = f'Outputs/ACC_spatial_plots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    plot_dict = {}\n",
    "\n",
    "    day_num = (week_lead*7) -1\n",
    "\n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_anomaly.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_ecmwf.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order)\n",
    "\n",
    "    print('Calculating ACC on GEFS and ECMWF')\n",
    "    gefs_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "    # gefs_BC_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(additive_bias_gefs_final.sel(L=day_num).expand_dims({'L':1})), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    ecmwf_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "    # ecmwf_BC_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(additive_bias_ecmwf_final.sel(L=day_num).expand_dims({'L':1})), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    plot_dict.update({'ECMWF':ecmwf_acc})\n",
    "    plot_dict.update({'GEFSv12':gefs_acc})\n",
    "    \n",
    "    ####################################################################################################################################\n",
    "    \n",
    "    #Load the XGBoost data\n",
    "    print('Working on XGBoost experiments')\n",
    "    xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{week_lead}_testing/*EX28*'))\n",
    "\n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in xgboost_files:\n",
    "        # break\n",
    "        test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "        add_to_file = verifications.load_XGBoost_file_and_make_ensemble_spread(gefs=gefs, file=i, source_of_XGBoost_reforecast='GEFSv12',day_num=day_num, region_name=region_name, test_year=test_year)\n",
    "        \n",
    "        xg_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "        plot_dict.update({test_name:xg_acc})\n",
    "\n",
    "\n",
    "    if single_experiment_or_all_experiments == 'all':\n",
    "        #Now for all predictions from UNET, make the ACC\n",
    "        #Can add *denseLar* for only the dense ones\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*')) #With a specific subset of data\n",
    "    else:\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{single_experiment_or_all_experiments}*')) #Will all data\n",
    "\n",
    "    #Get the ECMWF file\n",
    "    ec_unet = [i for i in unet_files if 'ECMWF' in i][0]\n",
    "    gef_unet = [i for i in unet_files if f'{single_experiment_or_all_experiments}_regular_RZSM' in i][0]\n",
    "    \n",
    "    print('Working on UNET experiments')\n",
    "    unet_files = [ec_unet,gef_unet]\n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in unet_files:\n",
    "                        \n",
    "        if 'ECMWF' in i:\n",
    "            new_source = 'ECMWF'\n",
    "        else:\n",
    "            new_source = 'GEFSv12'\n",
    "\n",
    "        test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "        \n",
    "        # break\n",
    "        add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "\n",
    "        unet_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "        plot_dict.update({test_name:unet_acc})\n",
    "    \n",
    "\n",
    "        \n",
    "    print('Adding EMOS results')\n",
    "    #Now add EMOS results\n",
    "    if region_name == 'CONUS':\n",
    "        #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "        emos_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "        emos_acc = emos_acc.sel(lead=day_num)\n",
    "    \n",
    "    plot_dict.update({'EMOS':emos_acc})\n",
    "    \n",
    "    #Get global max and min\n",
    "    global_max, global_min = verifications.global_max_min(plot_dict,'acc')\n",
    "\n",
    "    \n",
    "    cmap = 'bwr'\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows = 2, ncols= 3, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(10, 7), dpi=300)\n",
    "    \n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    lon = mask.X.values\n",
    "    lat = mask.Y.values\n",
    "    \n",
    "    axs_start = 0\n",
    "    for model in plot_dict.keys():\n",
    "        # break\n",
    "        data = plot_dict[model].acc.values\n",
    "        v = np.linspace(global_min, global_max, 20, endpoint=True)\n",
    "\n",
    "        map = Basemap(projection='cyl', llcrnrlat=lat[-1] - 1.5, urcrnrlat=lat[0],\n",
    "                      llcrnrlon=(lon[0] - 360 - 6), urcrnrlon=(lon[-1] - 360 + 20), resolution='l')\n",
    "        \n",
    "        x, y = map(*np.meshgrid(lon, lat))\n",
    "        # Adjust the text coordinates based on the actual data coordinates\n",
    "\n",
    "        im = axs[axs_start].contourf(x, y, data, levels=v, extend='both',\n",
    "                              transform=ccrs.PlateCarree(), cmap=cmap)\n",
    "        \n",
    "        gl = axs[axs_start].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                   linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "        gl.top_labels = False\n",
    "        gl.right_lables = False\n",
    "        gl.left_labels = True\n",
    "        gl.xformatter = LongitudeFormatter()\n",
    "        gl.yformatter = LatitudeFormatter()\n",
    "        axs[axs_start].coastlines()\n",
    "        axs[axs_start].set_aspect('equal')  # this makes the plots better\n",
    "        if model == f'{single_experiment_or_all_experiments}_ECMWF_regular_RZSM':\n",
    "            axs[axs_start].set_title(f'EXP{single_experiment_or_all_experiments[2:]}_E',fontsize=11)\n",
    "        elif model == f'{single_experiment_or_all_experiments}_regular_RZSM':\n",
    "            axs[axs_start].set_title(f'EXP{single_experiment_or_all_experiments[2:]}_G', fontsize=11)\n",
    "        else:\n",
    "            axs[axs_start].set_title(f'{model}',fontsize=11)\n",
    "            \n",
    "        axs_start+=1\n",
    "\n",
    "                            #(left, bottom, width, height)\n",
    "    cbar_ax = fig.add_axes([0.05, 0.05, .9, .04])\n",
    "    \n",
    "    # Draw the colorbar\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    # plt.suptitle(f'ACC on testing Dataset', fontsize=30)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    plt.savefig(f'{save_dir}/Wk{week_lead}_ACC_climpred.png')\n",
    "\n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d292fe50-6311-4d1d-b8ff-191e1e7948df",
   "metadata": {},
   "source": [
    "## Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb6c598-da54-4551-9ffa-5d5cb97d157c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# '''We only ran EMOS, XGBoost on CONUS'''\n",
    "# for week_lead in [1,2,3,4,5]:\n",
    "#     if region_name == 'CONUS':\n",
    "#         spatial_ACC_climpred_with_XGBOOST(week_lead, region_name, test_start, test_end, single_experiment_or_all_experiments=final_experiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aadfff2-f7d3-4272-9fb2-fb481d4f36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_ACC_climpred_with_XGBOOST_multiple_experiments(week_lead, region_name, test_start, test_end, ex1, ex2):\n",
    "\n",
    "    # week_lead = 3\n",
    "    # ex1='EX29'\n",
    "    # ex2 = 'EX24'\n",
    "    \n",
    "    save_dir = f'Outputs/ACC_spatial_plots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    plot_dict = {}\n",
    "\n",
    "    day_num = (week_lead*7) -1\n",
    "\n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_anomaly.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_ecmwf.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order)\n",
    "\n",
    "    print('Calculating ACC on GEFS and ECMWF')\n",
    "    gefs_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "    # gefs_BC_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(additive_bias_gefs_final.sel(L=day_num).expand_dims({'L':1})), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    ecmwf_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "    # ecmwf_BC_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(additive_bias_ecmwf_final.sel(L=day_num).expand_dims({'L':1})), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    plot_dict.update({'ECMWF':ecmwf_acc})\n",
    "    plot_dict.update({'GEFSv12':gefs_acc})\n",
    "    \n",
    "    ####################################################################################################################################\n",
    "    \n",
    "    # #Load the XGBoost data\n",
    "    # print('Working on XGBoost experiments')\n",
    "    # xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{week_lead}_testing/*EX28*'))\n",
    "    # #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    # for i in xgboost_files:\n",
    "    #     # break\n",
    "    #     test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "    #     add_to_file = verifications.load_XGBoost_file_and_make_ensemble_spread(gefs=gefs, file=i, source_of_XGBoost_reforecast='GEFSv12',day_num=day_num, region_name=region_name, test_year=test_year)\n",
    "        \n",
    "    #     xg_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    #     plot_dict.update({test_name:xg_acc})\n",
    "\n",
    "\n",
    "    if ex1 == 'all':\n",
    "        #Now for all predictions from UNET, make the ACC\n",
    "        #Can add *denseLar* for only the dense ones\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*')) #With a specific subset of data\n",
    "    else:\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{ex1}*')) #Will all data\n",
    "\n",
    "    #Get the ECMWF file\n",
    "    ec_unet = [i for i in unet_files if 'ECMWF' in i][0]\n",
    "    gef_unet = [i for i in unet_files if f'{ex1}_regular_RZSM' in i][0]\n",
    "    \n",
    "    print('Working on UNET experiments')\n",
    "    unet_files = [ec_unet,gef_unet]\n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in unet_files:\n",
    "                        \n",
    "        if 'ECMWF' in i:\n",
    "            new_source = 'ECMWF'\n",
    "        else:\n",
    "            new_source = 'GEFSv12'\n",
    "\n",
    "        test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "        \n",
    "        # break\n",
    "        add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "\n",
    "        unet_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "        plot_dict.update({test_name:unet_acc})\n",
    "    \n",
    "\n",
    "    #Now add the 2nd UNET experiment\n",
    "    unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{ex2}*')) #Will all data\n",
    "    gef_unet = [i for i in unet_files if f'{ex2}_regular_RZSM' in i][0]\n",
    "\n",
    "    for i in [gef_unet]:    \n",
    "        if 'ECMWF' in i:\n",
    "            new_source = 'ECMWF'\n",
    "        else:\n",
    "            new_source = 'GEFSv12'\n",
    "\n",
    "        test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "        \n",
    "        # break\n",
    "        add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "\n",
    "        unet_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "        plot_dict.update({test_name:unet_acc})\n",
    "    \n",
    "    # print('Adding EMOS results')\n",
    "    # #Now add EMOS results\n",
    "    # if region_name == 'CONUS':\n",
    "    #     #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "    #     emos_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "    #     emos_acc = emos_acc.sel(lead=day_num)\n",
    "    \n",
    "    # plot_dict.update({'EMOS':emos_acc})\n",
    "    \n",
    "    #Get global max and min\n",
    "    \n",
    "    global_max, global_min = verifications.global_max_min(plot_dict,'acc')\n",
    "    v = np.linspace(global_min, global_max, 20, endpoint=True)\n",
    "    v = [i for i in v if i <0] + [0] + [i for i in v if i >0] \n",
    "    norm = TwoSlopeNorm(0, vmin=v[0], vmax=v[-1])\n",
    "    \n",
    "    cmap = 'bwr'\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows = 3, ncols= 2, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(10, 8), dpi=300)\n",
    "    \n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    lon = mask.X.values\n",
    "    lat = mask.Y.values\n",
    "\n",
    "    plot_dictionary = {} #This will save the statistics for each model to see where the ACC skill is greater than a specific threshold\n",
    "    total_grid_cells = np.count_nonzero(mask_anom)\n",
    "    \n",
    "    axs_start = 0\n",
    "    for model in plot_dict.keys():\n",
    "        plot_dictionary[model] = {}\n",
    "        # break\n",
    "        data = plot_dict[model].acc.values\n",
    "\n",
    "        for threshold in [0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "            abv_thresh = np.where(data>threshold,1,0)\n",
    "            plot_dictionary[model][threshold] = np.count_nonzero(abv_thresh) / total_grid_cells\n",
    "            \n",
    "        \n",
    "\n",
    "        map = Basemap(projection='cyl', llcrnrlat=lat[-1] - 1.5, urcrnrlat=lat[0],\n",
    "                      llcrnrlon=(lon[0] - 360 - 6), urcrnrlon=(lon[-1] - 360 + 20), resolution='l')\n",
    "        \n",
    "        x, y = map(*np.meshgrid(lon, lat))\n",
    "        # Adjust the text coordinates based on the actual data coordinates\n",
    "\n",
    "        im = axs[axs_start].contourf(x, y, data, levels=v, extend='both',\n",
    "                              transform=ccrs.PlateCarree(), cmap=cmap,norm=norm)\n",
    "        \n",
    "        gl = axs[axs_start].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                   linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        gl.left_labels = True\n",
    "        gl.xformatter = LongitudeFormatter()\n",
    "        gl.yformatter = LatitudeFormatter()\n",
    "        gl.xlabel_style = {'size': 7}  # Adjust the font size as per your preference\n",
    "        gl.ylabel_style = {'size': 7}  # Adjust the font size as per your preference\n",
    "        \n",
    "        axs[axs_start].coastlines()\n",
    "        axs[axs_start].set_aspect('equal')  # this makes the plots better\n",
    "\n",
    "        FSIZE = 12\n",
    "        if model == f'{ex1}_ECMWF_regular_RZSM':\n",
    "            # axs[axs_start].set_title(f'EXP{ex1[2:]}_ECMWF',fontsize=11)\n",
    "            axs[axs_start].set_title(f'(C) UNet_E',fontsize=FSIZE)\n",
    "            axs[axs_start].title.pad = 10\n",
    "        elif model == f'{ex1}_regular_RZSM':\n",
    "            axs[axs_start].set_title(f'(D) UNet_G', fontsize=FSIZE)\n",
    "            # axs[axs_start].set_title(f'EXP{ex1[2:]}_GEFSv12', fontsize=11)\n",
    "            axs[axs_start].title.pad = 10\n",
    "        elif model == f'{ex2}_regular_RZSM':\n",
    "            axs[axs_start].set_title(f'(E) EXP24_OBS', fontsize=FSIZE)\n",
    "            # axs[axs_start].set_title(f'EXP{ex2[2:]}_OBS', fontsize=11)\n",
    "        else:\n",
    "            if model == 'ECMWF':\n",
    "                axs[axs_start].set_title(f'(A) {model}',fontsize=FSIZE)\n",
    "            else:\n",
    "                axs[axs_start].set_title(f'(B) {model}', fontsize=FSIZE)\n",
    "            \n",
    "        axs_start+=1\n",
    "\n",
    "                            #(left, bottom, width, height)\n",
    "    cbar_ax = fig.add_axes([0.13, 0.05, .76, .03])\n",
    "    \n",
    "    # Draw the colorbar\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    # plt.suptitle(f'ACC on testing Dataset', fontsize=30)\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    plot_dictionary['total_grid_cells'] = total_grid_cells\n",
    "    \n",
    "    plt.savefig(f'{save_dir}/Wk{week_lead}_ACC_climpred_multiple_experiments.png')\n",
    "\n",
    "    return(plot_dictionary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01021a-92d5-4f24-bafe-425673b51966",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def KGE(obs_array, forecast_array):\n",
    "    out_array = np.empty(shape=(48,96))\n",
    "    for Y in range(obs_array.shape[1]):\n",
    "        for X in range(obs_array.shape[2]):\n",
    "            kge, r, alpha, beta = he.evaluator(he.kge, forecast_array[:,Y,X], obs_array[:,Y,X])\n",
    "            out_array[Y,X] = kge\n",
    "    return(out_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a8b1cd-2c3e-467f-85ed-5526121f4d14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def take_mean_and_reduce_dimension(file):\n",
    "    return(file[putils.xarray_varname(file)].mean(dim='M').isel(L=0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48344a-2a4c-4c41-bad9-318fdce10344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_KGE_ensemble_mean_multiple_experiments(week_lead, region_name, test_start, test_end, ex1, ex2):\n",
    "\n",
    "    # week_lead = 3\n",
    "    # ex1='EX29'\n",
    "    # ex2 = 'EX24'\n",
    "    \n",
    "    save_dir = f'Outputs/KGE_spatial_plots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    plot_dict = {}\n",
    "\n",
    "    day_num = (week_lead*7) -1\n",
    "\n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_anomaly.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_ecmwf.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order)\n",
    "\n",
    "    obs_arr = take_mean_and_reduce_dimension(obs) #(104, 48, 96)\n",
    "    gefs_arr = take_mean_and_reduce_dimension(gefs) #(104, 48, 96)\n",
    "    ecmwf_arr = take_mean_and_reduce_dimension(ecmwf) #(104, 48, 96)\n",
    "\n",
    "    gefs_cal = KGE(obs_arr, gefs_arr)\n",
    "    ecmwf_cal = KGE(obs_arr, ecmwf_arr)\n",
    "\n",
    "    gefs_out = obs.mean(dim='M').isel(S=0).isel(L=0).copy(deep=True)\n",
    "    gefs_out.RZSM[:,:] = gefs_cal\n",
    "\n",
    "    ecmwf_out = obs.mean(dim='M').isel(S=0).isel(L=0).copy(deep=True)\n",
    "    ecmwf_out.RZSM[:,:] = ecmwf_cal\n",
    "    \n",
    "    plot_dict.update({'ECMWF':ecmwf_out})\n",
    "    plot_dict.update({'GEFSv12':gefs_out})\n",
    "    \n",
    "    ####################################################################################################################################\n",
    "\n",
    "    if ex1 == 'all':\n",
    "        #Now for all predictions from UNET, make the ACC\n",
    "        #Can add *denseLar* for only the dense ones\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*')) #With a specific subset of data\n",
    "    else:\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{ex1}*')) #Will all data\n",
    "\n",
    "    #Get the ECMWF file\n",
    "    ec_unet = [i for i in unet_files if 'ECMWF' in i][0]\n",
    "    gef_unet = [i for i in unet_files if f'{ex1}_regular_RZSM' in i][0]\n",
    "    \n",
    "    print('Working on UNET experiments')\n",
    "    unet_files = [ec_unet,gef_unet]\n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in unet_files:\n",
    "        # i=unet_files[0]     \n",
    "        if 'ECMWF' in i:\n",
    "            new_source = 'ECMWF'\n",
    "        else:\n",
    "            new_source = 'GEFSv12'\n",
    "\n",
    "        test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "        \n",
    "        # break\n",
    "        add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "        unet_arr = take_mean_and_reduce_dimension(add_to_file) #(104, 48, 96)\n",
    "        unet_cal = KGE(obs_arr, unet_arr)\n",
    "        unet_out = obs.mean(dim='M').isel(S=0).isel(L=0).copy(deep=True)\n",
    "        unet_out.RZSM[:,:] = unet_cal\n",
    "        \n",
    "        plot_dict.update({test_name:unet_out})\n",
    "    \n",
    "\n",
    "    #Now add the 2nd UNET experiment\n",
    "    unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{ex2}*')) #Will all data\n",
    "    gef_unet = [i for i in unet_files if f'{ex2}_regular_RZSM' in i][0]\n",
    "\n",
    "    for i in [gef_unet]:    \n",
    "        if 'ECMWF' in i:\n",
    "            new_source = 'ECMWF'\n",
    "        else:\n",
    "            new_source = 'GEFSv12'\n",
    "\n",
    "        test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "        \n",
    "        # break\n",
    "        add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "        unet_arr = take_mean_and_reduce_dimension(add_to_file) #(104, 48, 96)\n",
    "        unet_cal = KGE(obs_arr, unet_arr)\n",
    "        unet_out = obs.mean(dim='M').isel(S=0).isel(L=0).copy(deep=True)\n",
    "        unet_out.RZSM[:,:] = unet_cal\n",
    "        \n",
    "        plot_dict.update({test_name:unet_out})\n",
    "    \n",
    "    # print('Adding EMOS results')\n",
    "    # #Now add EMOS results\n",
    "    # if region_name == 'CONUS':\n",
    "    #     #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "    #     emos_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "    #     emos_acc = emos_acc.sel(lead=day_num)\n",
    "    \n",
    "    # plot_dict.update({'EMOS':emos_acc})\n",
    "    \n",
    "    #Get global max and min\n",
    "    \n",
    "    global_max, global_min = verifications.global_max_min(plot_dict,'RZSM')\n",
    "    v = np.linspace(-3, global_max, 20, endpoint=True)\n",
    "    v = [i for i in v if i <0] + [0] + [i for i in v if i >0] \n",
    "    norm = TwoSlopeNorm(0, vmin=v[0], vmax=v[-1])\n",
    "    \n",
    "    cmap = 'bwr'\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows = 3, ncols= 2, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(10, 8), dpi=300)\n",
    "    \n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    lon = mask.X.values\n",
    "    lat = mask.Y.values\n",
    "\n",
    "    # plot_dictionary = {} #This will save the statistics for each model to see where the ACC skill is greater than a specific threshold\n",
    "    # total_grid_cells = np.count_nonzero(mask_anom)\n",
    "    \n",
    "    axs_start = 0\n",
    "    for model in plot_dict.keys():\n",
    "        # plot_dictionary[model] = {}\n",
    "        # break\n",
    "        data = plot_dict[model].RZSM.values\n",
    "\n",
    "        # for threshold in [0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "        #     abv_thresh = np.where(data>threshold,1,0)\n",
    "        #     plot_dictionary[model][threshold] = np.count_nonzero(abv_thresh) / total_grid_cells\n",
    "            \n",
    "    \n",
    "        map = Basemap(projection='cyl', llcrnrlat=lat[-1] - 1.5, urcrnrlat=lat[0],\n",
    "                      llcrnrlon=(lon[0] - 360 - 6), urcrnrlon=(lon[-1] - 360 + 20), resolution='l')\n",
    "        \n",
    "        x, y = map(*np.meshgrid(lon, lat))\n",
    "        # Adjust the text coordinates based on the actual data coordinates\n",
    "\n",
    "        im = axs[axs_start].contourf(x, y, data, levels=v, extend='both',\n",
    "                              transform=ccrs.PlateCarree(), cmap=cmap,norm=norm)\n",
    "        \n",
    "        gl = axs[axs_start].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                   linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        gl.left_labels = True\n",
    "        gl.xformatter = LongitudeFormatter()\n",
    "        gl.yformatter = LatitudeFormatter()\n",
    "        gl.xlabel_style = {'size': 7}  # Adjust the font size as per your preference\n",
    "        gl.ylabel_style = {'size': 7}  # Adjust the font size as per your preference\n",
    "        \n",
    "        axs[axs_start].coastlines()\n",
    "        axs[axs_start].set_aspect('equal')  # this makes the plots better\n",
    "\n",
    "        FSIZE = 12\n",
    "        if model == f'{ex1}_ECMWF_regular_RZSM':\n",
    "            # axs[axs_start].set_title(f'EXP{ex1[2:]}_ECMWF',fontsize=11)\n",
    "            axs[axs_start].set_title(f'(C) UNet_E',fontsize=FSIZE)\n",
    "            axs[axs_start].title.pad = 10\n",
    "        elif model == f'{ex1}_regular_RZSM':\n",
    "            axs[axs_start].set_title(f'(D) UNet_G', fontsize=FSIZE)\n",
    "            # axs[axs_start].set_title(f'EXP{ex1[2:]}_GEFSv12', fontsize=11)\n",
    "            axs[axs_start].title.pad = 10\n",
    "        elif model == f'{ex2}_regular_RZSM':\n",
    "            axs[axs_start].set_title(f'(E) EXP24_OBS', fontsize=FSIZE)\n",
    "            # axs[axs_start].set_title(f'EXP{ex2[2:]}_OBS', fontsize=11)\n",
    "        else:\n",
    "            if model == 'ECMWF':\n",
    "                axs[axs_start].set_title(f'(A) {model}',fontsize=FSIZE)\n",
    "            else:\n",
    "                axs[axs_start].set_title(f'(B) {model}', fontsize=FSIZE)\n",
    "            \n",
    "        axs_start+=1\n",
    "\n",
    "                            #(left, bottom, width, height)\n",
    "    cbar_ax = fig.add_axes([0.13, 0.05, .76, .03])\n",
    "    \n",
    "    # Draw the colorbar\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    # plt.suptitle(f'ACC on testing Dataset', fontsize=30)\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    # plot_dictionary['total_grid_cells'] = total_grid_cells\n",
    "    \n",
    "    plt.savefig(f'{save_dir}/Wk{week_lead}_KGE_multiple_experiments.png')\n",
    "\n",
    "    return(f'Completed KGE plots lead {week_lead}.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de09eb-600f-4ce1-b0fd-6e111d3542f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def save_ACC_threshold_values(plot_dict):\n",
    "    save_dir = f'Outputs/ACC_spatial_plots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    grid_cells = plot_dict['total_grid_cells']\n",
    "    del plot_dict['total_grid_cells']\n",
    "\n",
    "    # Define the new order and names\n",
    "    new_order_and_names = [('ECMWF\\n(A)', 'ECMWF'), ('GEFSv12\\n(B)', 'GEFSv12'),\n",
    "                           ('UNet_E\\n(C)', 'EX29_ECMWF_regular_RZSM'), ('UNet_G\\n(D)', 'EX29_regular_RZSM'),\n",
    "                           ('EXP24_OBS\\n(E)', 'EX24_regular_RZSM'), \n",
    "                           ]\n",
    "    \n",
    "    # Reorder and rename the dictionary\n",
    "    plot_dict = reorder_and_rename(plot_dict, new_order_and_names)\n",
    "    \n",
    "    # Bar plot setup\n",
    "    models = list(plot_dict.keys())\n",
    "    thresholds = sorted(next(iter(plot_dict.values())).keys())\n",
    "    num_models = len(models)\n",
    "    num_thresholds = len(thresholds)\n",
    "    \n",
    "    bar_width = 0.15\n",
    "    bar_positions = np.arange(num_models)\n",
    "    \n",
    "    # Create the figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), dpi=300)\n",
    "    \n",
    "    # Set colors for the different thresholds\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, num_thresholds))\n",
    "    \n",
    "    # Plot data for each threshold\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        # Collect values for the current threshold across all models\n",
    "        values = [plot_dict[model][threshold] for model in models]\n",
    "        ax.bar(bar_positions + i * bar_width, values, width=bar_width, label=f'Threshold {threshold}', color=colors[i])\n",
    "    \n",
    "    # Adjust tick positions and labels\n",
    "    ax.set_xticks(bar_positions + (num_thresholds - 1) * bar_width / 2)\n",
    "    ax.set_xticklabels(models,fontsize=9)\n",
    "    ax.set_ylabel('Proportion of grid cells')\n",
    "    # ax.set_xlabel('Models')\n",
    "    ax.set_title(f\"Proportion of Skill Scores Above Thresholds. N = {grid_cells}\",fontsize=11)\n",
    "    ax.legend(title='Thresholds')\n",
    "    ax.grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "    plt.savefig(f'{save_dir}/Wk{week_lead}_ACC_bar_plots_above_ACC_threshold_extra_experiment.png')\n",
    "    plt.show()\n",
    "\n",
    "p = plot_dictionary.copy()\n",
    "save_ACC_threshold_values(plot_dict = p)\n",
    "\n",
    "def reorder_and_rename(dictionary, new_order_and_names):\n",
    "    new_dict = {}\n",
    "    for new_key, old_key in new_order_and_names:\n",
    "        if old_key in dictionary:\n",
    "            new_dict[new_key] = dictionary[old_key]\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383da09-0a15-4d0d-8609-c2b4cb6fcf3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''We only ran EMOS, XGBoost on CONUS'''\n",
    "for week_lead in [4]:\n",
    "    if region_name == 'CONUS':\n",
    "        plot_dictionary = spatial_ACC_climpred_with_XGBOOST_multiple_experiments(week_lead, region_name, test_start, test_end, ex1='EX29', ex2='EX24')\n",
    "        del plot_dictionary['EX24_regular_RZSM_2012']\n",
    "        p = plot_dictionary.copy()\n",
    "        save_ACC_threshold_values(plot_dict = p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb00679-3539-4a35-9dc5-a4bedde12c94",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''KGE plots'''\n",
    "for week_lead in [1,2,3,4,5]:\n",
    "    if region_name == 'CONUS':\n",
    "        spatial_KGE_ensemble_mean_multiple_experiments(week_lead, region_name, test_start, test_end, ex1='EX29', ex2='EX24')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4e66b-dae7-4bf6-b41d-cad3fa3d6438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7086df4f-5c91-42dc-9ac3-212443619eb3",
   "metadata": {},
   "source": [
    "# Lineplot CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907754b-8156-42a8-b704-aad88e450405",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Currently using this one'''\n",
    "\n",
    "def lineplot_CRPS_climpred_average_by_type(region_name, test_start, test_end):\n",
    "    print('Calculating CRPS on GEFS and ECMWF. Loading observation and baseline anomaly files.')\n",
    "    \n",
    "    '''This script will take an average of each type of model configuration'''\n",
    "    \n",
    "    save_dir = f'Outputs/CRPS_line_plots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    leads = [6,13,20,27]\n",
    "    obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=leads), baseline_anomaly.sel(L=leads), baseline_ecmwf.sel(L=leads)\n",
    "\n",
    "    gefs_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "    ecmwf_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    diff_nwp = (gefs_acc + ecmwf_acc)/2\n",
    "    df = add_lineplot_to_dataframe(df,diff_nwp,'GEFSv12', 'CRPS',10) #Just keep the name the same for later\n",
    "\n",
    "    print('Adding EMOS results')\n",
    "\n",
    "    First get the ACC values of GEFS and ECMWF relative to observations\n",
    "    # emos_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "    # df = add_lineplot_to_dataframe(df,emos_acc,'EMOS', 'CRPS',10)\n",
    "\n",
    "        '''This is for adding all 4 EMOS experiments, but for some ready it is breaking right now and I'm not sure why (it kills the kernel)'''\n",
    "    emos_files_full = sorted(glob(f'Data/EMOS/EMOS_11*test_predictions*.nc'))\n",
    "    \n",
    "    # Loop through all the EMOS file experiments\n",
    "    add_to_file_OG = ecmwf_acc.copy(deep=True)\n",
    "    add_to_file_OG.acc[:,:,:] = 0\n",
    "    \n",
    "    for idx,file in enumerate(emos_files_full):\n",
    "        for idL,lead in enumerate([1,2,3,4]):\n",
    "            print(f'working on file {file} and lead {lead}')\n",
    "            day_num = (lead*7) -1\n",
    "            emos_ = xr.open_dataset(file).rename({'idate':'S', 'model': 'M','vdate': 'L', 'latitude': 'Y', 'longitude': 'X'}).sel(S=slice(test_start,test_end))\n",
    "            \n",
    "            #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "            emos_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "            add_  = (add_to_file_OG.acc.sel(lead=day_num).values + emos_acc.acc.sel(lead=day_num).values)\n",
    "            add_to_file_OG.acc[idL,:,:] = add_\n",
    "            \n",
    "    add_to_file_OG = add_to_file_OG /len(emos_files_full)\n",
    "    #RE ADD MASK\n",
    "    mm = ~np.isnan(emos_.isel(M=0,S=0).sel(L=[6,13,20,27])).RZSM.values\n",
    "    add_to_file_OG = xr.where(mm==True,add_to_file_OG,np.nan)\n",
    "    df = add_lineplot_to_dataframe(df,add_to_file_OG,f'EMOS', 'ACC',10)\n",
    "    \n",
    "    ####################################################################################################################################\n",
    "\n",
    "    '''If we want to add XGBOOST, this is where we do it. But we only made a single prediction so we cannot do CRPS'''\n",
    "    # #Load the XGBoost data\n",
    "    # for week_ in [1,2,3,4]:\n",
    "    #     print('Working on XGBoost experiments')\n",
    "    #     xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{week_}_testing/*'))\n",
    "    #     num_xg = len(xgboost_files)\n",
    "        \n",
    "    #     out_values = gefs.mean(dim=['S','M','L']).copy(deep = True)\n",
    "    #     out_values.RZSM[:,:] = 0\n",
    "    #     out_values = out_values.rename({'RZSM':'crps', 'Y':'lat','X':'lon'})\n",
    "        \n",
    "    #     #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    #     for i in xgboost_files:\n",
    "    #         # break\n",
    "    #         add_to_file = gefs.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order).copy(deep = True)\n",
    "    #         # break\n",
    "    #         #Still working here\n",
    "    #         test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "    #         load_ = np.expand_dims(np.load(i),-1)\n",
    "    #         load_.shape\n",
    "    #         load_ = np.where(load_ == 0,np.nan,load_)\n",
    "    #         add_realizations = np.empty(shape=(load_.shape[0],11,load_.shape[1],load_.shape[2],load_.shape[3])) #This will help with climpred functions\n",
    "    #         for j in range(11):\n",
    "    #             add_realizations[:,j,:,:,:] = load_\n",
    "    #         add_realizations = verifications.reverse_min_max_scaling(add_realizations, region_name, day_num)\n",
    "    #         add_realizations.shape\n",
    "    #         add_realizations =  np.reshape(add_realizations,(add_realizations.shape[0], add_realizations.shape[1], add_realizations.shape[-1], add_realizations.shape[2], add_realizations.shape[3])) #Reshape to match gefs/ecmwf\n",
    "    #         add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = add_realizations\n",
    "\n",
    "    #         #has values\n",
    "    #         xg_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "    #         xg_acc = xg_acc.mean(dim = 'init')\n",
    "            \n",
    "    #         out_values = out_values + xg_acc\n",
    "        \n",
    "            \n",
    "    #     add_to_file_OG = out_values /num_xg\n",
    "    #     #RE ADD MASK\n",
    "    #     add_to_file_OG = xr.where(~np.isnan(ecmwf_acc.sel(lead=day_num).isel(init=0)),add_to_file_OG,np.nan)\n",
    "    #     df = add_lineplot_to_dataframe_average(df,add_to_file_OG,'XGBOOST', 'CRPS',week_)\n",
    "\n",
    "    for week_ in [1,2,3,4]:\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_}_testing/*')) #With a specific subset of data\n",
    "        \n",
    "        day_num = (week_*7) -1\n",
    "        \n",
    "        keep_files = []\n",
    "        ex = [f'EX{num}' for num in range(28)]\n",
    "        for num in ex:\n",
    "            if num == 'EX26':\n",
    "                pass\n",
    "            elif num in keep_files:\n",
    "                pass\n",
    "            else:\n",
    "                correct_experiments = [j for j in unet_files if num in j]\n",
    "                if len(correct_experiments) != 0:\n",
    "                    '''This if statement helps to avoid experiments that may have been moved or deleted from the scratch space'''\n",
    "                    only_2019 = common_UNET_experiments(correct_experiments)\n",
    "\n",
    "                    for i in only_2019:\n",
    "                        keep_files.append(i)\n",
    "\n",
    "        \n",
    "        \n",
    "        print('Working on UNET experiments')\n",
    "\n",
    "        black = ['EX0','EX13']\n",
    "        red = ['EX14','EX15','EX16','EX17','EX22','EX23','EX24','EX25']\n",
    "        blue = ['EX1','EX2','EX3','EX4','EX5','EX6','EX7','EX8','EX9','EX10','EX11','EX12',\n",
    "               'EX18','EX19','EX20','EX21','EX27','EX28']\n",
    "\n",
    "        bias_correction_black = [file for file in keep_files if any(black_item in file for black_item in black)]\n",
    "        hybrid_blue = [file for file in keep_files if any(black_item in file for black_item in blue)]\n",
    "        obs_red = [file for file in keep_files if any(black_item in file for black_item in red)]\n",
    "        \n",
    "        #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "        for model,name in zip([bias_correction_black, hybrid_blue, obs_red], ['DL_NWP_BC', 'DL_NWP_OBS', 'DL_OBS']):\n",
    "                    \n",
    "            add_to_file_OG = ecmwf_acc.sel(lead=day_num).copy()\n",
    "            add_to_file_OG.crps[:,:] = 0\n",
    "\n",
    "            # break\n",
    "            len_unet = len(model)\n",
    "            for i in model:\n",
    "                if 'ECMWF' in i:\n",
    "                    new_source = 'ECMWF'\n",
    "                else:\n",
    "                    new_source = 'GEFSv12'\n",
    "                    \n",
    "                test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "                ex_name = test_name.split('_')[0]\n",
    "                \n",
    "                add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "\n",
    "                unet_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "                add_to_file_OG  = add_to_file_OG + unet_acc\n",
    "\n",
    "            add_to_file_OG = add_to_file_OG /len_unet\n",
    "            #RE ADD MASK\n",
    "            add_to_file_OG = xr.where(~np.isnan(ecmwf_acc.sel(lead=day_num)),add_to_file_OG,np.nan)\n",
    "            df = add_lineplot_to_dataframe_average(df,add_to_file_OG,name, 'CRPS',week_)\n",
    "            \n",
    "    \n",
    "    #Get global max and min\n",
    "    global_max, global_min = df['CRPS'].max(), df['CRPS'].min()\n",
    "\n",
    "    # Ensure the 'Week' column is sorted to make the line plot correctly\n",
    "    df.sort_values(by=['Forecast', 'Week'], inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Set the plot size\n",
    "    plt.figure(figsize=(10, 6),dpi=300)\n",
    "    \n",
    "    # Use a clearer and more appealing style\n",
    "    plt.style.use('seaborn-v0_8-colorblind')\n",
    "    \n",
    "    # Custom color palette\n",
    "    palette = plt.get_cmap('tab10')\n",
    "    \n",
    "    # Group by the 'Forecast' column\n",
    "    grouped = df.groupby('Forecast')\n",
    "    \n",
    "    color_tracker = {}\n",
    "    marker_style = ['o', 'v', '^', '<', '>', 's', 'p', '*', '+']\n",
    "    \n",
    "    for i, (name, group) in enumerate(grouped):\n",
    "        custom_names2 = custom_names\n",
    "    \n",
    "        color = palette(i)\n",
    "        marker = marker_style[i % len(marker_style)]\n",
    "        \n",
    "        if color not in color_tracker:\n",
    "            plt.plot(group['Week'], group['CRPS'], label=custom_names2[name], color=color, marker=marker, linestyle='-', markersize=5)\n",
    "            color_tracker[color] = custom_names2[name]\n",
    "        else:\n",
    "            plt.plot(group['Week'], group['CRPS'], color=color, marker=marker, linestyle='-', markersize=5)\n",
    "            \n",
    "    \n",
    "    # Setting the title and labels with increased font sizes\n",
    "    plt.title('CRPS by Model and Week Lead', fontsize=16)\n",
    "    plt.xlabel('Week Lead', fontsize=14)\n",
    "    plt.ylabel('CRPS', fontsize=14)\n",
    "    \n",
    "    # Improve readability by adjusting tick parameters\n",
    "    plt.xticks([1, 2, 3, 4], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # Add a grid for better readability\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Create the legend with a slightly larger font size\n",
    "    plt.legend(title='Forecast (# models)', fontsize=12, title_fontsize=13)\n",
    "    \n",
    "    # Optionally, set a tight layout to ensure everything fits without overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f'{save_dir}/Lineplot_CRPS_average_climpred_improved.png', dpi=300)\n",
    "        \n",
    "    return('Completed')\n",
    "\n",
    "###############################################################################################################################################\n",
    "'''RUN'''\n",
    "lineplot_CRPS_climpred_average_by_type(region_name, test_start, test_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610c4a9-8fef-41fb-aefa-02ef676ee5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b90a8-ac0c-40de-9a7f-067a09bc6107",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def open_obs_and_baseline_files(region_name, lead, day_num, test_start, test_end):\n",
    "\n",
    "    obs_anomaly_SubX_format =xr.open_mfdataset(f'Data/GLEAM/RZSM_anomaly_reformat_SubX_format/{region_name}/RZSM_anomaly*.nc4').sel(L=[day_num]).astype(np.float32).load()\n",
    "    obs_anomaly_SubX_format = obs_anomaly_SubX_format.sel(S=slice(test_start,test_end))\n",
    "    \n",
    "    template_testing_only = obs_anomaly_SubX_format.copy(deep=True)\n",
    "    \n",
    "    var_OUT = np.empty(shape=(obs_anomaly_SubX_format.Y.shape[0], obs_anomaly_SubX_format.X.shape[0])) #48x96\n",
    "    \n",
    "    #Mask the final output to be np.nan for ocean values\n",
    "    var_OUT = np.where(mask_anom==1, np.nan, var_OUT)\n",
    "    var_OUT[:,:] = 0\n",
    "    \n",
    "    #######################################   Reforecast baseline files   ###########################################################################\n",
    "    # baseline_anomaly_file_list = sorted(glob('Data/GEFSv12_reforecast/soilw_bgrnd/baseline_RZSM_anomaly/RZSM*.nc'))\n",
    "    if region_name =='CONUS':\n",
    "        baseline_anomaly_file_list = sorted(glob('Data/GEFSv12_reforecast/soilw_bgrnd/baseline_RZSM_anomaly/soil*.nc'))\n",
    "        baseline_anomaly = xr.open_mfdataset(baseline_anomaly_file_list).sel(L=[day_num]).sel(S=slice(test_start,test_end)).astype(np.float32).load()\n",
    "    \n",
    "    else:\n",
    "        baseline_anomaly_file_list = sorted(glob(f'Data_{region_name}/GEFSv12_reforecast/soilw_bgrnd/baseline_RZSM_anomaly/soil*.nc'))\n",
    "        baseline_anomaly = xr.open_mfdataset(baseline_anomaly_file_list).sel(L=[day_num]).sel(S=slice(test_start,test_end)).astype(np.float32).load()\n",
    "        baseline_anomaly = xr.where(np.isnan(mask_anom),np.nan, baseline_anomaly)\n",
    "    \n",
    "    baseline_ecmwf_file_list = sorted(glob(f'Data/ECMWF/soilw_bgrnd_processed/{region_name}/baseline_RZSM_anomaly/soil*.nc'))\n",
    "    baseline_ecmwf = xr.open_mfdataset(baseline_ecmwf_file_list).sel(L=[day_num]).sel(S=slice(test_start,test_end)).astype(np.float32).load()\n",
    "\n",
    "    \n",
    "    #Need to open a template of ECMWF to mask the np.nan values that\n",
    "    return(obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd5b28-f631-464a-9460-32fbdf4a6c97",
   "metadata": {},
   "source": [
    "# Ridgeplot ACC with CONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be36f114-3374-4a53-8627-1c6039814176",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ridgeplot_ACC_climpred_with_XGBOOST(week_lead, region_name, test_start, test_end, single_experiment_or_all_experiments):\n",
    "    \n",
    "    day_num = (week_lead*7) -1\n",
    "\n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_anomaly.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_ecmwf.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order)\n",
    "\n",
    "    #For a single lead\n",
    "    print('Calculating ACC on GEFS and ECMWF')\n",
    "    gefs_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "    # gefs_BC_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(additive_bias_gefs_final.sel(L=day_num).expand_dims({'L':1})), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    ecmwf_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "    # ecmwf_BC_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(additive_bias_ecmwf_final.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order)), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    def flatten_files(file):\n",
    "        return(file[putils.xarray_varname(file)].values.flatten())\n",
    "\n",
    "    print('Masking land values for GEFS and ECMWF')\n",
    "    gefs_acc = flatten_files(gefs_acc)\n",
    "    # gefs_BC_acc = flatten_files(gefs_BC_acc)\n",
    "    ecmwf_acc = flatten_files(ecmwf_acc)\n",
    "    # ecmwf_BC_acc = flatten_files(ecmwf_BC_acc)\n",
    "    \n",
    "    #First get any missing values from ECMWF to properly construct ridgeplot\n",
    "    ec_not_nan = ~np.isnan(ecmwf_acc)\n",
    "\n",
    "    gefs_acc = gefs_acc[ec_not_nan]\n",
    "    # gefs_BC_acc = gefs_BC_acc[ec_not_nan]\n",
    "    ecmwf_acc = ecmwf_acc[ec_not_nan]\n",
    "    # ecmwf_BC_acc = ecmwf_BC_acc[ec_not_nan]\n",
    "\n",
    "    def add_to_dataframe(df, file, source_or_name):\n",
    "        a=pd.DataFrame()\n",
    "        a[source_or_name] =file\n",
    "        return(pd.concat([df,a],axis=1))\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    df = add_to_dataframe(df, ecmwf_acc, 'ECMWF')\n",
    "    # df = add_to_dataframe(df, ecmwf_BC_acc, 'ECMWF_BC')\n",
    "    df = add_to_dataframe(df, gefs_acc, 'GEFSv12')\n",
    "    # df = add_to_dataframe(df, gefs_BC_acc, 'GEFSv12_BC')\n",
    "    \n",
    "    ####################################################################################################################################\n",
    "    \n",
    "    #Load the XGBoost data\n",
    "    if region_name == 'CONUS':\n",
    "        \n",
    "        print('Working on XGBoost experiments')\n",
    "        xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{week_lead}_testing/*'))\n",
    "    \n",
    "        #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "        for i in xgboost_files:\n",
    "            # break\n",
    "            test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "            add_to_file = verifications.load_XGBoost_file_and_make_ensemble_spread(gefs=gefs, file=i, source_of_XGBoost_reforecast='GEFSv12',day_num=day_num, region_name=region_name, test_year=test_year)\n",
    "            \n",
    "            xg_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "            xg_acc = xg_acc.acc.values.flatten()\n",
    "            xg_acc = xg_acc[ec_not_nan]\n",
    "            df = add_to_dataframe(df, xg_acc, test_name)\n",
    "    \n",
    "    print('Adding EMOS results')\n",
    "    #Now add EMOS results\n",
    "    if region_name == 'CONUS':\n",
    "        emos_select = emos_.sel(L=day_num)\n",
    "        \n",
    "        #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "        emos_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "    \n",
    "        emos_final = emos_acc.sel(lead=day_num).acc.values.flatten()\n",
    "        emos_final = emos_final[ec_not_nan]\n",
    "        df = add_to_dataframe(df, emos_final, 'EMOS')\n",
    "\n",
    "    if single_experiment_or_all_experiments == 'all':\n",
    "        #Now for all predictions from UNET, make the ACC\n",
    "        #Can add *denseLar* for only the dense ones\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*')) #With a specific subset of data\n",
    "    else:\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{single_experiment_or_all_experiments}')) #Will all data\n",
    "    \n",
    "    if test_year == 2019:\n",
    "        unet_files = [i for i in unet_files if '2012' not in i]\n",
    "        \n",
    "    print('Working on UNET experiments')\n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in unet_files:\n",
    "\n",
    "        \n",
    "        if 'ECMWF' in i:\n",
    "            new_source = 'ECMWF'\n",
    "        else:\n",
    "            new_source = 'GEFSv12'\n",
    "\n",
    "        \n",
    "        if 'mean' in i:\n",
    "            # break\n",
    "            if 'final_mean_ensemble' in i:\n",
    "                # break\n",
    "                #Still working here\n",
    "                test_name = i.split('testing_')[-1].split('.npy')[0].split('ensemble_')[-1]\n",
    "                add_to_file = verifications.load_UNET_files_with_mean(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "\n",
    "                unet_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "                unet_acc = unet_acc.acc.values.flatten()\n",
    "                unet_acc = unet_acc[ec_not_nan]\n",
    "                df = add_to_dataframe(df, unet_acc, test_name)\n",
    "\n",
    "        else:\n",
    "            test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "            add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "\n",
    "            unet_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "            unet_acc = unet_acc.acc.values.flatten()\n",
    "            unet_acc = unet_acc[ec_not_nan]\n",
    "            df = add_to_dataframe(df, unet_acc, test_name)\n",
    "        \n",
    "\n",
    "    save_dir = f'Outputs/joyplots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    #Now plot all on 1 figure (Cant do it with joyplot)\n",
    "    # fig,axs = plt.subplots(nrows=1,ncols=3,figsize = (10,6))\n",
    "    # axs = axs.flatten()\n",
    "\n",
    "    '''I see that the persistence and climatology is the exact same for every dataset, so let's remove those and simply add as another'''\n",
    "    if single_experiment_or_all_experiments == 'all':\n",
    "        joypy.joyplot(df,colormap=cm.autumn_r,\n",
    "                     title=f\"ACC by Experiment Week {week_lead}\",\n",
    "                     fade=True,figsize=(10,30))\n",
    "    else:\n",
    "        joypy.joyplot(df,colormap=cm.autumn_r,\n",
    "             title=f\"ACC by Experiment Week {week_lead}\",\n",
    "             fade=True,figsize=(5,10))\n",
    "\n",
    "\n",
    "    plt.savefig(f'{save_dir}/Wk{week_lead}_ACC_climpred.png')\n",
    "\n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25548bf5-b62e-489d-9f3b-8c6a24b47773",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe4301-c601-4851-b723-8a92b437232a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for week_lead in [1,2,3,4,5]:\n",
    "    ridgeplot_ACC_climpred_with_XGBOOST(week_lead, region_name, test_start, test_end, single_experiment_or_all_experiments='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830bbe9e-f776-4ae8-86ab-5fcf87ce66fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "global admin_boundaries\n",
    "admin_boundaries = cfeature.NaturalEarthFeature(category='p',\n",
    "                                                name='admin_1_states_provinces_lines',\n",
    "                                                scale='50m',\n",
    "                                                facecolor='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea764a3-4adc-413e-97be-37c5e10f82f4",
   "metadata": {},
   "source": [
    "# Spatial ACC with only UNET and raw reforecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfdbcc5-d99d-4352-b6ed-3075e1f9d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_ACC_climpred_with_raw_and_UNET(week_lead, region_name, test_start, test_end, single_experiment_or_all_experiments):\n",
    "\n",
    "    save_dir = f'Outputs/ACC_spatial_plots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    plot_dict = {}\n",
    "    \n",
    "    day_num = (week_lead*7) -1\n",
    "\n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_anomaly.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_ecmwf.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order)\n",
    "\n",
    "    print('Calculating ACC on GEFS and ECMWF')\n",
    "    gefs_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "    # gefs_BC_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(additive_bias_gefs_final.sel(L=day_num).expand_dims({'L':1})), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    ecmwf_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "    # ecmwf_BC_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(additive_bias_ecmwf_final.sel(L=day_num).expand_dims({'L':1})), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "\n",
    "    plot_dict.update({'ECMWF':ecmwf_acc})\n",
    "    plot_dict.update({'GEFSv12':gefs_acc})\n",
    "    \n",
    "\n",
    "    if single_experiment_or_all_experiments == 'all':\n",
    "        #Now for all predictions from UNET, make the ACC\n",
    "        #Can add *denseLar* for only the dense ones\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*')) #With a specific subset of data\n",
    "    else:\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{single_experiment_or_all_experiments}*')) #Will all data\n",
    "\n",
    "    #Get the ECMWF file\n",
    "    ec_unet = [i for i in unet_files if 'ECMWF' in i][0]\n",
    "    gef_unet = [i for i in unet_files if f'{single_experiment_or_all_experiments}_regular_RZSM' in i][0]\n",
    "    \n",
    "    print('Working on UNET experiments')\n",
    "    unet_files = [ec_unet,gef_unet]\n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in unet_files:\n",
    "        \n",
    "        if 'ECMWF' in i:\n",
    "            new_source = 'ECMWF'\n",
    "        else:\n",
    "            new_source = 'GEFSv12'\n",
    "\n",
    "        test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "\n",
    "\n",
    "       \n",
    "        # break\n",
    "        add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "        unet_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "        \n",
    "        plot_dict.update({test_name:unet_acc})\n",
    "\n",
    "    \n",
    "    #Get global max and min\n",
    "    global_max, global_min = verifications.global_max_min(plot_dict,'acc')\n",
    "\n",
    "    #Get the data separated by 0\n",
    "    min_max = np.linspace(global_min,global_max,15,endpoint = True)\n",
    "    min_max2 = [i for i in min_max if i < 0] + [0] + [i for i in min_max if i > 0]\n",
    "    \n",
    "    cmap = 'bwr'\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows = 2, ncols= 2, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(10, 7), dpi=300)\n",
    "    \n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    lon = mask.X.values\n",
    "    lat = mask.Y.values\n",
    "\n",
    "    total_grid_cells = np.count_nonzero(mask_anom)\n",
    "    plot_dictionary = {} #This will save the statistics for each model to see where the ACC skill is greater than a specific threshold\n",
    "    axs_start = 0\n",
    "    for model in plot_dict.keys():\n",
    "        plot_dictionary[model] = {}\n",
    "        # break\n",
    "        data = plot_dict[model].acc.values\n",
    "\n",
    "        for threshold in [0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "            abv_thresh = np.where(data>threshold,1,0)\n",
    "            plot_dictionary[model][threshold] = np.count_nonzero(abv_thresh) / total_grid_cells\n",
    "\n",
    "        map = Basemap(projection='cyl', llcrnrlat=lat[-1] - 1.5, urcrnrlat=lat[0],\n",
    "                      llcrnrlon=(lon[0] - 360 - 6), urcrnrlon=(lon[-1] - 360 ), resolution='l')\n",
    "        \n",
    "        x, y = map(*np.meshgrid(lon, lat))\n",
    "        # Adjust the text coordinates based on the actual data coordinates\n",
    "\n",
    "        norm = TwoSlopeNorm(0, vmin=min_max2[0], vmax=min_max2[-1])\n",
    "        \n",
    "        im = axs[axs_start].contourf(x, y, data, levels=min_max2, extend='both',\n",
    "                              transform=ccrs.PlateCarree(), cmap=cmap, norm = norm)\n",
    "        \n",
    "        gl = axs[axs_start].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                   linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "        gl.xlabels_top = False\n",
    "        gl.ylabels_right = False\n",
    "        gl.ylabels_left = True\n",
    "        gl.xformatter = LongitudeFormatter()\n",
    "        gl.yformatter = LatitudeFormatter()\n",
    "        axs[axs_start].coastlines()\n",
    "        axs[axs_start].add_feature(cfeature.STATES, linestyle='-', edgecolor='black')\n",
    "        axs[axs_start].set_aspect('equal')  # this makes the plots better\n",
    "        axs[axs_start].set_title(f'{model} Week {week_lead}',fontsize=8)\n",
    "        axs_start+=1\n",
    "\n",
    "                            #(left, bottom, width, height)\n",
    "    cbar_ax = fig.add_axes([0.1, 0.1, .8, .03])\n",
    "    \n",
    "    # Draw the colorbar\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    cbar.set_label('ACC', fontsize=14)\n",
    "    # plt.suptitle(f'ACC on testing Dataset', fontsize=30)\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    plot_dictionary['total_grid_cells'] = total_grid_cells\n",
    "    plt.savefig(f'{save_dir}/Wk{week_lead}_ACC_climpred_only_NWP_and_UNET.png')\n",
    "\n",
    "    return(plot_dictionary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b466ccfd-74d1-4898-9470-a1864a0af5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109a47d-f282-42de-bda1-dfc169d10ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14551c53-efa5-4add-a879-12cf30de62f0",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4cadfb-cf81-4e22-af98-95ec59af424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for week_lead in [5]:\n",
    "    if region_name == 'CONUS':\n",
    "        plot_dictionary = spatial_ACC_climpred_with_raw_and_UNET(week_lead, region_name, test_start, test_end, single_experiment_or_all_experiments='EX27')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67758f83-bfb3-4be8-8d7c-d857babda545",
   "metadata": {},
   "source": [
    "# Spatial ACC < 20th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e23476-3054-4702-852c-0b7dfb2e587b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76ed5e1-6072-4865-a9ca-aa13418b9fcf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_climpred_ACC_by_init(fcst, obs):\n",
    "    '''For some reason the data from additive bias correction is incorrectly chunked, so this will fix it. But it's pretty slow'''\n",
    "    \n",
    "    fcst_name = list(fcst.keys())[0]\n",
    "    # Ensure forecast dataset is chunked correctly\n",
    "    fcst = fcst.chunk({'init': -1, 'lon': 'auto', 'lat': 'auto'})\n",
    "    hcast = climpred.HindcastEnsemble(fcst)\n",
    "    # Ensure observation dataset is chunked consistently\n",
    "    obs = obs.chunk({'time': -1, 'lon': 'auto', 'lat': 'auto'})\n",
    "    object_ = hcast.add_observations(obs)\n",
    "    return object_.verify(metric=\"acc\", comparison=\"e2o\", dim=['init'], alignment=\"same_inits\",skipna=True).rename({fcst_name: 'acc'}).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599253c7-4824-416c-b8fc-d7cb958bf967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "\n",
    "# week_lead=1\n",
    "# single_experiment_or_all_experiments = 'EX24_regular_RZSM'\n",
    "# percentile=20\n",
    "\n",
    "def spatial_ACC_less_than_percentile(week_lead, region_name, test_start, test_end, single_experiment_or_all_experiments,percentile):\n",
    "    '''Find the observation data that is below the 20th percentile as a mask, then do the ACC'''\n",
    "\n",
    "    \n",
    "    save_dir = f'Outputs/ACC_spatial_plots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    anom_obs_perc = xr.open_dataset(f'Data/GLEAM/anomaly_percentile_RZSM_full_distribution_with_different_thresholds.nc4')\n",
    "    anom_obs_perc = anom_obs_perc[f'{percentile}th_percentile']\n",
    "    anom_obs_perc = anom_obs_perc.rename({'latitude':'Y','longitude':'X'})\n",
    "\n",
    "    obs_original_subset = obs_original.sel(time=anom_obs_perc.time.values)\n",
    "    \n",
    "    #Find wher the observation is below the threshold %\n",
    "    obs_below_final = xr.where(obs_original_subset<anom_obs_perc, obs_original_subset,np.nan)\n",
    "    \n",
    "    obs_original\n",
    "    plot_dict = {}\n",
    "    \n",
    "    day_num = (week_lead*7) -1\n",
    "\n",
    "    leads = [6,13,20,27]\n",
    "    obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=leads), baseline_anomaly.sel(L=leads), baseline_ecmwf.sel(L=leads)\n",
    "\n",
    "    \n",
    "    out_percentile = obs.copy(deep=True)\n",
    "    out_percentile.RZSM[:,:,:,:,:] = np.nan\n",
    "    \n",
    "    #Get index values of where obs is < percentile\n",
    "    def find_if_below_percentile(obs, anom_obs_perc, out_percentile):\n",
    "        '''First loop through the observation for the day of year and see if each day is below the Nth percentile'''\n",
    "        for idS,date in enumerate(obs.S.values):\n",
    "            date_ = obs.S.values[idS]\n",
    "            for idx,lead in enumerate(obs.L.values):\n",
    "                # break\n",
    "                fin_date = date_+np.timedelta64(idx+1,'D')\n",
    "                \n",
    "                #now select the 3-d observation file\n",
    "                anom = anom_obs_perc.sel(time=fin_date)\n",
    "\n",
    "                #now find if below percentile\n",
    "                mask_ = xr.where(obs.isel(S=idS).isel(L=idx)<anom,obs.isel(S=idS).isel(L=idx),np.nan)\n",
    "                out_percentile.RZSM[idS,:,idx,:,:] = mask_.RZSM.values\n",
    "        \n",
    "        return(out_percentile)\n",
    "        \n",
    "    \n",
    "    out_percentile = find_if_below_percentile(obs, anom_obs_perc, out_percentile)\n",
    "\n",
    "    '''Now we have the anomaly values in which the observations (in SubX format) are below the specific percentile'''\n",
    "\n",
    "\n",
    "    '''Next, we need to only retrieve the raw forecasts on the same dates'''\n",
    "    gefs = xr.where(~np.isnan(out_percentile), gefs, np.nan)\n",
    "    ecmwf = xr.where(~np.isnan(out_percentile), ecmwf, np.nan)\n",
    "\n",
    "    def ACC_function_each_realization(obs, forecast):\n",
    "        '''This function is not completed. Look below for the better structure when i want to fix this function'''\n",
    "        out_file = out_percentile.isel(S=0,L=0).copy(deep=True)\n",
    "        out_file.RZSM[:,:,:] = np.nan\n",
    "\n",
    "        for idY,Y in enumerate(obs.Y.values):\n",
    "            for idX,X in enumerate(obs.X.values):\n",
    "                for idM, model in enumerate(obs.M.values):\n",
    "                    for idL,lead in enumerate(obs.L.values):\n",
    "                        '''Find the mean of obs and forecast'''\n",
    "                        # obs_m = np.nanmean(obs.RZSM.isel(M=idM,L=idL).values.flatten())\n",
    "                        # fcst_m = np.nanmean(forecast.RZSM.isel(M=idM,L=idL).values.flatten())\n",
    "        \n",
    "                        numerator = np.nansum(forecast.RZSM.isel(M=idM,L=idL,X=idX,Y=idY).values * obs.RZSM.isel(M=idM,L=idL,X=idX,Y=idY).values)\n",
    "                        # Calculate denominator of ACC\n",
    "                        denominator = np.sqrt(np.nansum(forecast.RZSM.isel(M=idM,L=idL,X=idX,Y=idY).values**2) * np.nansum( obs.RZSM.isel(M=idM,L=idL,X=idX,Y=idY).values**2))\n",
    "                            # Avoid division by zero\n",
    "                        if denominator == 0:\n",
    "                            out_file[idM,idY,idX] = np.nan\n",
    "                        else:\n",
    "                            # Calculate ACC\n",
    "                            acc = numerator / denominator\n",
    "                            out_file[idM,idY,idX] = acc\n",
    "\n",
    "    \n",
    "    def ACC_function_ensemble_mean(out_percentile, forecast):\n",
    "        '''Saving as (lead, latitude, longitude)'''\n",
    "        out_file = out_percentile.isel(S=0,M=0).copy(deep=True)\n",
    "        out_file.RZSM[:,:,:] = np.nan\n",
    "\n",
    "        obs_mean = out_percentile.mean(dim='M').load()\n",
    "        forecast_mean = forecast.mean(dim='M').load()\n",
    "\n",
    "        obs_mean_vals = obs_mean.RZSM.values\n",
    "        forecast_mean_vals = forecast_mean.RZSM.values\n",
    "\n",
    "        for idY,Y in enumerate(obs_mean.Y.values):\n",
    "            for idX,X in enumerate(obs_mean.X.values):\n",
    "                for idL,lead in enumerate(obs_mean.L.values):\n",
    "    \n",
    "                    numerator = np.nansum(forecast_mean_vals[:,idL,idY,idX].flatten() * obs_mean_vals[:,idL,idY,idX].flatten())\n",
    "                    # Calculate denominator of ACC\n",
    "                    denominator = np.sqrt(np.nansum(forecast_mean_vals[:,idL,idY,idX].flatten()**2) * np.nansum( obs_mean_vals[:,idL,idY,idX].flatten()**2))\n",
    "                        # Avoid division by zero\n",
    "                    if denominator == 0:\n",
    "                        out_file.RZSM[idL,idY,idX] = np.nan\n",
    "                    else:\n",
    "                        # Calculate ACC\n",
    "                        acc = numerator / denominator\n",
    "                        out_file.RZSM[idL,idY,idX] = acc\n",
    "                        \n",
    "        return(out_file)\n",
    "\n",
    "    gefs_ACC = ACC_function_ensemble_mean(out_percentile, gefs)\n",
    "    ecmwf_ACC = ACC_function_ensemble_mean(out_percentile, ecmwf)\n",
    "    \n",
    "                        \n",
    "    #Now apply the function to find where the anomalies are the below the 20th percentile\n",
    "    below_gefs = find_when_below_percentile(obs_anom_percentile, obs, gefs, percentile)\n",
    "    #has values\n",
    "    gefs_acc = create_climpred_ACC_by_init(fcst=verifications.rename_subx_for_climpred(below_gefs), obs=verifications.rename_obs_for_climpred(obs_original))\n",
    "    gefs_acc_t = create_climpred_ACC_by_init(fcst=verifications.rename_subx_for_climpred(gefs), obs=verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    \n",
    "    below_ecmwf = find_when_below_percentile(obs_anom_percentile, obs, ecmwf, percentile)\n",
    "    #has values\n",
    "    ecmwf_acc = create_climpred_ACC_by_init(fcst=verifications.rename_subx_for_climpred(below_ecmwf), obs=verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    plot_dict.update({'ECMWF':ecmwf_acc})\n",
    "    plot_dict.update({'GEFSv12':gefs_acc})\n",
    "    \n",
    "    ####################################################################################################################################\n",
    "    \n",
    "    #Load the XGBoost data\n",
    "    if region_name == 'CONUS':\n",
    "        print('Working on XGBoost experiments')\n",
    "        xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{week_lead}_testing/*EX28*'))\n",
    "    \n",
    "        #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "        for i in xgboost_files:\n",
    "            # break\n",
    "            add_to_file = verifications.load_XGBoost_file_and_make_ensemble_spread(gefs=gefs, file=i, source_of_XGBoost_reforecast='GEFSv12',day_num=day_num, region_name=region_name, test_year=test_year)\n",
    "\n",
    "            below_xg = find_when_below_percentile(obs_anom_percentile, obs, add_to_file, percentile)\n",
    "            #has values\n",
    "            xg_acc = create_climpred_ACC_by_init(fcst=verifications.rename_subx_for_climpred(below_xg), obs=verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    plot_dict.update({'XGBoost':xg_acc})\n",
    "\n",
    "    if single_experiment_or_all_experiments == 'all':\n",
    "        #Now for all predictions from UNET, make the ACC\n",
    "        #Can add *denseLar* for only the dense ones\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*')) #With a specific subset of data\n",
    "    else:\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{single_experiment_or_all_experiments}*')) #Will all data\n",
    "\n",
    "    #Get the ECMWF file\n",
    "    ec_unet = [i for i in unet_files if 'ECMWF' in i][0]\n",
    "    gef_unet = [i for i in unet_files if f'{single_experiment_or_all_experiments}_regular_RZSM' in i][0]\n",
    "    \n",
    "    print('Working on UNET experiments')\n",
    "    unet_files = [ec_unet,gef_unet]\n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in unet_files:\n",
    "        # break\n",
    "        add_to_file = gefs.copy(deep = True)\n",
    "        test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "        test =  verifications.reverse_min_max_scaling(np.load(i), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "        test = np.reshape(test,(test.shape[0]//11,11,test.shape[1],test.shape[2]))\n",
    "        test = np.expand_dims(test, -1)\n",
    "        #Now re-order the dimensions to match SubX\n",
    "        load_ =  np.reshape(test,(test.shape[0], test.shape[1], test.shape[-1], test.shape[2], test.shape[3]))\n",
    "        add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = load_\n",
    "    \n",
    "        below_unet = find_when_below_percentile(obs_anom_percentile, obs, add_to_file, percentile)\n",
    "        #has values\n",
    "        unet_acc = create_climpred_ACC_by_init(fcst=verifications.rename_subx_for_climpred(below_unet), obs=verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "      \n",
    "        plot_dict.update({test_name:unet_acc})\n",
    "    \n",
    "\n",
    "        \n",
    "    print('Adding EMOS results')\n",
    "    #Now add EMOS results\n",
    "    if region_name == 'CONUS':\n",
    "        #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "        below_emos = find_when_below_percentile(obs_anom_percentile, obs, emos_.sel(L=day_num), percentile)\n",
    "        #has values\n",
    "        emos_acc = create_climpred_ACC_by_init(fcst=verifications.rename_subx_for_climpred(below_emos), obs=verifications.rename_obs_for_climpred(obs_original))\n",
    "        \n",
    "        # emos_acc = create_climpred_ACC_by_init(fcst=verifications.rename_subx_for_climpred(emos_), obs=verifications.rename_obs_for_climpred(obs_original))\n",
    "        \n",
    "    plot_dict.update({'EMOS':emos_acc})\n",
    "    \n",
    "    #Get global max and min\n",
    "    global_max, global_min = verifications.global_max_min(plot_dict,'acc')\n",
    "\n",
    "    \n",
    "    cmap = 'bwr'\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows = 2, ncols= 3, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(10, 7), dpi=300)\n",
    "    \n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    lon = mask.X.values\n",
    "    lat = mask.Y.values\n",
    "    \n",
    "    axs_start = 0\n",
    "    for model in plot_dict.keys():\n",
    "        # break\n",
    "        data = plot_dict[model].acc.values\n",
    "        v = np.linspace(global_min, global_max, 20, endpoint=True)\n",
    "\n",
    "        map = Basemap(projection='cyl', llcrnrlat=lat[-1] - 1.5, urcrnrlat=lat[0],\n",
    "                      llcrnrlon=(lon[0] - 360 - 6), urcrnrlon=(lon[-1] - 360 + 20), resolution='l')\n",
    "        \n",
    "        x, y = map(*np.meshgrid(lon, lat))\n",
    "        # Adjust the text coordinates based on the actual data coordinates\n",
    "\n",
    "        im = axs[axs_start].contourf(x, y, data, levels=v, extend='both',\n",
    "                              transform=ccrs.PlateCarree(), cmap=cmap)\n",
    "        \n",
    "        gl = axs[axs_start].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                   linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "        gl.xlabels_top = False\n",
    "        gl.ylabels_right = False\n",
    "        gl.ylabels_left = True\n",
    "        gl.xformatter = LongitudeFormatter()\n",
    "        gl.yformatter = LatitudeFormatter()\n",
    "        axs[axs_start].coastlines()\n",
    "        axs[axs_start].set_aspect('equal')  # this makes the plots better\n",
    "        axs[axs_start].set_title(f'{model} Week {week_lead}',fontsize=8)\n",
    "        axs_start+=1\n",
    "\n",
    "                            #(left, bottom, width, height)\n",
    "    cbar_ax = fig.add_axes([0.05, 0.05, .9, .04])\n",
    "    \n",
    "    # Draw the colorbar\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    plt.suptitle(f'ACC on testing Dataset', fontsize=30)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    plt.savefig(f'{save_dir}/Wk{week_lead}_ACC_climpred_{percentile}th_percentile.png')\n",
    "\n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f4523-f0d5-4822-abae-619417a08f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3114150-5e55-44cb-b9b7-588a5c074844",
   "metadata": {},
   "source": [
    "# Lineplot ACC all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec95ac-944a-4369-ac62-27ade932ddaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_lineplot_to_dataframe(df,fcst_vals,name_of_fcst, metric,week):\n",
    "    # df = pd.DataFrame()\n",
    "\n",
    "    def return_color(name_of_fcst):\n",
    "        black = ['EX0','EX13'] # bias-corrected DL\n",
    "        red = ['EX14','EX15','EX16','EX17','EX22','EX23','EX24','EX25'] #Observation driven\n",
    "        blue = ['EX1','EX2','EX3','EX4','EX5','EX6','EX7','EX8','EX9','EX10','EX11','EX12',\n",
    "               'EX18','EX19','EX20','EX21','EX27','EX28'] #Hybrid\n",
    "        \n",
    "        green = ['ECMWF','GEFSv12']\n",
    "        \n",
    "        purple = 'XGBOOST'\n",
    "\n",
    "        yellow = 'EMOS'\n",
    "\n",
    "        if name_of_fcst in black:\n",
    "            color = 'black'\n",
    "        elif name_of_fcst in red:\n",
    "            color = 'red'\n",
    "        elif name_of_fcst in blue:\n",
    "            color = 'blue'\n",
    "        elif name_of_fcst in green:\n",
    "            color = 'green'\n",
    "        elif purple in name_of_fcst:\n",
    "            color = 'purple'\n",
    "        elif yellow in name_of_fcst:\n",
    "            color = 'yellow'\n",
    "            \n",
    "        return(color)\n",
    "\n",
    "    \n",
    "    if week==10:\n",
    "        for idx,lead in enumerate([6,13,20,27]):\n",
    "            \n",
    "            data = fcst_vals.sel(lead=lead).mean()[putils.xarray_varname(fcst_vals)].values\n",
    "            dict_ = {'Forecast':[name_of_fcst], 'Week':[idx+1], f'{metric}': [data], 'Color':return_color(name_of_fcst)}\n",
    "            df = pd.concat([df,pd.DataFrame.from_dict(dict_)])\n",
    "    else:\n",
    "        data = fcst_vals.mean()[putils.xarray_varname(fcst_vals)].values\n",
    "        dict_ = {'Forecast':[name_of_fcst], 'Week':[week], f'{metric}': [data], 'Color':return_color(name_of_fcst)}\n",
    "        df = pd.concat([df,pd.DataFrame.from_dict(dict_)])\n",
    "\n",
    "    \n",
    "    \n",
    "    return(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56a989-4de9-4707-9b31-4a5c8770bd96",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_lineplot_to_dataframe_average_with_min_max(df,fcst_vals,name_of_fcst, metric,week):\n",
    "    # df = pd.DataFrame()\n",
    "\n",
    "    def return_color(name_of_fcst):\n",
    "        black = ['DL_NWP_BC']\n",
    "        red = ['DL_OBS']\n",
    "        blue = ['DL_NWP_OBS']\n",
    "        \n",
    "        green = ['ECMWF','GEFSv12']\n",
    "        \n",
    "        purple = 'XGBOOST'\n",
    "\n",
    "        yellow = 'EMOS'\n",
    "\n",
    "        if name_of_fcst in black:\n",
    "            color = 'black'\n",
    "        elif name_of_fcst in red:\n",
    "            color = 'red'\n",
    "        elif name_of_fcst in blue:\n",
    "            color = 'blue'\n",
    "        elif name_of_fcst in green:\n",
    "            color = 'green'\n",
    "        elif purple in name_of_fcst:\n",
    "            color = 'purple'\n",
    "        elif yellow in name_of_fcst:\n",
    "            color = 'yellow'\n",
    "            \n",
    "        return(color)\n",
    "\n",
    "    \n",
    "    if week==10:\n",
    "        for idx,lead in enumerate([6,13,20,27]):\n",
    "            \n",
    "            data = fcst_vals.sel(lead=lead).mean()[putils.xarray_varname(fcst_vals)].values\n",
    "            dict_ = {'Forecast':[name_of_fcst], 'Week':[idx+1], f'{metric}': [data], 'Color':return_color(name_of_fcst)}\n",
    "            df = pd.concat([df,pd.DataFrame.from_dict(dict_)])\n",
    "    else:\n",
    "        data = fcst_vals.mean()[putils.xarray_varname(fcst_vals)].values\n",
    "        dict_ = {'Forecast':[name_of_fcst], 'Week':[week], f'{metric}': [data], 'Color':return_color(name_of_fcst)}\n",
    "        df = pd.concat([df,pd.DataFrame.from_dict(dict_)])\n",
    "\n",
    "    \n",
    "    \n",
    "    return(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88ce97-44b1-4843-a48d-3a2b0183f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''lineplot_ACC_climpred_with_XGBOOST'''\n",
    "def lineplot_ACC_climpred_with_XGBOOST(region_name, test_start, test_end):\n",
    "\n",
    "    save_dir = f'Outputs/ACC_line_plots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    #\n",
    "    leads = [6,13,20,27]\n",
    "    obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=leads), baseline_anomaly.sel(L=leads), baseline_ecmwf.sel(L=leads)\n",
    "    \n",
    "    print('Calculating ACC on GEFS and ECMWF')\n",
    "    \n",
    "    gefs_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    ecmwf_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    df = add_lineplot_to_dataframe(df,gefs_acc,'GEFSv12', 'ACC',10)\n",
    "    df = add_lineplot_to_dataframe(df,ecmwf_acc,'ECMWF', 'ACC',10)\n",
    "\n",
    "    print('Adding EMOS results')\n",
    "\n",
    "    #Loop through all the EMOS file experiments\n",
    "    for idx,file in enumerate(sorted(glob(f'Data/EMOS/*test_predictions*.nc'))):\n",
    "        op = xr.open_dataset(file).rename({'idate':'S', 'model': 'M','vdate': 'L', 'latitude': 'Y', 'longitude': 'X'}).sel(S=slice(test_start,test_end))\n",
    "        #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "        emos_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "        df = add_lineplot_to_dataframe(df,emos_acc,f'EMOS', 'ACC',10)\n",
    "\n",
    "    \n",
    "    #First get the ACC values of GEFS and ECMWF relative to observations (add only a single EMOS file with the code below)\n",
    "    # emos_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "    # df = add_lineplot_to_dataframe(df,emos_acc,'EMOS', 'ACC',10)\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "    #Load the XGBoost data\n",
    "    for lead in [1,2,3,4]:\n",
    "        day_num = (lead*7) -1\n",
    "        \n",
    "        print('Working on XGBoost experiments')\n",
    "        xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{lead}_testing/*'))\n",
    "    \n",
    "        #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "        for i in xgboost_files:\n",
    "            # break\n",
    "            test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "            add_to_file = verifications.load_XGBoost_file_and_make_ensemble_spread(gefs=gefs, file=i, source_of_XGBoost_reforecast='GEFSv12',day_num=day_num, region_name=region_name, test_year=test_year)\n",
    "            \n",
    "            xg_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "            \n",
    "            df = add_lineplot_to_dataframe(df,xg_acc,test_name, 'ACC',lead)\n",
    "\n",
    "\n",
    "    for week_ in [1,2,3,4]:\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_}_testing/*')) #With a specific subset of data\n",
    "\n",
    "        if test_year == 2019:\n",
    "            unet_files = [i for i in unet_files if '2012' not in i]\n",
    "            \n",
    "        day_num = (week_*7) -1\n",
    "        \n",
    "        keep_files = []\n",
    "        ex = [f'EX{num}' for num in range(28)]\n",
    "        for num in ex:\n",
    "            if num == 'EX26':\n",
    "                pass\n",
    "            elif num in keep_files:\n",
    "                pass\n",
    "            else:\n",
    "                correct_experiments = [j for j in unet_files if num in j]\n",
    "                only_RZSM = [j for j in correct_experiments if 'RZSM' in j]\n",
    "                only_RZSM = [j for j in only_RZSM if 'final' not in j]\n",
    "                only_RZSM = [j for j in only_RZSM if 'Residual' not in j]\n",
    "                \n",
    "                if num in ['EX27']:\n",
    "                    only_RZSM = [j for j in only_RZSM if 'ECMWF' not in j]\n",
    "                try:\n",
    "                    keep_files.append(only_RZSM[0])\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            \n",
    "        print('Working on UNET experiments')\n",
    "    \n",
    "        #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "        for i in keep_files:   \n",
    "            if 'ECMWF' in i:\n",
    "                new_source = 'ECMWF'\n",
    "            else:\n",
    "                new_source = 'GEFSv12'\n",
    "            # break\n",
    "\n",
    "            \n",
    "            test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "            \n",
    "            add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "            \n",
    "            ex_name = test_name.split('_')[0]\n",
    "\n",
    "            unet_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "            \n",
    "            df = add_lineplot_to_dataframe(df,unet_acc,ex_name, 'ACC',week_)\n",
    "        \n",
    "    \n",
    "    #Get global max and min\n",
    "    global_max, global_min = df['ACC'].max(), df['ACC'].min()\n",
    "\n",
    "    # Ensure the 'Week' column is sorted to make the line plot correctly\n",
    "    df.sort_values(by=['Forecast', 'Week'], inplace=True)\n",
    "    \n",
    "\n",
    "    # Set the plot size\n",
    "    plt.figure(figsize=(10, 6),dpi=300)\n",
    "    \n",
    "    # Group by the 'Forecast' column\n",
    "    grouped = df.groupby('Forecast')\n",
    "    \n",
    "    # Dictionary to map forecast to custom names\n",
    "    \n",
    "    # Track colors to ensure unique legend entries\n",
    "    color_tracker = {}\n",
    "    for name, group in grouped:\n",
    "        custom_names = {'ECMWF': 'NWP', 'GEFSv12': 'NWP','EMOS': 'Statistical'}\n",
    "        \n",
    "        if ('EX' in name) or ('XGBOOST' in name):\n",
    "            custom_names =  return_name(name)\n",
    "        # break\n",
    "        # Get the first color for the group\n",
    "        color = group['Color'].iloc[0]\n",
    "        # Plot each group, but only add a label if the color is not yet in the tracker\n",
    "        if color not in color_tracker:\n",
    "            plt.plot(group['Week'], group['ACC'], label=custom_names[name], color=color)\n",
    "            color_tracker[color] = custom_names[name]\n",
    "        else:\n",
    "            plt.plot(group['Week'], group['ACC'], color=color)\n",
    "    \n",
    "    # Setting the title and labels\n",
    "    plt.title('ACC by Week and Forecast')\n",
    "    plt.xlabel('Week')\n",
    "    plt.ylabel('ACC')\n",
    "    \n",
    "    # Create the legend\n",
    "    plt.legend(title='Forecast Type')\n",
    "    \n",
    "\n",
    "    plt.savefig(f'{save_dir}/Lineplot_ACC_climpred.png',dpi=300)\n",
    "\n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5479dc-4692-41e9-893b-34707ee0832d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''lineplot_ACC_climpred_average_by_type_with_min_max_plots'''\n",
    "'''Only the single value for all the experiments with the min and maximum values plotted on the final plot'''\n",
    "\n",
    "def lineplot_ACC_climpred_average_by_type_with_min_max_plots(region_name, test_start, test_end):\n",
    "\n",
    "\n",
    "    max_vals = {}\n",
    "    min_vals = {}\n",
    "    \n",
    "    '''This script will take an average of each type of model configuration'''\n",
    "    \n",
    "    save_dir = f'Outputs/ACC_line_plots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    #\n",
    "    leads = [6,13,20,27]\n",
    "    obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=leads), baseline_anomaly.sel(L=leads), baseline_ecmwf.sel(L=leads)\n",
    "    \n",
    "    print('Calculating ACC on GEFS and ECMWF')\n",
    "    \n",
    "    gefs_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    ecmwf_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    diff_nwp = (gefs_acc + ecmwf_acc)/2\n",
    "    df = add_lineplot_to_dataframe(df,diff_nwp,'GEFSv12', 'ACC',10) #Just keep the name the same for later\n",
    "\n",
    "    for l in [6,13,20,27]:\n",
    "\n",
    "        max_ = np.nanmax([gefs_acc[putils.xarray_varname(gefs_acc)].sel(lead=l).max().values, ecmwf_acc[putils.xarray_varname(ecmwf_acc)].sel(lead=l).max().values])\n",
    "        min_ = np.nanmin([gefs_acc[putils.xarray_varname(gefs_acc)].sel(lead=l).min().values, ecmwf_acc[putils.xarray_varname(ecmwf_acc)].sel(lead=l).min().values])\n",
    "\n",
    "        max_vals[f'GEFSv12_Lead{l}'] = max_\n",
    "        min_vals[f'GEFSv12_Lead{l}'] = min_\n",
    "    \n",
    "    print('Adding EMOS results')\n",
    "\n",
    "    #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "    emos_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "    df = add_lineplot_to_dataframe(df,emos_acc,'EMOS', 'ACC',10)\n",
    "\n",
    "    for l in [6,13,20,27]:\n",
    "\n",
    "        max_ = np.nanmax(emos_acc[putils.xarray_varname(emos_acc)].sel(lead=l).max().values)\n",
    "        min_ = np.nanmin(emos_acc[putils.xarray_varname(emos_acc)].sel(lead=l).min().values)\n",
    "\n",
    "        max_vals[f'EMOS_Lead{l}'] = max_\n",
    "        min_vals[f'EMOS_Lead{l}'] = min_\n",
    "        \n",
    "    ####################################################################################################################################\n",
    "\n",
    "    \n",
    "    # #Load the XGBoost data\n",
    "    # for lead in [1,2,3,4]:\n",
    "        \n",
    "    #     max_list = []\n",
    "    #     min_list = []\n",
    "    \n",
    "    #     day_num = (lead*7) -1\n",
    "        \n",
    "    #     print('Working on XGBoost experiments')\n",
    "    #     xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{lead}_testing/*'))\n",
    "    #     len_xg = len(xgboost_files)\n",
    "        \n",
    "    #     add_to_file_OG = ecmwf_acc.sel(lead=day_num).copy()\n",
    "    #     add_to_file_OG.acc[:,:] = 0\n",
    "        \n",
    "    #     #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    #     for i in xgboost_files:\n",
    "    #         # break\n",
    "    #         add_to_file = verifications.load_XGBoost_file_and_make_ensemble_spread(gefs=gefs, file=i, source_of_XGBoost_reforecast='GEFSv12',day_num=day_num, region_name=region_name, test_year=test_year)\n",
    "            \n",
    "    #         xg_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "    #         max_list.append(np.nanmax(xg_acc[putils.xarray_varname(xg_acc)].values))\n",
    "    #         min_list.append(np.nanmin(xg_acc[putils.xarray_varname(xg_acc)].values))\n",
    "            \n",
    "    #         add_to_file_OG  = add_to_file_OG + xg_acc\n",
    "\n",
    "    #     max_vals[f'XGBOOST_Lead{lead}'] = max(max_list)\n",
    "    #     min_vals[f'XGBOOST_Lead{lead}'] = min(min_list)\n",
    "        \n",
    "    #     add_to_file_OG = add_to_file_OG /len_xg\n",
    "    #     #RE ADD MASK\n",
    "    #     add_to_file_OG = xr.where(~np.isnan(ecmwf_acc.sel(lead=day_num)),add_to_file_OG,np.nan)\n",
    "    #     df = add_lineplot_to_dataframe(df,add_to_file_OG,'XGBOOST', 'ACC',lead)\n",
    "\n",
    "\n",
    "    for week_ in [1,2,3,4]:\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_}_testing/*')) #With a specific subset of data\n",
    "\n",
    "        day_num = (week_*7) -1\n",
    "        \n",
    "        keep_files = []\n",
    "        ex = [f'EX{num}' for num in range(28)]\n",
    "        for num in ex:\n",
    "            if num == 'EX26':\n",
    "                pass\n",
    "            elif num in keep_files:\n",
    "                pass\n",
    "            else:\n",
    "                correct_experiments = [j for j in unet_files if num in j]\n",
    "                only_RZSM = [j for j in correct_experiments if 'RZSM' in j]\n",
    "                only_RZSM = [j for j in only_RZSM if 'final' not in j]\n",
    "                if num in ['EX27']:\n",
    "                    only_RZSM = [j for j in only_RZSM if 'ECMWF' not in j]\n",
    "                    \n",
    "                keep_files.append(only_RZSM[0])\n",
    "\n",
    "        \n",
    "        \n",
    "        print('Working on UNET experiments')\n",
    "\n",
    "        black = ['EX0','EX13']\n",
    "        red = ['EX14','EX15','EX16','EX17','EX22','EX23','EX24','EX25']\n",
    "        blue = ['EX1','EX2','EX3','EX4','EX5','EX6','EX7','EX8','EX9','EX10','EX11','EX12',\n",
    "               'EX18','EX19','EX20','EX21','EX27','EX28']\n",
    "\n",
    "        bias_correction_black = [file for file in keep_files if any(black_item in file for black_item in black)]\n",
    "        hybrid_blue = [file for file in keep_files if any(black_item in file for black_item in blue)]\n",
    "        obs_red = [file for file in keep_files if any(black_item in file for black_item in red)]\n",
    "        \n",
    "        #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "        for model,name in zip([bias_correction_black, hybrid_blue, obs_red], ['DM-BC_DL', 'DL-DM', 'DL']):\n",
    "                                    \n",
    "            max_list = []\n",
    "            min_list = []\n",
    "            \n",
    "            add_to_file_OG = ecmwf_acc.sel(lead=day_num).copy()\n",
    "            add_to_file_OG.acc[:,:] = 0\n",
    "\n",
    "            # break\n",
    "            len_unet = len(model)\n",
    "            for i in model:\n",
    "                \n",
    "                if 'ECMWF' in i:\n",
    "                    new_source = 'ECMWF'\n",
    "                else:\n",
    "                    new_source = 'GEFSv12'\n",
    "                    \n",
    "                test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "                ex_name = test_name.split('_')[0]\n",
    "                \n",
    "                add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "                \n",
    "                unet_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "                add_to_file_OG  = add_to_file_OG + unet_acc\n",
    "                \n",
    "                max_list.append(np.nanmax(unet_acc[putils.xarray_varname(unet_acc)].values))\n",
    "                min_list.append(np.nanmin(unet_acc[putils.xarray_varname(unet_acc)].values))\n",
    "\n",
    "            max_vals[f'{name}_Lead{week_}'] = max(max_list)\n",
    "            min_vals[f'{name}_Lead{week_}'] = min(min_list)\n",
    "        \n",
    "            add_to_file_OG = add_to_file_OG /len_unet\n",
    "            #RE ADD MASK\n",
    "            add_to_file_OG = xr.where(~np.isnan(ecmwf_acc.sel(lead=day_num)),add_to_file_OG,np.nan)\n",
    "            df = add_lineplot_to_dataframe_average(df,add_to_file_OG,name, 'ACC',week_)\n",
    "            \n",
    "    \n",
    "    #Get global max and min        \n",
    "    global_max, global_min = max(max_vals.values()), min(min_vals.values())\n",
    "\n",
    "    # Ensure the 'Week' column is sorted to make the line plot correctly\n",
    "    df.sort_values(by=['Forecast', 'Week'], inplace=True)\n",
    "    \n",
    "    def return_name(name):\n",
    "        if name == 'Bias-correction':\n",
    "            name_out = 'Bias-correction'\n",
    "        \n",
    "        elif name == 'Obs.-driven':\n",
    "            name_out = 'Observation driven'\n",
    "        \n",
    "        elif name == 'Hybrid':\n",
    "            name_out =  'Hybrid'\n",
    "    \n",
    "        elif 'XGBOOST' in name:\n",
    "            name_out = 'ML'\n",
    "        \n",
    "        custom_names = {name: name_out}\n",
    "    \n",
    "        return(custom_names)\n",
    "\n",
    "\n",
    "    \n",
    "    # Set the plot size\n",
    "    plt.figure(figsize=(10, 6),dpi=300)\n",
    "    \n",
    "    # Use a clearer and more appealing style\n",
    "    plt.style.use('seaborn-v0_8-colorblind')\n",
    "    \n",
    "    # Custom color palette\n",
    "    palette = plt.get_cmap('tab10')\n",
    "    \n",
    "    # Group by the 'Forecast' column\n",
    "    grouped = df.groupby('Forecast')\n",
    "    \n",
    "    color_tracker = {}\n",
    "    marker_style = ['o', 'v', '^', '<', '>', 's', 'p', '*', '+']\n",
    "    \n",
    "    for i, (name, group) in enumerate(grouped):\n",
    "        custom_names = {\n",
    "            'GEFSv12': 'NWP Raw (2)', 'EMOS': 'EMOS (1)', \n",
    "            'Bias-correction': 'Bias-correction (3)', 'Obs.-driven': 'Obs.-driven (8)',\n",
    "            'Hybrid': 'Hybrid (18)', 'XGBOOST': 'ML (2)'\n",
    "        }\n",
    "        # break\n",
    "    \n",
    "        color = palette(i)\n",
    "        marker = marker_style[i % len(marker_style)]\n",
    "        \n",
    "        if color not in color_tracker:\n",
    "            plt.plot(group['Week'], group['ACC'], label=custom_names[name], color=color, marker=marker, linestyle='-', markersize=5)\n",
    "            color_tracker[color] = custom_names[name]\n",
    "        else:\n",
    "            plt.plot(group['Week'], group['ACC'], color=color, marker=marker, linestyle='-', markersize=5)\n",
    "\n",
    "        all_lead_vals_max =  [i for i in max_vals.keys() if name in i]\n",
    "        all_lead_vals_min =  [i for i in max_vals.keys() if name in i]\n",
    "\n",
    "        max_vals_out = [max_vals[name] for name in all_lead_vals_max]\n",
    "        min_vals_out = [min_vals[name] for name in all_lead_vals_min]\n",
    "        \n",
    "        plt.fill_between(group['Week'], min_vals_out, max_vals_out, color=color, alpha=0.1)\n",
    "        \n",
    "    # Add a horizontal line at y=0.5\n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--', linewidth=1)\n",
    "    \n",
    "    # Setting the title and labels with increased font sizes\n",
    "    plt.title('ACC by Model and Week Lead', fontsize=16)\n",
    "    plt.xlabel('Week Lead', fontsize=14)\n",
    "    plt.ylabel('ACC', fontsize=14)\n",
    "    \n",
    "    # Improve readability by adjusting tick parameters\n",
    "    plt.xticks([1, 2, 3, 4], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # Add a grid for better readability\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Create the legend with a slightly larger font size\n",
    "    plt.legend(title='Forecast (# models)', fontsize=12, title_fontsize=13)\n",
    "    \n",
    "    # Optionally, set a tight layout to ensure everything fits without overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f'{save_dir}/Lineplot_ACC_average_climpred_improved_with_min_max.png', dpi=300)\n",
    "        \n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d920fbb-80f5-42ad-9591-e401cbbd2238",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''lineplot_ACC_climpred_average_by_type_with_min_max_std_deviation_plots'''\n",
    "'''Only the single value for all the experiments with the min and maximum values plotted on the final plot'''\n",
    "\n",
    "def lineplot_ACC_climpred_average_by_type_with_min_max_std_deviation_plots(region_name, test_start, test_end, num_std):\n",
    "\n",
    "\n",
    "    std_max_vals = {}\n",
    "    std_min_vals = {}\n",
    "    \n",
    "    '''This script will take an average of each type of model configuration'''\n",
    "    \n",
    "    save_dir = f'Outputs/ACC_line_plots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    #\n",
    "    leads = [6,13,20,27]\n",
    "    obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=leads), baseline_anomaly.sel(L=leads), baseline_ecmwf.sel(L=leads)\n",
    "    \n",
    "    print('Calculating ACC on GEFS and ECMWF')\n",
    "    \n",
    "    gefs_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    ecmwf_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    diff_nwp = (gefs_acc + ecmwf_acc)/2\n",
    "    df = add_lineplot_to_dataframe(df,diff_nwp,'GEFSv12', 'ACC',10) #Just keep the name the same for later\n",
    "\n",
    "    for idx,l in enumerate([6,13,20,27]):\n",
    "\n",
    "        std_ = np.nanmean([np.nanstd(gefs_acc[putils.xarray_varname(gefs_acc)].sel(lead=l).values), np.nanstd(ecmwf_acc[putils.xarray_varname(ecmwf_acc)].sel(lead=l).values)])\n",
    "        mean_ = np.nanmean([np.nanmean(gefs_acc[putils.xarray_varname(gefs_acc)].sel(lead=l).values), np.nanmean(ecmwf_acc[putils.xarray_varname(ecmwf_acc)].sel(lead=l).values)])\n",
    "        \n",
    "\n",
    "        std_max_vals[f'GEFSv12_Lead{idx+1}'] = mean_ + std_*num_std\n",
    "        std_min_vals[f'GEFSv12_Lead{idx+1}'] = mean_ - std_*num_std\n",
    "    \n",
    "    print('Adding EMOS results')\n",
    "\n",
    "    #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "    emos_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "    df = add_lineplot_to_dataframe(df,emos_acc,'EMOS', 'ACC',10)\n",
    "\n",
    "    for idx,l in enumerate([6,13,20,27]):\n",
    "\n",
    "        std_ = np.nanmean(np.nanstd(emos_acc[putils.xarray_varname(emos_acc)].sel(lead=l).values))\n",
    "        mean_ = np.nanmean(np.nanmean(emos_acc[putils.xarray_varname(emos_acc)].sel(lead=l).values))\n",
    "        \n",
    "\n",
    "        std_max_vals[f'EMOS_Lead{idx+1}'] = mean_ + std_*num_std\n",
    "        std_min_vals[f'EMOS_Lead{idx+1}'] = mean_ - std_*num_std\n",
    "        \n",
    "    ####################################################################################################################################\n",
    "\n",
    "    \n",
    "    # #Load the XGBoost data\n",
    "    # for lead in [1,2,3,4]:\n",
    "        \n",
    "    #     max_list = []\n",
    "    #     min_list = []\n",
    "    \n",
    "    #     day_num = (lead*7) -1\n",
    "        \n",
    "    #     print('Working on XGBoost experiments')\n",
    "    #     xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{lead}_testing/*'))\n",
    "    #     len_xg = len(xgboost_files)\n",
    "        \n",
    "    #     add_to_file_OG = ecmwf_acc.sel(lead=day_num).copy()\n",
    "    #     add_to_file_OG.acc[:,:] = 0\n",
    "        \n",
    "    #     #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    #     for i in xgboost_files:\n",
    "    #         # break\n",
    "    #         add_to_file = gefs.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order).copy(deep = True)\n",
    "    #         # break\n",
    "    #         #Still working here\n",
    "    #         test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "    #         load_ = np.expand_dims(np.load(i),-1)\n",
    "    #         load_.shape\n",
    "    #         load_ = np.where(load_ == 0,np.nan,load_)\n",
    "            \n",
    "    #         add_realizations = np.empty(shape=(load_.shape[0],11,load_.shape[1],load_.shape[2],load_.shape[3])) #This will help with climpred functions\n",
    "    #         add_realizations.shape #shape is (104, 11, 48, 96, 1)\n",
    "            \n",
    "    #         for j in range(11):\n",
    "    #             add_realizations[:,j,:,:,:] = load_\n",
    "                \n",
    "    #         add_realizations = verifications.reverse_min_max_scaling(add_realizations, region_name, day_num)\n",
    "    #         add_realizations.shape\n",
    "            \n",
    "    #         add_realizations =  np.reshape(add_realizations,(add_realizations.shape[0], add_realizations.shape[1], add_realizations.shape[-1], add_realizations.shape[2], add_realizations.shape[3])) #Reshape to match gefs/ecmwf\n",
    "    #         add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = add_realizations\n",
    "            \n",
    "    #         xg_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    #         std_ = np.nanmean(np.nanstd(xg_acc[putils.xarray_varname(xg_acc)].values))\n",
    "    #         mean_ = np.nanmean(np.nanmean(xg_acc[putils.xarray_varname(xg_acc)].values))\n",
    "\n",
    "    #         max_list.append(mean_ + std_*num_std)\n",
    "    #         min_list.append(mean_ - std_*num_std)\n",
    "            \n",
    "    #         add_to_file_OG  = add_to_file_OG + xg_acc\n",
    "\n",
    "    #     std_max_vals[f'XGBOOST_Lead{lead}'] = max(max_list)\n",
    "    #     std_min_vals[f'XGBOOST_Lead{lead}'] = min(min_list)\n",
    "        \n",
    "    #     add_to_file_OG = add_to_file_OG /len_xg\n",
    "    #     #RE ADD MASK\n",
    "    #     add_to_file_OG = xr.where(~np.isnan(ecmwf_acc.sel(lead=day_num)),add_to_file_OG,np.nan)\n",
    "    #     df = add_lineplot_to_dataframe(df,add_to_file_OG,'XGBOOST', 'ACC',lead)\n",
    "\n",
    "\n",
    "    for week_ in [1,2,3,4]:\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_}_testing/*')) #With a specific subset of data\n",
    "\n",
    "        day_num = (week_*7) -1\n",
    "        \n",
    "        keep_files = []\n",
    "        ex = [f'EX{num}' for num in range(28)]\n",
    "        for num in ex:\n",
    "            if num == 'EX26':\n",
    "                pass\n",
    "            elif num in keep_files:\n",
    "                pass\n",
    "            else:\n",
    "                correct_experiments = [j for j in unet_files if num in j]\n",
    "                only_RZSM = [j for j in correct_experiments if 'RZSM' in j]\n",
    "                only_RZSM = [j for j in only_RZSM if 'final' not in j]\n",
    "                if num in ['EX27']:\n",
    "                    only_RZSM = [j for j in only_RZSM if 'ECMWF' not in j]\n",
    "                    \n",
    "                keep_files.append(only_RZSM[0])\n",
    "\n",
    "        \n",
    "        \n",
    "        print('Working on UNET experiments')\n",
    "\n",
    "        black = ['EX0','EX13']\n",
    "        red = ['EX14','EX15','EX16','EX17','EX22','EX23','EX24','EX25']\n",
    "        blue = ['EX1','EX2','EX3','EX4','EX5','EX6','EX7','EX8','EX9','EX10','EX11','EX12',\n",
    "               'EX18','EX19','EX20','EX21','EX27','EX28']\n",
    "\n",
    "        bias_correction_black = [file for file in keep_files if any(black_item in file for black_item in black)]\n",
    "        hybrid_blue = [file for file in keep_files if any(black_item in file for black_item in blue)]\n",
    "        obs_red = [file for file in keep_files if any(black_item in file for black_item in red)]\n",
    "        \n",
    "        #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "        for model,name in zip([bias_correction_black, hybrid_blue, obs_red], ['DL_NWP_BC', 'DL_NWP_OBS', 'DL_OBS']):\n",
    "                                    \n",
    "            max_list = []\n",
    "            min_list = []\n",
    "            \n",
    "            add_to_file_OG = ecmwf_acc.sel(lead=day_num).copy()\n",
    "            add_to_file_OG.acc[:,:] = 0\n",
    "\n",
    "            # break\n",
    "            len_unet = len(model)\n",
    "            for i in model:\n",
    "                if 'ECMWF' in i:\n",
    "                    new_source = 'ECMWF'\n",
    "                else:\n",
    "                    new_source = 'GEFSv12'\n",
    "                \n",
    "                add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "                \n",
    "                unet_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "                add_to_file_OG  = add_to_file_OG + unet_acc\n",
    "                \n",
    "                std_ = np.nanmean(np.nanstd(unet_acc[putils.xarray_varname(unet_acc)].values))\n",
    "                mean_ = np.nanmean(np.nanmean(unet_acc[putils.xarray_varname(unet_acc)].values))\n",
    "    \n",
    "                max_list.append(mean_ + std_*num_std)\n",
    "                min_list.append(mean_ - std_*num_std)\n",
    "\n",
    "            std_max_vals[f'{name}_Lead{week_}'] = max(max_list)\n",
    "            std_min_vals[f'{name}_Lead{week_}'] = min(min_list)\n",
    "        \n",
    "            add_to_file_OG = add_to_file_OG /len_unet\n",
    "            #RE ADD MASK\n",
    "            add_to_file_OG = xr.where(~np.isnan(ecmwf_acc.sel(lead=day_num)),add_to_file_OG,np.nan)\n",
    "            df = add_lineplot_to_dataframe_average(df,add_to_file_OG,name, 'ACC',week_)\n",
    "            \n",
    "    \n",
    "    #Get global max and min        \n",
    "    global_max, global_min = max(std_max_vals.values()), min(std_min_vals.values())\n",
    "\n",
    "    # Ensure the 'Week' column is sorted to make the line plot correctly\n",
    "    df.sort_values(by=['Forecast', 'Week'], inplace=True)\n",
    "    \n",
    "\n",
    "    ################         PLOT          ###########################\n",
    "\n",
    "    # Set the plot size\n",
    "    plt.figure(figsize=(10, 6),dpi=300)\n",
    "    \n",
    "    # Use a clearer and more appealing style\n",
    "    plt.style.use('seaborn-v0_8-colorblind')\n",
    "    \n",
    "    # Custom color palette\n",
    "    palette = plt.get_cmap('tab10')\n",
    "    \n",
    "    # Group by the 'Forecast' column\n",
    "    grouped = df.groupby('Forecast')\n",
    "    \n",
    "    color_tracker = {}\n",
    "    marker_style = ['o', 'v', '^', '<', '>', 's', 'p', '*', '+']\n",
    "    \n",
    "    for i, (name, group) in enumerate(grouped):\n",
    "\n",
    "        # break\n",
    "    \n",
    "        color = palette(i)\n",
    "        marker = marker_style[i % len(marker_style)]\n",
    "        \n",
    "        if color not in color_tracker:\n",
    "            plt.plot(group['Week'], group['ACC'], label=custom_names[name], color=color, marker=marker, linestyle='-', markersize=5)\n",
    "            color_tracker[color] = custom_names[name]\n",
    "        else:\n",
    "            plt.plot(group['Week'], group['ACC'], color=color, marker=marker, linestyle='-', markersize=5)\n",
    "\n",
    "        all_lead_vals_max =  [i for i in std_max_vals.keys() if name in i]\n",
    "        all_lead_vals_min =  [i for i in std_min_vals.keys() if name in i]\n",
    "\n",
    "        # sorted(all_lead_vals_max)\n",
    "        \n",
    "\n",
    "        max_vals_out = [std_max_vals[name] for name in all_lead_vals_max]\n",
    "        min_vals_out = [std_min_vals[name] for name in all_lead_vals_min]\n",
    "        \n",
    "        plt.fill_between(group['Week'], min_vals_out, max_vals_out, color=color, alpha=0.1)\n",
    "        \n",
    "    # Add a horizontal line at y=0.5\n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--', linewidth=1)\n",
    "    \n",
    "    # Setting the title and labels with increased font sizes\n",
    "    plt.title('ACC by Model and Week Lead', fontsize=16)\n",
    "    plt.xlabel('Week Lead', fontsize=14)\n",
    "    plt.ylabel('ACC', fontsize=14)\n",
    "    \n",
    "    # Improve readability by adjusting tick parameters\n",
    "    plt.xticks([1, 2, 3, 4], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # Add a grid for better readability\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Create the legend with a slightly larger font size\n",
    "    plt.legend(title='Forecast (# models)', fontsize=12, title_fontsize=13)\n",
    "    \n",
    "    # Optionally, set a tight layout to ensure everything fits without overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f'{save_dir}/Lineplot_ACC_average_climpred_improved_within_{num_std}_standard_deviation.png', dpi=300)\n",
    "        \n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa651a0-c751-4932-8562-24adc9d04ff7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''lineplot_ACC_climpred_average_by_type_with_min_max_std_deviation_plots'''\n",
    "'''Only the single value for all the experiments with the min and maximum values plotted on the final plot'''\n",
    "\n",
    "def lineplot_ACC_climpred_average_by_type_with_min_max_std_deviation_plots_as_error_bars(region_name, test_start, test_end, num_std):\n",
    "\n",
    "\n",
    "    std_max_vals = {}\n",
    "    std_min_vals = {}\n",
    "    \n",
    "    '''This script will take an average of each type of model configuration'''\n",
    "    \n",
    "    save_dir = f'Outputs/ACC_line_plots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    #\n",
    "    leads = [6,13,20,27]\n",
    "    obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=leads), baseline_anomaly.sel(L=leads), baseline_ecmwf.sel(L=leads)\n",
    "    \n",
    "    print('Calculating ACC on GEFS and ECMWF')\n",
    "    \n",
    "    gefs_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    ecmwf_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    diff_nwp = (gefs_acc + ecmwf_acc)/2\n",
    "    df = add_lineplot_to_dataframe(df,diff_nwp,'GEFSv12', 'ACC',10) #Just keep the name the same for later\n",
    "\n",
    "    for idx,l in enumerate([6,13,20,27]):\n",
    "\n",
    "        std_ = np.nanmean([np.nanstd(gefs_acc[putils.xarray_varname(gefs_acc)].sel(lead=l).values), np.nanstd(ecmwf_acc[putils.xarray_varname(ecmwf_acc)].sel(lead=l).values)])\n",
    "        mean_ = np.nanmean([np.nanmean(gefs_acc[putils.xarray_varname(gefs_acc)].sel(lead=l).values), np.nanmean(ecmwf_acc[putils.xarray_varname(ecmwf_acc)].sel(lead=l).values)])\n",
    "        \n",
    "\n",
    "        std_max_vals[f'GEFSv12_Lead{idx+1}'] = mean_ + std_*num_std\n",
    "        std_min_vals[f'GEFSv12_Lead{idx+1}'] = mean_ - std_*num_std\n",
    "    \n",
    "    print('Adding EMOS results')\n",
    "\n",
    "    #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "    emos_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "    df = add_lineplot_to_dataframe(df,emos_acc,'EMOS', 'ACC',10)\n",
    "\n",
    "    for idx,l in enumerate([6,13,20,27]):\n",
    "\n",
    "        std_ = np.nanmean(np.nanstd(emos_acc[putils.xarray_varname(emos_acc)].sel(lead=l).values))\n",
    "        mean_ = np.nanmean(np.nanmean(emos_acc[putils.xarray_varname(emos_acc)].sel(lead=l).values))\n",
    "        \n",
    "\n",
    "        std_max_vals[f'EMOS_Lead{idx+1}'] = mean_ + std_*num_std\n",
    "        std_min_vals[f'EMOS_Lead{idx+1}'] = mean_ - std_*num_std\n",
    "        \n",
    "    ####################################################################################################################################\n",
    "\n",
    "    \n",
    "    # #Load the XGBoost data\n",
    "    # for lead in [1,2,3,4]:\n",
    "        \n",
    "    #     max_list = []\n",
    "    #     min_list = []\n",
    "    \n",
    "    #     day_num = (lead*7) -1\n",
    "        \n",
    "    #     print('Working on XGBoost experiments')\n",
    "    #     xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{lead}_testing/*'))\n",
    "    #     len_xg = len(xgboost_files)\n",
    "        \n",
    "    #     add_to_file_OG = ecmwf_acc.sel(lead=day_num).copy()\n",
    "    #     add_to_file_OG.acc[:,:] = 0\n",
    "        \n",
    "    #     #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    #     for i in xgboost_files:\n",
    "    #         # break\n",
    "    #         add_to_file = gefs.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order).copy(deep = True)\n",
    "    #         # break\n",
    "    #         #Still working here\n",
    "    #         test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "    #         load_ = np.expand_dims(np.load(i),-1)\n",
    "    #         load_.shape\n",
    "    #         load_ = np.where(load_ == 0,np.nan,load_)\n",
    "            \n",
    "    #         add_realizations = np.empty(shape=(load_.shape[0],11,load_.shape[1],load_.shape[2],load_.shape[3])) #This will help with climpred functions\n",
    "    #         add_realizations.shape #shape is (104, 11, 48, 96, 1)\n",
    "            \n",
    "    #         for j in range(11):\n",
    "    #             add_realizations[:,j,:,:,:] = load_\n",
    "                \n",
    "    #         add_realizations = verifications.reverse_min_max_scaling(add_realizations, region_name, day_num)\n",
    "    #         add_realizations.shape\n",
    "            \n",
    "    #         add_realizations =  np.reshape(add_realizations,(add_realizations.shape[0], add_realizations.shape[1], add_realizations.shape[-1], add_realizations.shape[2], add_realizations.shape[3])) #Reshape to match gefs/ecmwf\n",
    "    #         add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = add_realizations\n",
    "            \n",
    "    #         xg_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    #         std_ = np.nanmean(np.nanstd(xg_acc[putils.xarray_varname(xg_acc)].values))\n",
    "    #         mean_ = np.nanmean(np.nanmean(xg_acc[putils.xarray_varname(xg_acc)].values))\n",
    "\n",
    "    #         max_list.append(mean_ + std_*num_std)\n",
    "    #         min_list.append(mean_ - std_*num_std)\n",
    "            \n",
    "    #         add_to_file_OG  = add_to_file_OG + xg_acc\n",
    "\n",
    "    #     std_max_vals[f'XGBOOST_Lead{lead}'] = max(max_list)\n",
    "    #     std_min_vals[f'XGBOOST_Lead{lead}'] = min(min_list)\n",
    "        \n",
    "    #     add_to_file_OG = add_to_file_OG /len_xg\n",
    "    #     #RE ADD MASK\n",
    "    #     add_to_file_OG = xr.where(~np.isnan(ecmwf_acc.sel(lead=day_num)),add_to_file_OG,np.nan)\n",
    "    #     df = add_lineplot_to_dataframe(df,add_to_file_OG,'XGBOOST', 'ACC',lead)\n",
    "\n",
    "\n",
    "    for week_ in [1,2,3,4]:\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_}_testing/*')) #With a specific subset of data\n",
    "\n",
    "        day_num = (week_*7) -1\n",
    "        \n",
    "        keep_files = []\n",
    "        ex = [f'EX{num}' for num in range(28)]\n",
    "        for num in ex:\n",
    "            if num == 'EX26':\n",
    "                pass\n",
    "            elif num in keep_files:\n",
    "                pass\n",
    "            else:\n",
    "                correct_experiments = [j for j in unet_files if num in j]\n",
    "                only_RZSM = [j for j in correct_experiments if 'RZSM' in j]\n",
    "                only_RZSM = [j for j in only_RZSM if 'final' not in j]\n",
    "                if num in ['EX27']:\n",
    "                    only_RZSM = [j for j in only_RZSM if 'ECMWF' not in j]\n",
    "                    \n",
    "                keep_files.append(only_RZSM[0])\n",
    "\n",
    "        \n",
    "        \n",
    "        print('Working on UNET experiments')\n",
    "\n",
    "        black = ['EX0','EX13']\n",
    "        red = ['EX14','EX15','EX16','EX17','EX22','EX23','EX24','EX25']\n",
    "        blue = ['EX1','EX2','EX3','EX4','EX5','EX6','EX7','EX8','EX9','EX10','EX11','EX12',\n",
    "               'EX18','EX19','EX20','EX21','EX27','EX28']\n",
    "\n",
    "        bias_correction_black = [file for file in keep_files if any(black_item in file for black_item in black)]\n",
    "        hybrid_blue = [file for file in keep_files if any(black_item in file for black_item in blue)]\n",
    "        obs_red = [file for file in keep_files if any(black_item in file for black_item in red)]\n",
    "        \n",
    "        #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "        for model,name in zip([bias_correction_black, hybrid_blue, obs_red], ['DL_NWP_BC', 'DL_NWP_OBS', 'DL_OBS']):\n",
    "                                    \n",
    "            max_list = []\n",
    "            min_list = []\n",
    "            \n",
    "            add_to_file_OG = ecmwf_acc.sel(lead=day_num).copy()\n",
    "            add_to_file_OG.acc[:,:] = 0\n",
    "\n",
    "            # break\n",
    "            len_unet = len(model)\n",
    "            for i in model:\n",
    "                if 'ECMWF' in i:\n",
    "                    new_source = 'ECMWF'\n",
    "                else:\n",
    "                    new_source = 'GEFSv12'\n",
    "                \n",
    "                add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "                \n",
    "                unet_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "                add_to_file_OG  = add_to_file_OG + unet_acc\n",
    "                \n",
    "                std_ = np.nanmean(np.nanstd(unet_acc[putils.xarray_varname(unet_acc)].values))\n",
    "                mean_ = np.nanmean(np.nanmean(unet_acc[putils.xarray_varname(unet_acc)].values))\n",
    "    \n",
    "                max_list.append(mean_ + std_*num_std)\n",
    "                min_list.append(mean_ - std_*num_std)\n",
    "\n",
    "            std_max_vals[f'{name}_Lead{week_}'] = max(max_list)\n",
    "            std_min_vals[f'{name}_Lead{week_}'] = min(min_list)\n",
    "        \n",
    "            add_to_file_OG = add_to_file_OG /len_unet\n",
    "            #RE ADD MASK\n",
    "            add_to_file_OG = xr.where(~np.isnan(ecmwf_acc.sel(lead=day_num)),add_to_file_OG,np.nan)\n",
    "            df = add_lineplot_to_dataframe_average(df,add_to_file_OG,name, 'ACC',week_)\n",
    "            \n",
    "    \n",
    "    #Get global max and min        \n",
    "    global_max, global_min = max(std_max_vals.values()), min(std_min_vals.values())\n",
    "\n",
    "    # Ensure the 'Week' column is sorted to make the line plot correctly\n",
    "    df.sort_values(by=['Forecast', 'Week'], inplace=True)\n",
    "\n",
    "    # Set the plot size\n",
    "    plt.figure(figsize=(10, 6),dpi=300)\n",
    "    \n",
    "    # Use a clearer and more appealing style\n",
    "    plt.style.use('seaborn-v0_8-colorblind')\n",
    "    \n",
    "    # Custom color palette\n",
    "    palette = plt.get_cmap('tab10')\n",
    "    \n",
    "    # Group by the 'Forecast' column\n",
    "    grouped = df.groupby('Forecast')\n",
    "    \n",
    "    color_tracker = {}\n",
    "    marker_style = ['o', 'v', '^', '<', '>', 's', 'p', '*', '+']\n",
    "    \n",
    "    for i, (name, group) in enumerate(grouped):\n",
    "\n",
    "        # break\n",
    "    \n",
    "        color = palette(i)\n",
    "        marker = marker_style[i % len(marker_style)]\n",
    "        \n",
    "        if color not in color_tracker:\n",
    "            plt.plot(group['Week'], group['ACC'], label=custom_names[name], color=color, marker=marker, linestyle='-', markersize=5)\n",
    "            color_tracker[color] = custom_names[name]\n",
    "        else:\n",
    "            plt.plot(group['Week'], group['ACC'], color=color, marker=marker, linestyle='-', markersize=5)\n",
    "\n",
    "        all_lead_vals_max =  [i for i in std_max_vals.keys() if name in i]\n",
    "        all_lead_vals_min =  [i for i in std_min_vals.keys() if name in i]\n",
    "\n",
    "        # sorted(all_lead_vals_max)\n",
    "        \n",
    "\n",
    "        max_vals_out = [std_max_vals[name] for name in all_lead_vals_max]\n",
    "        min_vals_out = [std_min_vals[name] for name in all_lead_vals_min]\n",
    "        \n",
    "        # plt.fill_between(group['Week'], min_vals_out, max_vals_out, color=color, alpha=0.1)\n",
    "        plt.errorbar(group['Week'], group['ACC'], yerr=max_vals_out-group['ACC'], fmt='-o', capsize=5, capthick=1, ecolor=color)\n",
    "        \n",
    "    # Add a horizontal line at y=0.5\n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--', linewidth=1)\n",
    "    \n",
    "    # Setting the title and labels with increased font sizes\n",
    "    plt.title('ACC by Model and Week Lead', fontsize=16)\n",
    "    plt.xlabel('Week Lead', fontsize=14)\n",
    "    plt.ylabel('ACC', fontsize=14)\n",
    "    \n",
    "    # Improve readability by adjusting tick parameters\n",
    "    plt.xticks([1, 2, 3, 4], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # Add a grid for better readability\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Create the legend with a slightly larger font size\n",
    "    plt.legend(title='Forecast (# models)', fontsize=12, title_fontsize=13)\n",
    "    \n",
    "    # Optionally, set a tight layout to ensure everything fits without overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f'{save_dir}/Lineplot_ACC_average_climpred_improved_within_{num_std}_standard_deviation_as_error_bar.png', dpi=300)\n",
    "        \n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a4a2fb-cc6b-426a-b0d8-b1e79e63c50f",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d3d31-b3f9-4b13-b9e0-78d5de1795ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "lineplot_ACC_climpred_with_XGBOOST(region_name, test_start, test_end)\n",
    "lineplot_ACC_climpred_average_by_type(region_name, test_start, test_end)\n",
    "\n",
    "'''Line plots with mean ACC and min/max plots'''\n",
    "lineplot_ACC_climpred_average_by_type_with_min_max_plots(region_name, test_start, test_end)\n",
    "\n",
    "''' Line plots with mean ACC and 1 standard deviation around mean'''\n",
    "#Removed XGBOOST from plots\n",
    "lineplot_ACC_climpred_average_by_type_with_min_max_std_deviation_plots(region_name, test_start, test_end,num_std=1)\n",
    "lineplot_ACC_climpred_average_by_type_with_min_max_std_deviation_plots(region_name, test_start, test_end,num_std=2)\n",
    "\n",
    "'''ACC with std and error bars as the shading'''\n",
    "lineplot_ACC_climpred_average_by_type_with_min_max_std_deviation_plots_as_error_bars(region_name, test_start, test_end,num_std=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1f989-27e7-4519-a46a-dc663cec308d",
   "metadata": {},
   "source": [
    "# Spatial CRPS CONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09423255-1490-4617-9da5-c4e0a6b54ff1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def spatial_CRPS_climpred_with_EMOS(week_lead, region_name, test_start, test_end, single_experiment_or_all_experiments):\n",
    "\n",
    "    save_dir = f'Outputs/CRPS_spatial_plots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    plot_dict = {}\n",
    "    \n",
    "    day_num = (week_lead*7) -1\n",
    "\n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    #For a single lead\n",
    "    obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_anomaly.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_ecmwf.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order)\n",
    "\n",
    "    print('Calculating CRPS on GEFS and ECMWF')\n",
    "    gefs_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "    # gefs_BC_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(additive_bias_gefs_final.sel(L=day_num).expand_dims({'L':1})), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    ecmwf_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "    # ecmwf_BC_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(additive_bias_ecmwf_final.sel(L=day_num).expand_dims({'L':1})), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    plot_dict.update({'ECMWF':ecmwf_acc})\n",
    "    plot_dict.update({'GEFSv12':gefs_acc})\n",
    "    \n",
    "    ####################################################################################################################################\n",
    "\n",
    "    print('Adding EMOS results')\n",
    "    #Now add EMOS results\n",
    "    if region_name == 'CONUS':\n",
    "        #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "        emos_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "        emos_acc = emos_acc.sel(lead=day_num)\n",
    "        \n",
    "    plot_dict.update({'EMOS':emos_acc})\n",
    "\n",
    "    if single_experiment_or_all_experiments == 'all':\n",
    "        #Now for all predictions from UNET, make the ACC\n",
    "        #Can add *denseLar* for only the dense ones\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*')) #With a specific subset of data\n",
    "    else:\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{single_experiment_or_all_experiments}*')) #Will all data\n",
    "\n",
    "    #Get the ECMWF file\n",
    "    ec_unet = [i for i in unet_files if 'ECMWF' in i][0]\n",
    "    gef_unet = [i for i in unet_files if f'{single_experiment_or_all_experiments}_regular_RZSM' in i][0]\n",
    "    \n",
    "    print('Working on UNET experiments')\n",
    "    unet_files = [ec_unet,gef_unet]\n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in unet_files:\n",
    "        # break\n",
    "        test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "        add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "        unet_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "        plot_dict.update({test_name:unet_acc})\n",
    "    \n",
    "    #Load the XGBoost data\n",
    "    if region_name == 'CONUS':\n",
    "        print('Working on XGBoost experiments')\n",
    "        xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{week_lead}_testing/*'))\n",
    "    \n",
    "        #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "        for i in xgboost_files:\n",
    "            # break\n",
    "            test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "            add_to_file = verifications.load_XGBoost_file_and_make_ensemble_spread(gefs=gefs, file=i, source_of_XGBoost_reforecast='GEFSv12',day_num=day_num, region_name=region_name, test_year=test_year)\n",
    "            #has values\n",
    "            xg_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "            plot_dict.update({'XGBoost':xg_acc})\n",
    "    \n",
    "    #Get global max and min\n",
    "    global_max, global_min = verifications.global_max_min(plot_dict, 'crps')\n",
    "\n",
    "    global_max = 0.05\n",
    "    cmap = 'bwr'\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows = 2, ncols= 3, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(10, 7), dpi=300)\n",
    "    \n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    lon = mask.X.values\n",
    "    lat = mask.Y.values\n",
    "    \n",
    "    axs_start = 0\n",
    "    for model in plot_dict.keys():\n",
    "        # break\n",
    "        data = plot_dict[model][putils.xarray_varname(plot_dict[model])].mean(dim='init').values\n",
    "        v = np.linspace(global_min, global_max, 20, endpoint=True)\n",
    "\n",
    "        map = Basemap(projection='cyl', llcrnrlat=lat[-1] - 1.5, urcrnrlat=lat[0],\n",
    "                      llcrnrlon=(lon[0] - 360 - 6), urcrnrlon=(lon[-1] - 360 + 20), resolution='l')\n",
    "        \n",
    "        x, y = map(*np.meshgrid(lon, lat))\n",
    "        # Adjust the text coordinates based on the actual data coordinates\n",
    "\n",
    "        im = axs[axs_start].contourf(x, y, data, levels=v, extend='both',\n",
    "                              transform=ccrs.PlateCarree(), cmap=cmap)\n",
    "        \n",
    "        gl = axs[axs_start].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                   linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "        gl.xlabels_top = False\n",
    "        gl.ylabels_right = False\n",
    "        gl.ylabels_left = True\n",
    "        gl.xformatter = LongitudeFormatter()\n",
    "        gl.yformatter = LatitudeFormatter()\n",
    "        axs[axs_start].coastlines()\n",
    "        axs[axs_start].set_aspect('equal')  # this makes the plots better\n",
    "        axs[axs_start].set_title(f'{model} Week {week_lead}',fontsize=8)\n",
    "        axs_start+=1\n",
    "    \n",
    "    cbar_ax = fig.add_axes([0.05, 0.03, .9, .04])\n",
    "    \n",
    "    # Draw the colorbar\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    plt.suptitle(f'CRPS on testing Dataset', fontsize=20)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f'{save_dir}/Wk{week_lead}_CRPS_climpred.png')\n",
    "\n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28a7152-ba3b-408b-8921-a5c33565ef94",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11bfe5c-16a3-4735-af36-9ace713bd73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We only ran EMOS, XGBoost on CONUS'''\n",
    "for week_lead in [1,2,3,4,5]:\n",
    "    if region_name == 'CONUS':\n",
    "        spatial_CRPS_climpred_with_EMOS(week_lead, region_name, test_start, test_end, single_experiment_or_all_experiments='EX27') #cannot do XGBoost, no ensemble made\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54529d47-c8c2-42b7-9ad4-d037c083470c",
   "metadata": {},
   "source": [
    "# Spatial CRPSS CONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c01120-1828-4faf-99c7-3b22598a0aed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def spatial_CRPSS_climpred_with_EMOS(week_lead, region_name, test_start, test_end, single_experiment_or_all_experiments):\n",
    "\n",
    "#     save_dir = f'Outputs/CRPSS_spatial_plots/{region_name}'\n",
    "#     os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "#     plot_dict = {}\n",
    "    \n",
    "#     day_num = (week_lead*7) -1\n",
    "\n",
    "#     print('Loading observation and baseline anomaly files')\n",
    "#     #For a single lead\n",
    "#     obs, gefs, ecmwf, var_OUT_overwrite, template_testing_only_by_lead= select_data_by_lead(obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only, day_num)\n",
    "\n",
    "#     print('Calculating CRPSS on GEFS and ECMWF')\n",
    "#     gefs_acc = verifications.create_climpred_crpss_skill_score(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "#     # gefs_BC_acc = verifications.create_climpred_crpss_skill_score(verifications.rename_subx_for_climpred(additive_bias_gefs_final.sel(L=day_num).expand_dims({'L':1})), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "#     ecmwf_acc = verifications.create_climpred_crpss_skill_score(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "#     # ecmwf_BC_acc = verifications.create_climpred_crpss_skill_score(verifications.rename_subx_for_climpred(additive_bias_ecmwf_final.sel(L=day_num).expand_dims({'L':1})), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "#     plot_dict.update({'ECMWF':ecmwf_acc})\n",
    "#     plot_dict.update({'GEFSv12':gefs_acc})\n",
    "    \n",
    "#     ####################################################################################################################################\n",
    "\n",
    "#     print('Adding EMOS results')\n",
    "#     #Now add EMOS results\n",
    "#     if region_name == 'CONUS':\n",
    "#         #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "#         emos_acc = verifications.create_climpred_crpss_skill_score(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original))\n",
    "#         emos_acc = emos_acc.sel(lead=day_num)\n",
    "        \n",
    "#     plot_dict.update({'EMOS':emos_acc})\n",
    "\n",
    "#     if single_experiment_or_all_experiments == 'all':\n",
    "#         #Now for all predictions from UNET, make the ACC\n",
    "#         #Can add *denseLar* for only the dense ones\n",
    "#         unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*')) #With a specific subset of data\n",
    "#     else:\n",
    "#         unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{single_experiment_or_all_experiments}*')) #Will all data\n",
    "\n",
    "#     #Get the ECMWF file\n",
    "#     ec_unet = [i for i in unet_files if 'ECMWF' in i][0]\n",
    "#     gef_unet = [i for i in unet_files if f'{single_experiment_or_all_experiments}_regular_RZSM' in i][0]\n",
    "    \n",
    "#     print('Working on UNET experiments')\n",
    "#     unet_files = [ec_unet,gef_unet]\n",
    "#     #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "#     for i in unet_files:\n",
    "#         # break\n",
    "#         add_to_file = verifications.load_UNET_files(gefs=gefs, file=i, region_name=region_name, day_num=day_num,new_source=new_source,test_year=test_year)\n",
    "#         test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "#         unet_acc = verifications.create_climpred_crpss_skill_score(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "#         plot_dict.update({test_name:unet_acc})\n",
    "    \n",
    "\n",
    "    \n",
    "#     #Get global max and min\n",
    "#     global_max, global_min = verifications.global_max_min(plot_dict, 'crpss')\n",
    "    \n",
    "#     v = np.linspace(-3, 1, 20, endpoint=True)\n",
    "#     pos_vals = [i for i in v if i >0]\n",
    "#     neg_vals = [i for i in v if i <0]\n",
    "\n",
    "#     neg_vals.append(0)\n",
    "#     v = neg_vals + pos_vals\n",
    "#     norm = TwoSlopeNorm(0, vmin=v[0], vmax=v[-1])\n",
    "    \n",
    "#     cmap = 'bwr'\n",
    "    \n",
    "#     fig, axs = plt.subplots(\n",
    "#         nrows = 2, ncols= 3, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(10, 7), dpi=300)\n",
    "    \n",
    "#     axs = axs.flatten()\n",
    "    \n",
    "#     lon = mask.X.values\n",
    "#     lat = mask.Y.values\n",
    "    \n",
    "#     axs_start = 0\n",
    "#     for model in plot_dict.keys():\n",
    "#         # break\n",
    "#         data = plot_dict[model][putils.xarray_varname(plot_dict[model])].mean(dim='init').values\n",
    "\n",
    "#         map = Basemap(projection='cyl', llcrnrlat=lat[-1] - 1.5, urcrnrlat=lat[0],\n",
    "#                       llcrnrlon=(lon[0] - 360 - 6), urcrnrlon=(lon[-1] - 360 + 20), resolution='l')\n",
    "        \n",
    "#         x, y = map(*np.meshgrid(lon, lat))\n",
    "#         # Adjust the text coordinates based on the actual data coordinates\n",
    "\n",
    "#         im = axs[axs_start].contourf(x, y, data, levels=v, extend='both',\n",
    "#                               transform=ccrs.PlateCarree(), cmap=cmap, norm=norm)\n",
    "        \n",
    "#         gl = axs[axs_start].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "#                                    linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "#         gl.xlabels_top = False\n",
    "#         gl.ylabels_right = False\n",
    "#         gl.ylabels_left = True\n",
    "#         gl.xformatter = LongitudeFormatter()\n",
    "#         gl.yformatter = LatitudeFormatter()\n",
    "#         axs[axs_start].coastlines()\n",
    "#         axs[axs_start].set_aspect('equal')  # this makes the plots better\n",
    "#         axs[axs_start].set_title(f'{model} Week {week_lead}',fontsize=8)\n",
    "#         axs_start+=1\n",
    "    \n",
    "#     cbar_ax = fig.add_axes([0.05, 0.03, .9, .04])\n",
    "    \n",
    "#     # Draw the colorbar\n",
    "#     cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "#     plt.suptitle(f'CRPSS on testing Dataset', fontsize=20)\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     plt.savefig(f'{save_dir}/Wk{week_lead}_CRPSS_climpred.png')\n",
    "\n",
    "#     return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bbfca6-c0bc-46c5-bc99-8352bed672d2",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d06c6-9544-460c-8c7a-ca41366c8c03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''We only ran EMOS on CONUS'''\n",
    "for week_lead in [1,2,3,4,5]:\n",
    "    if region_name == 'CONUS':\n",
    "        spatial_CRPSS_climpred_with_EMOS(week_lead, region_name, test_start, test_end, single_experiment_or_all_experiments='EX27') #cannot do XGBoost, no ensemble made\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1334c-e41b-47dc-858b-45f7059eded1",
   "metadata": {},
   "source": [
    "# Percentiles of ensemble mean. CONUS Only. We are only choosing a single Experiment number and comparing across all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a25a71-3521-470a-b010-93cff49ea375",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecmwf_perc\n",
    "gefs_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14287f77-1a31-4b2e-af20-ce9f616e8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evalute_percentiles_ensemble_mean(week_lead, region_name, test_start, test_end, unet_experiment):\n",
    "\n",
    "#     #Test \n",
    "#     # week_lead=1\n",
    "#     # percentile_eval = 20\n",
    "\n",
    "#     #Save dir\n",
    "#     save_dir = f'Data/correct_anomaly_percentile_statistics/{region_name}'\n",
    "#     os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "#     save_ecmwf = f'{save_dir}/Wk{week_lead}_ecmwf_stats.npy'\n",
    "#     save_gefs = f'{save_dir}/Wk{week_lead}_gefs_stats.npy'\n",
    "#     save_xg = f'{save_dir}/Wk{week_lead}_xgboost_stats.npy'\n",
    "#     save_emos = f'{save_dir}/Wk{week_lead}_emos_stats.npy'\n",
    "#     save_obs_binary = f'{save_dir}/Wk{week_lead}_emos_stats.npy'\n",
    "    \n",
    "#     day_num = (week_lead*7) -1\n",
    "\n",
    "    \n",
    "#     def check_if_below_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_gefs_vals,u_ecmwf_vals, x_vals,emos_vals):\n",
    "#         perc_= obs_percent[f'{percentile_num}th_percentile'].values\n",
    "        \n",
    "#         def find_percentage(perc_,o_vals,fcst):\n",
    "#             correct =  np.count_nonzero(~np.isnan(np.where((o_vals<perc_)&(fcst<perc_),1,np.nan)),axis=0)\n",
    "#             correct.shape #(104, 48, 96)\n",
    "#             incorrect =  np.count_nonzero(~np.isnan(np.where((o_vals<perc_)&(fcst>perc_),2,np.nan)),axis=0)\n",
    "#             return(correct/(correct + incorrect))\n",
    "\n",
    "#         g_perc = find_percentage(perc_,o_vals,g_vals)\n",
    "#         g_perc.shape\n",
    "#         e_perc = find_percentage(perc_,o_vals,e_vals)\n",
    "#         u_gefs_perc = find_percentage(perc_,o_vals,u_gefs_vals)\n",
    "#         u_ecmwf_perc = find_percentage(perc_,o_vals,u_ecmwf_vals)\n",
    "#         x_perc = find_percentage(perc_,o_vals,x_vals)\n",
    "#         emos_perc = find_percentage(perc_,o_vals,x_vals)\n",
    "#         return(g_perc,e_perc,u_gefs_perc,u_ecmwf_perc,x_perc,emos_perc)\n",
    "\n",
    "\n",
    "    \n",
    "#     def check_if_above_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_gefs_vals,u_ecmwf_vals, x_vals,emos_vals):\n",
    "#         perc_= obs_percent[f'{percentile_num}th_percentile'].values\n",
    "        \n",
    "#         def find_percentage(perc_,o_vals,fcst):\n",
    "#             correct =  np.count_nonzero(~np.isnan(np.where((o_vals>perc_)&(fcst>perc_),1,np.nan)),axis=0)\n",
    "#             correct.shape #(104, 48, 96)\n",
    "#             incorrect =  np.count_nonzero(~np.isnan(np.where((o_vals>perc_)&(fcst<perc_),2,np.nan)),axis=0)\n",
    "#             return(correct/(correct + incorrect))\n",
    "\n",
    "#         g_perc = find_percentage(perc_,o_vals,g_vals)\n",
    "#         g_perc.shape\n",
    "#         e_perc = find_percentage(perc_,o_vals,e_vals)\n",
    "#         u_gefs_perc = find_percentage(perc_,o_vals,u_gefs_vals)\n",
    "#         u_ecmwf_perc = find_percentage(perc_,o_vals,u_ecmwf_vals)\n",
    "#         x_perc = find_percentage(perc_,o_vals,x_vals)\n",
    "#         emos_perc = find_percentage(perc_,o_vals,x_vals)\n",
    "#         return(g_perc,e_perc,u_gefs_perc,u_ecmwf_perc,x_perc,emos_perc)\n",
    "\n",
    "    \n",
    "#     print('Loading observation and baseline anomaly files')\n",
    "#     obs, gefs, ecmwf = obs_anomaly_SubX_format.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_anomaly.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order), baseline_ecmwf.sel(L=day_num).expand_dims({'L':1}).transpose(*dim_order)\n",
    "\n",
    "#     obs_percent = obs_anom_percentile.sel(L=day_num).sel(M=0)\n",
    "#     obs_percent['95th_percentile'].shape #(104, 48, 96)\n",
    "\n",
    "#     #Get UNET prediction (single file)\n",
    "#     #Now for all predictions from UNET, make the ACC\n",
    "#     unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{unet_experiment}*'))\n",
    "#     #Get the ECMWF file\n",
    "\n",
    "#     ec_unet = [i for i in unet_files if 'ECMWF' in i][0]\n",
    "#     gef_unet = [i for i in unet_files if f'{single_experiment_or_all_experiments}_regular_RZSM' in i][0]\n",
    "    \n",
    "\n",
    "#     test_ec_unet =  verifications.reverse_min_max_scaling(np.load(ec_unet), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "#     test_ec_unet = np.reshape(test_ec_unet,(test_ec_unet.shape[0]//11,11,test_ec_unet.shape[1],test_ec_unet.shape[2]))\n",
    "#     test_ec_unet.shape\n",
    "#     #Now mask the input\n",
    "#     test_ec_unet = np.where(mask_anom == 1,test_ec_unet,np.nan)\n",
    "#     save_unet_ecmwf = f\"{save_dir}/{ec_unet.split('/')[-1]}\"\n",
    "    \n",
    "#     test_gef_unet =  verifications.reverse_min_max_scaling(np.load(gef_unet), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "#     test_gef_unet = np.reshape(test_gef_unet,(test_gef_unet.shape[0]//11,11,test_gef_unet.shape[1],test_gef_unet.shape[2]))\n",
    "#     test_gef_unet.shape\n",
    "#     #Now mask the input\n",
    "#     test_gef_unet = np.where(mask_anom == 1,test_gef_unet,np.nan)\n",
    "#     save_unet_gefs = f\"{save_dir}/{gef_unet.split('/')[-1]}\"\n",
    "\n",
    "#     emos_testing\n",
    "    \n",
    "#     final_perc_gefs = np.zeros(shape=(48,96,8)) #Adding 8 channels for the different anomaly spreads\n",
    "#     final_perc_gefs[:,:,:] = np.nan\n",
    "#     final_perc_ecmwf = final_perc_gefs.copy()\n",
    "#     final_perc_unet_gefs = final_perc_gefs.copy()\n",
    "#     final_perc_unet_ecmwf = final_perc_gefs.copy()\n",
    "#     final_perc_emos = final_perc_gefs.copy()\n",
    "    \n",
    "#     #XGBoost\n",
    "#     #Load the XGBoost data\n",
    "#     if region_name == 'CONUS':\n",
    "#         xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{week_lead}_testing/*EX28*'))[0]\n",
    "    \n",
    "#         # break\n",
    "#         #Still working here\n",
    "#         test_name = xgboost_files.split('testing_')[-1].split('.npy')[0]\n",
    "#         load_ = np.expand_dims(np.load(xgboost_files),-1)\n",
    "#         load_.shape\n",
    "#         load_ = np.where(load_ == 0,np.nan,load_)\n",
    "#         load_ =  verifications.reverse_min_max_scaling(load_, region_name, day_num,'GEFSv12',test_year)#We only want the last channel\n",
    "        \n",
    "#         xg = np.empty(shape=(load_.shape[0],11,load_.shape[1],load_.shape[2],load_.shape[3])) #This will help with climpred functions\n",
    "#         for j in range(11):\n",
    "#             xg[:,j,:,:,:] = load_\n",
    "    \n",
    "#         xg = xg.squeeze()\n",
    "#         xg.shape\n",
    "#         final_perc_xg = final_perc_gefs.copy()\n",
    "#         x_vals = np.nanmean(xg,axis=1)\n",
    "#     else:\n",
    "#         x_vals = final_perc_gefs.copy()\n",
    "        \n",
    "#     #Check if the predicted anomaly is below each threshold\n",
    "\n",
    "#     file = baseline_anomaly\n",
    "#     file.RZSM.shape\n",
    "#     out_check_gefs_base = np.zeros(shape=(104,11,48,96,8)) #Adding 8 channels for the different anomaly spreads\n",
    "\n",
    "#     #Test\n",
    "#     # idx = 0\n",
    "#     # mx = 0\n",
    "#     # ix = 10\n",
    "#     # iy =10 #NEGATIVE ANOMALY VALUE\n",
    "#     # iy =5 #POSITIVE ANOMALY VALUE\n",
    "\n",
    "#     #Use np.where to find the values of the percentile\n",
    "\n",
    "#     #Take ensemble mean\n",
    "#     o_vals = np.nanmean(obs.RZSM[:,:,0,:,:].values,axis=1)\n",
    "#     g_vals =  np.nanmean(gefs.RZSM[:,:,0,:,:].values,axis=1)\n",
    "#     e_vals =  np.nanmean(ecmwf.RZSM[:,:,0,:,:].values,axis=1)\n",
    "#     u_gefs_vals = np.nanmean(test_gef_unet,axis=1)\n",
    "#     u_ecmwf_vals = np.nanmean(test_ec_unet,axis=1)\n",
    "#     emos_vals = np.nanmean(emos_testing.sel(L=day_num).RZSM,axis=1)\n",
    "\n",
    "#     u_gefs_vals.shape\n",
    "#     x_vals.shape\n",
    "\n",
    "#     ### Returns return(g_perc,e_perc,u_gefs_perc,u_ecmwf_perc,x_perc)\n",
    "#     ### order [obs_percent, percentile_num, o_vals, g_vals, e_vals, u_gefs_vals,u_ecmwf_vals, x_vals]\n",
    "    \n",
    "#     for idx,percentile_num in enumerate([5,10,20,33]):\n",
    "#         '''We are adding 4 to make sure that we get the indices correct, we already added data from below percentiles'''\n",
    "#         final_perc_gefs[:,:,idx], final_perc_ecmwf[:,:,idx], final_perc_unet_gefs[:,:,idx], final_perc_unet_ecmwf[:,:,idx], \\\n",
    "#         final_perc_xg[:,:,idx],final_perc_emos[:,:,idx] = check_if_below_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_gefs_vals, u_ecmwf_vals,x_vals,emos_vals)\n",
    "\n",
    "    \n",
    "#     for idx,percentile_num in enumerate([66,80,90,95]):\n",
    "#         '''We are adding 4 to make sure that we get the indices correct, we already added data from below percentiles'''\n",
    "#         final_perc_gefs[:,:,idx+4], final_perc_ecmwf[:,:,idx+4], final_perc_unet_gefs[:,:,idx+4], final_perc_unet_ecmwf[:,:,idx+4], \\\n",
    "#         final_perc_xg[:,:,idx+4],final_perc_emos[:,:,idx+4] = check_if_above_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_gefs_vals, u_ecmwf_vals,x_vals,emos_vals)\n",
    "\n",
    "\n",
    "#     #Save files for later use\n",
    "#     np.save(save_ecmwf,final_perc_ecmwf)\n",
    "#     np.save(save_gefs, final_perc_gefs)\n",
    "#     np.save(save_unet_gefs, final_perc_unet_gefs)\n",
    "#     np.save(save_unet_ecmwf, final_perc_unet_ecmwf)\n",
    "#     np.save(save_xg, final_perc_xg)\n",
    "#     np.save(save_emos, final_perc_emos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9e4da-3936-4ac4-a552-0e6536c821b9",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b077ad-64b1-4a61-9c59-e537a02157f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We only ran EMOS, XGBoost on CONUS'''\n",
    "for week_lead in [1,2,3,4,5]:\n",
    "    if region_name == 'CONUS':\n",
    "        evalute_percentiles_ensemble_mean(week_lead, region_name, test_start, test_end, unet_experiment='EX27') #cannot do XGBoost, no ensemble made\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c00e13-cd6a-4e5a-9b52-3a594a49a132",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9300975-7bdb-4e82-936f-79e852a64b5d",
   "metadata": {},
   "source": [
    "## Creates files for each lead for _stats_TP_FP_ensemble_meanIdist.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50befc-eca0-452a-a846-b634a0d0ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_binary_for_hit_rate_with_ensemble_mean_actual(week_lead, region_name, test_start, test_end, unet_experiment,\n",
    "                                                       unet_gefs_perc,raw_ecmwf_perc,raw_gefs_perc):\n",
    "\n",
    "    #Test \n",
    "    # week_lead=1\n",
    "    # percentile_eval = 20\n",
    "\n",
    "    day_num = (week_lead*7) -1\n",
    "    \n",
    "    #Save dir\n",
    "    save_dir = f'Data/correct_anomaly_percentile_statistics/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{unet_experiment}*'))\n",
    "    gef_unet = [i for i in unet_files if f'{unet_experiment}_regular_RZSM' in i][0]\n",
    "    \n",
    "    save_ecmwf = f'{save_dir}/Wk{week_lead}_ecmwf_stats_TP_FP_ensemble_meanIdist.npy'\n",
    "    save_gefs = f'{save_dir}/Wk{week_lead}_gefs_stats_TP_FP_ensemble_meanIdist.npy'\n",
    "    save_obs_binary = f'{save_dir}/Wk{week_lead}_obs_stats_TP_FP_ensemble_meanIdist.npy'\n",
    "    save_unet_gefs = f\"{save_dir}/{gef_unet.split('/')[-1].split('.npy')[0]}_stats_TP_FP_ensemble_meanIdist.npy\"\n",
    "\n",
    "    def check_if_below_percentile(obs_percent, percentile_num, gef_perc_vals, ecmwf_perc_vals, u_gefs_vals):\n",
    "\n",
    "        obs_binary = np.where((obs_percent<percentile_num),1,0)\n",
    "        gefs_binary = np.where((gef_perc_vals<percentile_num),1,0)\n",
    "        ecmwf_binary = np.where((ecmwf_perc_vals<percentile_num),1,0)\n",
    "        u_gefs_binary = np.where((u_gefs_vals<percentile_num),1,0)\n",
    "    \n",
    "        return(obs_binary,gefs_binary,ecmwf_binary,u_gefs_binary)\n",
    "\n",
    "    def check_if_above_percentile(obs_percent, percentile_num, gef_perc_vals, ecmwf_perc_vals, u_gefs_vals):\n",
    "\n",
    "        obs_binary = np.where((obs_percent>percentile_num),1,0)\n",
    "        gefs_binary = np.where((gef_perc_vals>percentile_num),1,0)\n",
    "        ecmwf_binary = np.where((ecmwf_perc_vals>percentile_num),1,0)\n",
    "        u_gefs_binary = np.where((u_gefs_vals>percentile_num),1,0)\n",
    "\n",
    "        return(obs_binary,gefs_binary,ecmwf_binary,u_gefs_binary)\n",
    "\n",
    "    #OBSERVATIONS\n",
    "    obs_percent = obs_percentile_only.sel(L=day_num).sel(M=0) #(104, 48, 96)\n",
    "    unet_gefs_perc=unet_gefs_perc.sel(L=day_num)\n",
    "    raw_ecmwf_perc=raw_ecmwf_perc.sel(L=day_num)\n",
    "    raw_gefs_perc=raw_gefs_perc.sel(L=day_num)\n",
    "    \n",
    "\n",
    "\n",
    "    obs_percent = obs_percent.RZSM_percentile.values\n",
    "    gef_perc_vals = raw_gefs_perc.RZSM.values\n",
    "    ecmwf_perc_vals = raw_ecmwf_perc.RZSM.values\n",
    "    u_gefs_vals= unet_gefs_perc.RZSM.values\n",
    "\n",
    "    out_shape = (104,48,96,8)\n",
    "    final_bin_gefs = np.where(np.expand_dims(mask_anom==1,-1),np.empty(shape = out_shape),np.nan)\n",
    "    final_bin_ecmwf= np.where(np.expand_dims(mask_anom==1,-1),np.empty(shape = out_shape),np.nan)\n",
    "    final_bin_obs= np.where(np.expand_dims(mask_anom==1,-1),np.empty(shape = out_shape),np.nan)\n",
    "    final_bin_unet_gefs= np.where(np.expand_dims(mask_anom==1,-1),np.empty(shape = out_shape),np.nan)\n",
    "\n",
    "    \n",
    "    for idx,percentile_num in enumerate([5,10,20,33]):\n",
    "        '''We are adding 4 to make sure that we get the indices correct, we already added data from below percentiles'''\n",
    "        final_bin_obs[:,:,:,idx], final_bin_gefs[:,:,:,idx], final_bin_ecmwf[:,:,:,idx], final_bin_unet_gefs[:,:,:,idx], \\\n",
    "         = check_if_below_percentile(obs_percent, percentile_num,  gef_perc_vals, ecmwf_perc_vals, u_gefs_vals)\n",
    "\n",
    "    \n",
    "    for idx,percentile_num in enumerate([66,80,90,95]):\n",
    "        '''We are adding 4 to make sure that we get the indices correct, we already added data from below percentiles'''\n",
    "        final_bin_obs[:,:,:,idx+4], final_bin_gefs[:,:,:,idx+4], final_bin_ecmwf[:,:,:,idx+4], final_bin_unet_gefs[:,:,:,idx+4], \\\n",
    "         = check_if_above_percentile(obs_percent, percentile_num, gef_perc_vals, ecmwf_perc_vals, u_gefs_vals)\n",
    "\n",
    "\n",
    "    #Save files for later use\n",
    "    np.save(save_ecmwf,final_bin_ecmwf)\n",
    "    np.save(save_gefs, final_bin_gefs)\n",
    "    np.save(save_unet_gefs, final_bin_unet_gefs)\n",
    "    np.save(save_obs_binary, final_bin_obs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8588d24-cf0b-4bef-94be-406c4c3cdbbd",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4cc68-1f31-4f58-9128-b1a3f4a0286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We only ran EMOS, XGBoost on CONUS'''\n",
    "#UNET experiment\n",
    "unet_gefs_perc = xr.open_mfdataset(f'Data/UNET_percentiles_MEM/{experiment}/*',combine='nested',concat_dim=['S']).sel(S=slice(test_start,test_end))\n",
    "\n",
    "#Raw ECMWF\n",
    "raw_ecmwf_perc = xr.open_mfdataset(f'Data/ECMWF/soilw_bgrnd_processed/CONUS/percentiles_MEM/*',combine='nested',concat_dim=['S']).sel(S=slice(test_start,test_end))\n",
    "\n",
    "#Raw ECMWF\n",
    "raw_gefs_perc = xr.open_mfdataset(f'Data/GEFSv12_reforecast/soilw_bgrnd/percentiles_MEM/*',combine='nested',concat_dim=['S']).sel(S=slice(test_start,test_end))\n",
    "\n",
    "for week_lead in [1,2,3,4,5]:\n",
    "    if region_name == 'CONUS':\n",
    "        setup_binary_for_hit_rate_with_ensemble_mean_actual(week_lead, region_name, \n",
    "                                                            test_start, test_end, \n",
    "                                                            unet_experiment='EX29',\n",
    "                                                           unet_gefs_perc=unet_gefs_perc,\n",
    "                                                           raw_ecmwf_perc=raw_ecmwf_perc,\n",
    "                                                           raw_gefs_perc=raw_gefs_perc) #cannot do XGBoost, no ensemble made\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde13203-3a9f-4a4f-b8b7-70979bf3124b",
   "metadata": {},
   "source": [
    "# Plot the HitRate for CONUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9dcd8-961e-496d-8faa-d822d6c0d763",
   "metadata": {},
   "source": [
    "### For ensemble mean with the raw reforecasts drawn from their own distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad352b-d293-4641-9f76-7908e484adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_HITRATE_data_by_percentile_with_own_distribution(percentile,unet_experiment):\n",
    "\n",
    "    #where files are saved\n",
    "    dir_ = f'Data/correct_anomaly_percentile_statistics/{region_name}'\n",
    "    \n",
    "    save_dir = f'Outputs/hit_rate_stats/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    percentile_list = [5,10,20,33,66,80,90,95] #These are also the indices of the percentile values\n",
    "\n",
    "\n",
    "    if percentile == 20:\n",
    "        idPercentile = 2\n",
    "    elif percentile == 10:\n",
    "        idPercentile = 1\n",
    "    elif percentile == 5:\n",
    "        idPercentile = 0\n",
    "    elif percentile == 33:\n",
    "        idPercentile = 3\n",
    "\n",
    "    hitRate = {}\n",
    "\n",
    "    for week_lead in [3,4,5]:\n",
    "        all_files = sorted(glob(f'{dir_}/Wk{week_lead}_testing*{unet_experiment}*Idist*'))\n",
    "        ecm_file_unet = [i for i in all_files if 'ECMWF' in i][0]\n",
    "        gef_file_unet = [i for i in all_files if 'ECMWF' not in i][0]\n",
    "        \n",
    "        hitRate[f'Week {week_lead}'] = {}\n",
    "        \n",
    "        # break\n",
    "        save_ecmwf = f'{dir_}/Wk{week_lead}_ecmwf_stats_TP_FP_ensemble_meanIdist.npy'\n",
    "        save_gefs = f'{dir_}/Wk{week_lead}_gefs_stats_TP_FP_ensemble_meanIdist.npy'\n",
    "        save_unet_gefs = gef_file_unet\n",
    "        save_unet_ecmwf = ecm_file_unet\n",
    "        save_xg = f'{dir_}/Wk{week_lead}_xgboost_stats_TP_FP_ensemble_meanIdist.npy'\n",
    "        save_emos = f'{dir_}/Wk{week_lead}_emos_stats_TP_FP_ensemble_meanIdist.npy'\n",
    "        \n",
    "        obs_binary = np.load(f'{dir_}/Wk{week_lead}_obs_stats_TP_FP_ensemble_meanIdist.npy')\n",
    "        obs_binary = obs_binary[:,:,:,idPercentile]\n",
    "    \n",
    "        for ref_name, ref_file in zip(['GEFSv12', 'ECMWF', 'XGBoost', f'{unet_experiment}_GEFSv12', f'{unet_experiment}_ECMWF','EMOS' ],[save_gefs, save_ecmwf, save_xg, save_unet_gefs, save_unet_ecmwf, save_emos]):\n",
    "            # break\n",
    "            print(f'Working on forecast {ref_name} lead {week_lead}.')\n",
    "            \n",
    "            out_plot = np.zeros(shape=(48,96))\n",
    "            out_count = np.zeros(shape=(48,96))\n",
    "            # ax = axes[ax_start]\n",
    "            \n",
    "            # break\n",
    "            reforecast = np.load(ref_file)\n",
    "            reforecast.shape\n",
    "            ref_subset = reforecast[:,:,:,idPercentile]\n",
    "            ref_subset.shape\n",
    "            \n",
    "            for idX in range(obs_binary.shape[2]):\n",
    "                for idy in range(obs_binary.shape[1]):\n",
    "                    \n",
    "                    observed_labels = obs_binary[:,idy,idX].flatten()\n",
    "                    predicted_labels = ref_subset[:,idy,idX].flatten()\n",
    "                    if np.all(np.isnan(observed_labels)) or np.all(np.isnan(predicted_labels)):\n",
    "                        out_plot[idy,idX] = np.nan\n",
    "                        out_count[idy,idX] = np.nan\n",
    "                    else:\n",
    "                        #Create contingency table\n",
    "                        # cm = confusion_matrix(observed_labels, predicted_labels) \n",
    "                        # accuracy = accuracy_score(observed_labels, predicted_labels) #Fraction of correct prediction\n",
    "                        # precision = precision_score(observed_labels, predicted_labels,pos_label=1) #True Discovery Rate\n",
    "                        recall = recall_score(observed_labels, predicted_labels,pos_label=1) #Hit Rate/ True positive rate\n",
    "                        out_count[idy,idX] = sum(observed_labels)\n",
    "                        # f1 = f1_score(observed_labels, predicted_labels,pos_label=1) #harmonic mean of the precision and recall\n",
    "                            \n",
    "                        # tnr = recall_score(observed_labels, predicted_labels, pos_label = 0)\n",
    "                        # fpr = 1 - tnr\n",
    "        \n",
    "                        out_plot[idy,idX] = recall\n",
    "                        \n",
    "            hitRate[f'Week {week_lead}']['OBS'] = out_count\n",
    "            hitRate[f'Week {week_lead}'][ref_name] = out_plot\n",
    "            \n",
    "    return(hitRate,unet_experiment,percentile)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc52eed-5360-45d5-8fc7-58a0bc61fca0",
   "metadata": {},
   "source": [
    "### For ensemble mean with only the observation distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3bb52c-dd31-40d7-852b-f6062430eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_HITRATE_data_by_percentile(percentile,unet_experiment):\n",
    "\n",
    "    #where files are saved\n",
    "    dir_ = f'Data/correct_anomaly_percentile_statistics/{region_name}'\n",
    "    \n",
    "    save_dir = f'Outputs/hit_rate_stats/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    percentile_list = [5,10,20,33,66,80,90,95] #These are also the indices of the percentile values\n",
    "\n",
    "\n",
    "    if percentile == 20:\n",
    "        idPercentile = 2\n",
    "    elif percentile == 10:\n",
    "        idPercentile = 1\n",
    "    elif percentile == 5:\n",
    "        idPercentile = 0\n",
    "    elif percentile == 33:\n",
    "        idPercentile = 3\n",
    "\n",
    "    hitRate = {}\n",
    "\n",
    "    for week_lead in [3,4,5]:\n",
    "        all_files = sorted(glob(f'{dir_}/Wk{week_lead}_testing**{unet_experiment}*ensemble_mean*'))\n",
    "        ecm_file_unet = [i for i in all_files if 'ECMWF' in i][0]\n",
    "        gef_file_unet = [i for i in all_files if 'ECMWF' not in i][0]\n",
    "        \n",
    "        hitRate[f'Week {week_lead}'] = {}\n",
    "        \n",
    "        # break\n",
    "        save_ecmwf = f'{dir_}/Wk{week_lead}_ecmwf_stats_TP_FP_ensemble_mean.npy'\n",
    "        save_gefs = f'{dir_}/Wk{week_lead}_gefs_stats_TP_FP_ensemble_mean.npy'\n",
    "        save_unet_gefs = gef_file_unet\n",
    "        save_unet_ecmwf = ecm_file_unet\n",
    "        save_xg = f'{dir_}/Wk{week_lead}_xgboost_stats_TP_FP_ensemble_mean.npy'\n",
    "        save_emos = f'{dir_}/Wk{week_lead}_emos_stats_TP_FP_ensemble_mean.npy'\n",
    "        \n",
    "        obs_binary = np.load(f'{dir_}/Wk{week_lead}_obs_stats_TP_FP_ensemble_mean.npy')\n",
    "        obs_binary = obs_binary[:,:,:,idPercentile]\n",
    "    \n",
    "        for ref_name, ref_file in zip(['GEFSv12', 'ECMWF', 'XGBoost', f'{unet_experiment}_GEFSv12', f'{unet_experiment}_ECMWF','EMOS' ],[save_gefs, save_ecmwf, save_xg, save_unet_gefs, save_unet_ecmwf, save_emos]):\n",
    "            # break\n",
    "            print(f'Working on forecast {ref_name} lead {week_lead}.')\n",
    "            \n",
    "            out_plot = np.zeros(shape=(48,96))\n",
    "            out_count = np.zeros(shape=(48,96))\n",
    "            # ax = axes[ax_start]\n",
    "            \n",
    "            # break\n",
    "            reforecast = np.load(ref_file)\n",
    "            reforecast.shape\n",
    "            ref_subset = reforecast[:,:,:,idPercentile]\n",
    "            ref_subset.shape\n",
    "            \n",
    "            for idX in range(obs_binary.shape[2]):\n",
    "                for idy in range(obs_binary.shape[1]):\n",
    "                    \n",
    "                    observed_labels = obs_binary[:,idy,idX].flatten()\n",
    "                    predicted_labels = ref_subset[:,idy,idX].flatten()\n",
    "                    if np.all(np.isnan(observed_labels)):\n",
    "                        out_plot[idy,idX] = np.nan\n",
    "                        out_count[idy,idX] = np.nan\n",
    "                    else:\n",
    "                        #Create contingency table\n",
    "                        # cm = confusion_matrix(observed_labels, predicted_labels) \n",
    "                        # accuracy = accuracy_score(observed_labels, predicted_labels) #Fraction of correct prediction\n",
    "                        # precision = precision_score(observed_labels, predicted_labels,pos_label=1) #True Discovery Rate\n",
    "                        recall = recall_score(observed_labels, predicted_labels,pos_label=1) #Hit Rate/ True positive rate\n",
    "                        out_count[idy,idX] = sum(observed_labels)\n",
    "                        # f1 = f1_score(observed_labels, predicted_labels,pos_label=1) #harmonic mean of the precision and recall\n",
    "                            \n",
    "                        # tnr = recall_score(observed_labels, predicted_labels, pos_label = 0)\n",
    "                        # fpr = 1 - tnr\n",
    "        \n",
    "                        out_plot[idy,idX] = recall\n",
    "                        \n",
    "            hitRate[f'Week {week_lead}']['OBS'] = out_count\n",
    "            hitRate[f'Week {week_lead}'][ref_name] = out_plot\n",
    "            \n",
    "    return(hitRate,unet_experiment,percentile)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d611d-81ac-4569-9253-77ab947e13c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_without_obs(hitRate):\n",
    "    rmax_, rmin_ = [],[]\n",
    "    omax_, omin_ = [],[]\n",
    "    for lead in list(hitRate.keys()):\n",
    "        # break\n",
    "        for model in hitRate[lead].keys():\n",
    "            if model == 'OBS':\n",
    "                omax_.append(np.nanmax(hitRate[lead][model]))\n",
    "                omin_.append(np.nanmin(hitRate[lead][model]))\n",
    "            elif ('EMOS' in model) or ('XGBoost' in model) or (f'{experiment}_GEFSv12' in model):\n",
    "                pass\n",
    "            else:\n",
    "                rmax_.append(np.nanmax(hitRate[lead][model]))\n",
    "                rmin_.append(np.nanmin(hitRate[lead][model]))\n",
    "\n",
    "    return(max(rmax_), min(rmin_), max(omax_), min(omin_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e40e9e-71b0-4351-9d5c-baa49a11ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_percentile_for_hitRate_CONUS(hitRate,experiment,percentile):\n",
    "\n",
    "    save_dir = f'Outputs/hit_rate/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    #Keep only 1 instance of the observation counts below the 20th percentile\n",
    "    rmax_, rmin_, omax_,omin_ = get_min_max_without_obs(hitRate)\n",
    "        \n",
    "    # global_max, global_min = max(max_), min(min_)\n",
    "    v_obs = np.linspace(1, omax_, 8, endpoint=True,dtype=int)\n",
    "    v_ref = np.linspace(rmin_, rmax_, 15, endpoint=True)\n",
    "    pos_vals = [i for i in v_ref if i >0.50]\n",
    "    neg_vals = [i for i in v_ref if i <0.50]\n",
    "    neg_vals.append(0.50)\n",
    "    \n",
    "    v_ref = neg_vals + pos_vals\n",
    "    \n",
    "    norm_ref = TwoSlopeNorm(0.50, vmin=v_ref[0], vmax=v_ref[-1])\n",
    "\n",
    "    cmap = 'Reds'\n",
    "    cmap_obs = 'YlOrRd'\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows = 3, ncols= 4, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(10, 5), dpi=300)\n",
    "    \n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    lon = mask.X.values\n",
    "    lat = mask.Y.values\n",
    "    grid_label_fontsize = 7\n",
    "    \n",
    "    axs_start = 0\n",
    "    for lead in hitRate.keys():\n",
    "        for model in hitRate[lead].keys():\n",
    "            if ('EMOS' in model) or ('XGBoost' in model) or (f'{experiment}_GEFSv12' in model):\n",
    "                pass\n",
    "            else:\n",
    "                data = hitRate[lead][model]\n",
    "                map = Basemap(projection='cyl', llcrnrlat=lat[-1] - 1.5, urcrnrlat=lat[0],\n",
    "                              llcrnrlon=(lon[0] - 360 - 6), urcrnrlon=(lon[-1] - 360 + 20), resolution='l')\n",
    "                \n",
    "                x, y = map(*np.meshgrid(lon, lat))\n",
    "                # Adjust the text coordinates based on the actual data coordinates\n",
    "\n",
    "                if model == 'OBS':\n",
    "                    data = np.where(data==0,np.nan,data)\n",
    "                    obs = axs[axs_start].contourf(x, y, data, levels=v_obs, extend='both',\n",
    "                      transform=ccrs.PlateCarree(), cmap=cmap_obs)\n",
    "                else:\n",
    "                    data = np.where(data==0,np.nan,data)\n",
    "                    im = axs[axs_start].contourf(x, y, data, levels=v_ref, extend='both',\n",
    "                                          transform=ccrs.PlateCarree(), cmap=cmap,norm=norm_ref)\n",
    "                \n",
    "                gl = axs[axs_start].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                           linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "                gl.top_labels = False\n",
    "                gl.right_labels = False\n",
    "                \n",
    "                if axs_start in [0,4,8]:\n",
    "                    gl.left_labels = True\n",
    "                \n",
    "                gl.xlabel_style = {'fontsize': grid_label_fontsize}  # Set the fontsize for the x labels\n",
    "                gl.ylabel_style = {'fontsize': grid_label_fontsize}  # Set the fontsize for the y labels\n",
    "                   \n",
    "                gl.xformatter = LongitudeFormatter()\n",
    "                gl.yformatter = LatitudeFormatter()\n",
    "                axs[axs_start].coastlines()\n",
    "                axs[axs_start].set_aspect('equal')  # this makes the plots better\n",
    "                if model == 'OBS':\n",
    "                    axs[axs_start].set_title(f'GLEAM {lead}',fontsize=8)\n",
    "                else:\n",
    "                    axs[axs_start].set_title(f'{model} {lead}',fontsize=8)\n",
    "                axs_start+=1\n",
    "\n",
    "    # Creating colorbars  [left, bottom, width, height] \n",
    "    cbar_ax_obs = fig.add_axes([0.035, -0.02, .21, .03])\n",
    "    cbar_ax_ref = fig.add_axes([0.27, -0.02, .7, .03])\n",
    "\n",
    "    cbar_obs = fig.colorbar(obs, cax=cbar_ax_obs, orientation='horizontal')\n",
    "    cbar_ref = fig.colorbar(im, cax=cbar_ax_ref, orientation='horizontal')\n",
    "\n",
    "    cbar_obs.set_label('Forecast Weeks < 20th percentile')\n",
    "    cbar_ref.set_label('TPR')\n",
    "    \n",
    "    # cbar1 = fig.colorbar(im, ax=axs[:4], orientation='horizontal', pad=0.05, fraction=0.04)\n",
    "    # cbar2 = fig.colorbar(im, ax=axs[9:], orientation='horizontal', pad=0.05, fraction=0.04)\n",
    "    # cbar_ax = fig.add_axes([0.05, 0.0, .9, .04])\n",
    "\n",
    "\n",
    "    \n",
    "    # Draw the colorbar\n",
    "    # cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    plt.suptitle(f'Hit Rate ( <{percentile}th percentile ) ensemble mean', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(f'Saving into {save_dir}/{experiment}_hit_rate_{percentile}th_percentile.png')\n",
    "    plt.savefig(f'{save_dir}/{experiment}_hit_rate_{percentile}th_percentile.png',bbox_inches='tight')\n",
    "\n",
    "    return(0)\n",
    "\n",
    "# plot_by_percentile_for_hitRate_CONUS(hitRate,experiment,percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbedb6e-955b-4b39-93ce-061371fae21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_percentile_for_hitRate_CONUS_with_own_distribution(hitRate,experiment,percentile):\n",
    "\n",
    "    save_dir = f'Outputs/hit_rate/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    #Keep only 1 instance of the observation counts below the 20th percentile\n",
    "    rmax_, rmin_, omax_,omin_ = get_min_max_without_obs(hitRate)\n",
    "        \n",
    "    # global_max, global_min = max(max_), min(min_)\n",
    "    v_obs = np.linspace(1, omax_, 8, endpoint=True,dtype=int)\n",
    "    v_ref = np.linspace(rmin_, rmax_, 15, endpoint=True)\n",
    "    pos_vals = [i for i in v_ref if i >0.50]\n",
    "    neg_vals = [i for i in v_ref if i <0.50]\n",
    "    neg_vals.append(0.50)\n",
    "    \n",
    "    v_ref = neg_vals + pos_vals\n",
    "    \n",
    "    norm_ref = TwoSlopeNorm(0.50, vmin=v_ref[0], vmax=v_ref[-1])\n",
    "\n",
    "    cmap = 'Reds'\n",
    "    cmap_obs = 'YlOrRd'\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows = len(hitRate.keys()), ncols= 4, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(10, 5), dpi=300)\n",
    "    \n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    lon = mask.X.values\n",
    "    lat = mask.Y.values\n",
    "    grid_label_fontsize = 7\n",
    "    \n",
    "    axs_start = 0\n",
    "    for lead in hitRate.keys():\n",
    "        for model in hitRate[lead].keys():\n",
    "            if ('EMOS' in model) or ('XGBoost' in model) or (f'{experiment}_ECMWF' in model):\n",
    "                pass\n",
    "            else:\n",
    "                data = hitRate[lead][model]\n",
    "                map = Basemap(projection='cyl', llcrnrlat=lat[-1] - 1.5, urcrnrlat=lat[0],\n",
    "                              llcrnrlon=(lon[0] - 360 - 6), urcrnrlon=(lon[-1] - 360 + 20), resolution='l')\n",
    "                \n",
    "                x, y = map(*np.meshgrid(lon, lat))\n",
    "                # Adjust the text coordinates based on the actual data coordinates\n",
    "\n",
    "                if model == 'OBS':\n",
    "                    data = np.where(data==0,np.nan,data)\n",
    "                    obs = axs[axs_start].contourf(x, y, data, levels=v_obs, extend='both',\n",
    "                      transform=ccrs.PlateCarree(), cmap=cmap_obs)\n",
    "                else:\n",
    "                    data = np.where(data==0,np.nan,data)\n",
    "                    im = axs[axs_start].contourf(x, y, data, levels=v_ref, extend='both',\n",
    "                                          transform=ccrs.PlateCarree(), cmap=cmap,norm=norm_ref)\n",
    "                \n",
    "                gl = axs[axs_start].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                           linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "                gl.top_labels = False\n",
    "                gl.right_labels = False\n",
    "                \n",
    "                if axs_start in [0,4,8]:\n",
    "                    gl.left_labels = True\n",
    "                \n",
    "                gl.xlabel_style = {'fontsize': grid_label_fontsize}  # Set the fontsize for the x labels\n",
    "                gl.ylabel_style = {'fontsize': grid_label_fontsize}  # Set the fontsize for the y labels\n",
    "                   \n",
    "                gl.xformatter = LongitudeFormatter()\n",
    "                gl.yformatter = LatitudeFormatter()\n",
    "                axs[axs_start].coastlines()\n",
    "                axs[axs_start].set_aspect('equal')  # this makes the plots better\n",
    "                if model == 'OBS':\n",
    "                    axs[axs_start].set_title(f'GLEAM {lead}',fontsize=8)\n",
    "                elif model == f'{experiment}_GEFSv12':\n",
    "                    \n",
    "                    axs[axs_start].set_title(f'DM-DL_G {lead}',fontsize=8)\n",
    "                else:\n",
    "                    axs[axs_start].set_title(f'{model} {lead}',fontsize=8)\n",
    "                axs_start+=1\n",
    "\n",
    "    # Creating colorbars  [left, bottom, width, height] \n",
    "    cbar_ax_obs = fig.add_axes([0.038, -0.02, .21, .03])\n",
    "    cbar_ax_ref = fig.add_axes([0.27, -0.02, .7, .03])\n",
    "\n",
    "    cbar_obs = fig.colorbar(obs, cax=cbar_ax_obs, orientation='horizontal')\n",
    "    cbar_ref = fig.colorbar(im, cax=cbar_ax_ref, orientation='horizontal')\n",
    "\n",
    "    cbar_obs.set_label('Forecast Weeks < 20th percentile')\n",
    "    cbar_ref.set_label('TPR')\n",
    "    \n",
    "    # cbar1 = fig.colorbar(im, ax=axs[:4], orientation='horizontal', pad=0.05, fraction=0.04)\n",
    "    # cbar2 = fig.colorbar(im, ax=axs[9:], orientation='horizontal', pad=0.05, fraction=0.04)\n",
    "    # cbar_ax = fig.add_axes([0.05, 0.0, .9, .04])\n",
    "\n",
    "\n",
    "    \n",
    "    # Draw the colorbar\n",
    "    # cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    plt.suptitle(f'Hit Rate ( <{percentile}th percentile ) ensemble mean', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(f'Saving into {save_dir}/{experiment}_hit_rate_{percentile}th_percentileIdist.png')\n",
    "    plt.savefig(f'{save_dir}/{experiment}_hit_rate_{percentile}th_percentileIdist.png',bbox_inches='tight')\n",
    "\n",
    "    return(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa4aaaf-c228-450d-8c5e-e915fd2a13c5",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4323ecf-fd61-4ad9-8e2e-8dbd04e45f38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Raw ECMWF and GEFSv12 have their own distribution rather than using only observation distribution'''\n",
    "percentile = 20\n",
    "\n",
    "if region_name == 'CONUS':\n",
    "    hitRate, experiment,percentile =  return_HITRATE_data_by_percentile_with_own_distribution(percentile=percentile,unet_experiment='EX29')\n",
    "    plot_by_percentile_for_hitRate_CONUS_with_own_distribution(hitRate,experiment,percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14a142-c6e5-4d35-9668-39c75d2dbadd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Raw ECMWF and GEFSv12 rely on only observation distribution'''\n",
    "\n",
    "percentile = 20\n",
    "\n",
    "if region_name == 'CONUS':\n",
    "    hitRate, experiment,percentile =  return_HITRATE_data_by_percentile(percentile=percentile,unet_experiment='EX29')\n",
    "    plot_by_percentile_for_hitRate_CONUS(hitRate,experiment,percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c09479-7711-43d1-a1da-3b7b508f2f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04893706-7cac-43d0-a0cb-5770dce49bc0",
   "metadata": {},
   "source": [
    "# Find the percentile difference between correctly predicted percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523bde9f-111e-40f8-95a4-69a127c66035",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def setup_percentage_difference_of_TP_with_ensemble_mean(week_lead, region_name, test_start, test_end, unet_experiment):\n",
    "\n",
    "    ''' FORECAST ANOMALY - OBSERVATION ANOMALY'''\n",
    "    #Test \n",
    "    # week_lead=1\n",
    "    # percentile_eval = 20\n",
    "\n",
    "    #Save dir\n",
    "    dir_ = f'Data/correct_anomaly_percentile_statistics/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    save_ecmwf = f'{save_dir}/Wk{week_lead}_ecmwf_stats_TP_difference_ensemble_mean.npy'\n",
    "    save_gefs = f'{save_dir}/Wk{week_lead}_gefs_stats_TP_difference_ensemble_mean.npy'\n",
    "    save_xg = f'{save_dir}/Wk{week_lead}_xgboost_stats_TP_difference_ensemble_mean.npy'\n",
    "    save_obs_binary = f'{save_dir}/Wk{week_lead}_obs_stats_TP_difference_ensemble_mean.npy'\n",
    "    save_emos = f'{save_dir}/Wk{week_lead}_emos_stats_TP_difference_ensemble_mean.npy'\n",
    "    \n",
    "    day_num = (week_lead*7) -1\n",
    "\n",
    "    def check_if_below_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_gefs_vals,u_ecmwf_vals, \n",
    "                                  x_vals,emos_vals):\n",
    "        perc_= obs_percent[f'{percentile_num}th_percentile'].values\n",
    "        \n",
    "        def find_percentage(perc_,o_vals,fcst):\n",
    "            fcst_ =  np.where(fcst<perc_,1,0)\n",
    "            obs_binary = np.where((o_vals<perc_),1,0)\n",
    "\n",
    "            #Now find the deviation when true\n",
    "            correct_fcst = np.where(fcst_,fcst,np.nan)\n",
    "            correct_obs = np.where(obs_binary,o_vals,np.nan)\n",
    "            \n",
    "            subtract = correct_fcst - correct_obs\n",
    "            subtract = np.nanmean(subtract,axis=0)\n",
    "            \n",
    "            '''Take the mean to find the probability of correct'''\n",
    "            subtract.shape #(48, 96)\n",
    "\n",
    "            return(subtract)\n",
    "\n",
    "        g_perc = find_percentage(perc_,o_vals,g_vals)\n",
    "        g_perc.shape\n",
    "        e_perc = find_percentage(perc_,o_vals,e_vals)\n",
    "        u_gefs_perc = find_percentage(perc_,o_vals,u_gefs_vals)\n",
    "        u_ecmwf_perc = find_percentage(perc_,o_vals,u_ecmwf_vals)\n",
    "        x_perc = find_percentage(perc_,o_vals,x_vals)\n",
    "        emos_perc = find_percentage(perc_,o_vals,emos_vals)\n",
    "        return(g_perc,e_perc,u_gefs_perc,u_ecmwf_perc,x_perc,emos_perc)\n",
    "\n",
    "\n",
    "    \n",
    "    def check_if_above_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_gefs_vals,u_ecmwf_vals, \n",
    "                                  x_vals,emos_vals):\n",
    "        perc_= obs_percent[f'{percentile_num}th_percentile'].values\n",
    "        \n",
    "        def find_percentage(perc_,o_vals,fcst):\n",
    "            fcst_ =  np.where(fcst>perc_,1,0)\n",
    "            obs_binary = np.where((o_vals>perc_),1,0)\n",
    "            '''Take the mean to find the probability of correct'''\n",
    "            fcst.shape #(104, 48, 96)\n",
    "            #Now find the deviation when true\n",
    "\n",
    "            correct_fcst = np.where(fcst_,fcst,np.nan)\n",
    "            correct_obs = np.where(obs_binary,o_vals,np.nan)\n",
    "            \n",
    "            subtract = correct_fcst - correct_obs\n",
    "            subtract = np.nanmean(subtract,axis=0)\n",
    "            \n",
    "            return(subtract)\n",
    "\n",
    "        g_perc = find_percentage(perc_,o_vals,g_vals)\n",
    "        g_perc.shape\n",
    "        e_perc = find_percentage(perc_,o_vals,e_vals)\n",
    "        u_gefs_perc = find_percentage(perc_,o_vals,u_gefs_vals)\n",
    "        u_ecmwf_perc = find_percentage(perc_,o_vals,u_ecmwf_vals)\n",
    "        x_perc = find_percentage(perc_,o_vals,x_vals)\n",
    "        emos_perc = find_percentage(perc_,o_vals,emos_vals)\n",
    "        \n",
    "        return(g_perc,e_perc,u_gefs_perc,u_ecmwf_perc,x_perc,emos_perc)\n",
    "\n",
    "    \n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    obs, gefs, ecmwf, var_OUT_overwrite, template_testing_only_by_lead= select_data_by_lead(obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only, day_num)\n",
    "\n",
    "    obs_percent = obs_anom_percentile.sel(L=day_num).sel(M=0)\n",
    "    obs_percent['95th_percentile'].shape #(104, 48, 96)\n",
    "\n",
    "    #Get UNET prediction (single file)\n",
    "    #Now for all predictions from UNET, make the ACC\n",
    "    unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{unet_experiment}*'))\n",
    "    #Get the ECMWF file\n",
    "\n",
    "    ec_unet = [i for i in unet_files if 'ECMWF' in i][0]\n",
    "    gef_unet = [i for i in unet_files if f'{unet_experiment}_regular_RZSM' in i][0]\n",
    "    \n",
    "\n",
    "    test_ec_unet =  verifications.reverse_min_max_scaling(np.load(ec_unet), region_name, day_num, 'ECMWF', 2019)[2,:,:,:,0] #We only want the last channel\n",
    "    test_ec_unet = np.reshape(test_ec_unet,(test_ec_unet.shape[0]//11,11,test_ec_unet.shape[1],test_ec_unet.shape[2]))\n",
    "    test_ec_unet.shape\n",
    "    #Now mask the input\n",
    "    test_ec_unet = np.where(mask_anom == 1,test_ec_unet,np.nan)\n",
    "    save_unet_ecmwf = f\"{save_dir}/{ec_unet.split('/')[-1].split('.npy')[0]}_stats_TP_difference_ensemble_mean.npy\"\n",
    "    \n",
    "    test_gef_unet =  verifications.reverse_min_max_scaling(np.load(gef_unet), region_name, day_num, 'GEFSv12', 2019)[2,:,:,:,0] #We only want the last channel\n",
    "    test_gef_unet = np.reshape(test_gef_unet,(test_gef_unet.shape[0]//11,11,test_gef_unet.shape[1],test_gef_unet.shape[2]))\n",
    "    test_gef_unet.shape\n",
    "    #Now mask the input\n",
    "    test_gef_unet = np.where(mask_anom == 1,test_gef_unet,np.nan)\n",
    "    save_unet_gefs = f\"{save_dir}/{gef_unet.split('/')[-1].split('.npy')[0]}_stats_TP_difference_ensemble_mean.npy\"\n",
    "\n",
    "    final_perc_gefs = np.zeros(shape=(48,96,8)) #Adding 8 channels for the different anomaly spreads\n",
    "    final_perc_gefs[:,:,:] = np.nan\n",
    "    final_perc_ecmwf = final_perc_gefs.copy()\n",
    "    final_perc_unet_gefs = final_perc_gefs.copy()\n",
    "    final_perc_unet_ecmwf = final_perc_gefs.copy()\n",
    "    final_perc_emos = final_perc_gefs.copy()\n",
    "    \n",
    "    #XGBoost\n",
    "    #Load the XGBoost data\n",
    "    if region_name == 'CONUS':\n",
    "        xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{week_lead}_testing/*EX28*'))[0]\n",
    "    \n",
    "        # break\n",
    "        #Still working here\n",
    "        test_name = xgboost_files.split('testing_')[-1].split('.npy')[0]\n",
    "        load_ = np.expand_dims(np.load(xgboost_files),-1)\n",
    "        load_.shape\n",
    "        load_ = np.where(load_ == 0,np.nan,load_)\n",
    "        load_ =  verifications.reverse_min_max_scaling(load_, region_name, day_num,'GEFSv12',test_year)#We only want the last channel\n",
    "        \n",
    "        xg = np.empty(shape=(load_.shape[0],11,load_.shape[1],load_.shape[2],load_.shape[3])) #This will help with climpred functions\n",
    "        for j in range(11):\n",
    "            xg[:,j,:,:,:] = load_\n",
    "    \n",
    "        xg = xg.squeeze()\n",
    "        xg.shape\n",
    "        final_perc_xg = final_perc_gefs.copy()\n",
    "        x_vals = np.nanmean(xg,axis=1)\n",
    "    else:\n",
    "        x_vals = final_perc_gefs.copy()\n",
    "        \n",
    "    #Check if the predicted anomaly is below each threshold\n",
    "\n",
    "    file = baseline_anomaly\n",
    "    file.RZSM.shape\n",
    "    #Test\n",
    "    # idx = 0\n",
    "    # mx = 0\n",
    "    # ix = 10\n",
    "    # iy =10 #NEGATIVE ANOMALY VALUE\n",
    "    # iy =5 #POSITIVE ANOMALY VALUE\n",
    "\n",
    "    #Use np.where to find the values of the percentile\n",
    "\n",
    "    #Take ensemble mean\n",
    "    o_vals = np.nanmean(obs.RZSM[:,:,0,:,:].values,axis=1)\n",
    "    g_vals =  np.nanmean(gefs.RZSM[:,:,0,:,:].values,axis=1)\n",
    "    e_vals =  np.nanmean(ecmwf.RZSM[:,:,0,:,:].values,axis=1)\n",
    "    u_gefs_vals = np.nanmean(test_gef_unet,axis=1)\n",
    "    u_ecmwf_vals = np.nanmean(test_ec_unet,axis=1)\n",
    "    emos_vals = np.nanmean(emos_testing.sel(L=day_num).RZSM,axis=1)\n",
    "\n",
    "    u_gefs_vals.shape\n",
    "    x_vals.shape\n",
    "\n",
    "    ### Returns return(g_perc,e_perc,u_gefs_perc,u_ecmwf_perc,x_perc)\n",
    "    ### order [obs_percent, percentile_num, o_vals, g_vals, e_vals, u_gefs_vals,u_ecmwf_vals, x_vals]\n",
    "    \n",
    "    \n",
    "    for idx,percentile_num in enumerate([5,10,20,33]):\n",
    "        '''We are adding 4 to make sure that we get the indices correct, we already added data from below percentiles'''\n",
    "        final_perc_gefs[:,:,idx], final_perc_ecmwf[:,:,idx], final_perc_unet_gefs[:,:,idx], final_perc_unet_ecmwf[:,:,idx], \\\n",
    "        final_perc_xg[:,:,idx],final_perc_emos[:,:,idx] = check_if_below_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_gefs_vals, u_ecmwf_vals,x_vals,emos_vals)\n",
    "\n",
    "    \n",
    "    for idx,percentile_num in enumerate([66,80,90,95]):\n",
    "        '''We are adding 4 to make sure that we get the indices correct, we already added data from below percentiles'''\n",
    "        final_perc_gefs[:,:,idx+4], final_perc_ecmwf[:,:,idx+4], final_perc_unet_gefs[:,:,idx+4], final_perc_unet_ecmwf[:,:,idx+4], \\\n",
    "        final_perc_xg[:,:,idx+4],final_perc_emos[:,:,idx+4] = check_if_above_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_gefs_vals, u_ecmwf_vals,x_vals,emos_vals)\n",
    "\n",
    "\n",
    "    #Save files for later use\n",
    "    np.save(save_ecmwf,final_perc_ecmwf)\n",
    "    np.save(save_gefs, final_perc_gefs)\n",
    "    np.save(save_unet_gefs, final_perc_unet_gefs)\n",
    "    np.save(save_unet_ecmwf, final_perc_unet_ecmwf)\n",
    "    np.save(save_xg, final_perc_xg)\n",
    "    np.save(save_emos, final_perc_emos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb7b68-d7d5-403b-bd25-d1ab74a69da0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_binary_occurrence_data(week_lead,unet_experiment,region_name):\n",
    "    if percentile == 20:\n",
    "        idPercentile = 2\n",
    "    elif percentile == 10:\n",
    "        idPercentile = 1\n",
    "    elif percentile == 5:\n",
    "        idPercentile = 0\n",
    "    elif percentile == 33:\n",
    "        idPercentile = 3\n",
    "    dir_ = f'Data/correct_anomaly_percentile_statistics/{region_name}'\n",
    "    all_files = sorted(glob(f'{dir_}/Wk{week_lead}_testing*{unet_experiment}*Idist*'))\n",
    "    ecm_file_unet = [i for i in all_files if 'ECMWF' in i][0]\n",
    "    gef_file_unet = [i for i in all_files if 'ECMWF' not in i][0]\n",
    "\n",
    "    ecm_bin_unet = np.load(ecm_file_unet)[:,:,:,idPercentile]\n",
    "    gef_bin_unet = np.load(gef_file_unet)[:,:,:,idPercentile]\n",
    "    \n",
    "    # break\n",
    "    ecmwf_bin = f'{dir_}/Wk{week_lead}_ecmwf_stats_TP_FP_ensemble_meanIdist.npy'\n",
    "    ecmwf_bin = np.load(ecmwf_bin)[:,:,:,idPercentile]\n",
    "    \n",
    "    gefs_bin = f'{dir_}/Wk{week_lead}_gefs_stats_TP_FP_ensemble_meanIdist.npy'\n",
    "    gefs_bin = np.load(gefs_bin)[:,:,:,idPercentile]\n",
    "\n",
    "    # save_xg = f'{dir_}/Wk{week_lead}_xgboost_stats_TP_FP_ensemble_meanIdist.npy'\n",
    "    # save_emos = f'{dir_}/Wk{week_lead}_emos_stats_TP_FP_ensemble_meanIdist.npy'\n",
    "\n",
    "    obs_bin = np.load(f'{dir_}/Wk{week_lead}_obs_stats_TP_FP_ensemble_meanIdist.npy')\n",
    "    obs_bin = obs_bin[:,:,:,idPercentile]\n",
    "    return (ecm_bin_unet,gef_bin_unet, ecmwf_bin, gefs_bin, obs_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c157b2-7266-46b8-aa4e-5b6722c31ed6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def setup_percentile_difference_of_TP_with_ensemble_mean_with_own_distribution(region_name, \n",
    "                                                                               test_start, test_end, unet_experiment,percentile):\n",
    "    \n",
    "    ''' FORECAST ANOMALY - OBSERVATION ANOMALY'''\n",
    "    #Test \n",
    "    # week_lead=3\n",
    "    # percentile = 20\n",
    "\n",
    "    #Save dir\n",
    "    save_dir = f'Data/correct_anomaly_percentile_statistics/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    percentage_dict = {}\n",
    "\n",
    "    for week_lead in [3,4,5]:\n",
    "        print(f'Working on lead {week_lead}')\n",
    "        percentage_dict[f'Week {week_lead}'] = {}\n",
    "        \n",
    "        save_ecmwf = f'{save_dir}/Wk{week_lead}_ecmwf_stats_TP_percentile_difference_ensemble_meanIdist.npy'\n",
    "        save_gefs = f'{save_dir}/Wk{week_lead}_gefs_stats_TP_percentile_difference_ensemble_meanIdist.npy'\n",
    "        save_unet_gefs = f'{save_dir}/Wk{week_lead}_{unet_experiment}_stats_TP_percentile_difference_ensemble_meanIdist.npy'\n",
    "        \n",
    "        day_num = (week_lead*7) -1\n",
    "\n",
    "        '''What we need for this operation\n",
    "        1.) OBS percentile values\n",
    "        2.) Raw ECMWF and GEFSv12 percentile values\n",
    "        3.) Percentile values of Experiment UNet\n",
    "        4.) Binary values for the specific percentile'''\n",
    "    \n",
    "        '''1.) OBS percentile values'''\n",
    "        percentile_files = xr.open_mfdataset('/glade/work/klesinger/FD_RZSM_deep_learning/Data/GLEAM/RZSM_percentile_reformat/*',combine='nested',concat_dim=['S']).sel(S=slice(test_start,test_end)).sel(L=day_num).mean(dim='M')\n",
    "        percentile_files_obs = percentile_files.RZSM_percentile.values\n",
    "        percentile_files_obs.shape\n",
    "    \n",
    "        '''2.) Raw ECMWF and GEFSv12 percentile values'''\n",
    "        raw_ecmwf_percentiles = ecmwf_perc.RZSM.sel(L=day_num).values\n",
    "        raw_gefs_percentiles = gefs_perc.RZSM.sel(L=day_num).values\n",
    "        raw_gefs_percentiles.shape\n",
    "    \n",
    "        '''2.) Percentile values of Experiment UNet'''\n",
    "        percentile_unet = xr.open_mfdataset(f'/glade/work/klesinger/FD_RZSM_deep_learning/Data/UNET_percentiles_MEM/{unet_experiment}/*',combine='nested',concat_dim=['S']).sel(S=slice(test_start,test_end)).sel(L=day_num)\n",
    "        percentile_unet = percentile_unet.RZSM.values\n",
    "        percentile_unet.shape\n",
    "        \n",
    "        '''4.) Raw ECMWF and GEFSv12 percentile values'''\n",
    "        ecm_bin_unet,gef_bin_unet, ecmwf_bin, gefs_bin, obs_bin = load_binary_occurrence_data(week_lead,unet_experiment,region_name)\n",
    "\n",
    "        percentage_dict[f'Week {week_lead}']['OBS'] = np.nansum(obs_bin,axis=0)\n",
    "        \n",
    "        '''Now feed them all into the function to find the difference where there is a 1 for less than the percentile'''\n",
    "    \n",
    "        obs_perc = np.where(percentile_files_obs<=20,percentile_files_obs,np.nan)\n",
    "        gefs_perc1 = np.where((percentile_files_obs<=20) & (raw_gefs_percentiles<=20),raw_gefs_percentiles,np.nan)\n",
    "        ecmwf_perc1 = np.where((percentile_files_obs<=20) & (raw_ecmwf_percentiles<=20),raw_ecmwf_percentiles,np.nan)\n",
    "        gef_perc_unet = np.where((percentile_files_obs<=20) & (percentile_unet<=20),percentile_unet,np.nan) #Only doing GEFSv12, haven't done the percentiles for UNET ECMWF EX29\n",
    "    \n",
    "        #Now Forecast - Observations of the percentiles\n",
    "        fin_gefs = gefs_perc1 - obs_perc\n",
    "        fin_ecmwf = ecmwf_perc1 - obs_perc\n",
    "        fin_unet_gefs = gef_perc_unet - obs_perc\n",
    "\n",
    "        fin_gefs = np.where(mask_anom == 1,fin_gefs,np.nan)\n",
    "        fin_ecmwf = np.where(mask_anom == 1,fin_ecmwf,np.nan)\n",
    "        fin_unet_gefs = np.where(mask_anom == 1,fin_unet_gefs,np.nan)\n",
    "\n",
    "        percentage_dict[f'Week {week_lead}']['GEFSv12'] = np.nanmean(fin_gefs,axis=0)\n",
    "        percentage_dict[f'Week {week_lead}']['ECMWF'] = np.nanmean(fin_ecmwf,axis=0)\n",
    "        percentage_dict[f'Week {week_lead}']['DL-DM_G'] = np.nanmean(fin_unet_gefs,axis=0)\n",
    "    \n",
    "        final_perc_gefs = np.zeros(shape=(48,96)) #Adding 8 channels for the different anomaly spreads\n",
    "        final_perc_gefs[:,:] = np.nanmean(fin_gefs,axis=0)\n",
    "        final_perc_gefs\n",
    "        \n",
    "        final_perc_ecmwf = final_perc_gefs.copy()\n",
    "        final_perc_ecmwf[:,:] = np.nanmean(fin_ecmwf,axis=0)\n",
    "        \n",
    "        final_perc_unet_gefs = final_perc_gefs.copy()\n",
    "        final_perc_unet_gefs[:,:] = np.nanmean(fin_unet_gefs,axis=0)\n",
    "    \n",
    "        #Save files for later use\n",
    "        np.save(save_ecmwf,final_perc_ecmwf)\n",
    "        np.save(save_gefs, final_perc_gefs)\n",
    "        np.save(save_unet_gefs, final_perc_unet_gefs)\n",
    "\n",
    "    return(percentage_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d81d0-6233-4f1f-906c-f0f37b4409d8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_by_percentile_difference_for_hitRate_CONUS_with_own_distribution(percentage_dict,percentile,experiment='EX29'):\n",
    "\n",
    "    save_dir = f'Outputs/hit_rate/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    #Keep only 1 instance of the observation counts below the 20th percentile\n",
    "    rmax_, rmin_, omax_,omin_ = get_min_max_without_obs(hitRate = percentage_dict)\n",
    "        \n",
    "    # global_max, global_min = max(max_), min(min_)\n",
    "    v_obs = np.linspace(1, omax_, 8, endpoint=True,dtype=int)\n",
    "    v_ref = np.linspace(rmin_, rmax_, 15, endpoint=True)\n",
    "    pos_vals = [i for i in v_ref if i >0.0]\n",
    "    neg_vals = [i for i in v_ref if i <0.0]\n",
    "    neg_vals.append(0.0)\n",
    "    \n",
    "    v_ref = neg_vals + pos_vals\n",
    "    \n",
    "    norm_ref = TwoSlopeNorm(0.0, vmin=v_ref[0], vmax=v_ref[-1])\n",
    "\n",
    "    cmap = 'RdBu'\n",
    "    cmap_obs = 'YlOrRd'\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows = len(percentage_dict.keys()), ncols= 4, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(10, 5), dpi=300)\n",
    "    \n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    lon = mask.X.values\n",
    "    lat = mask.Y.values\n",
    "    grid_label_fontsize = 7\n",
    "    \n",
    "    axs_start = 0\n",
    "    for lead in percentage_dict.keys():\n",
    "        for model in percentage_dict[lead].keys():\n",
    "            if ('EMOS' in model) or ('XGBoost' in model) or (f'{experiment}_ECMWF' in model):\n",
    "                pass\n",
    "            else:\n",
    "                data = percentage_dict[lead][model]\n",
    "                map = Basemap(projection='cyl', llcrnrlat=lat[-1] - 1.5, urcrnrlat=lat[0],\n",
    "                              llcrnrlon=(lon[0] - 360 - 6), urcrnrlon=(lon[-1] - 360 + 20), resolution='l')\n",
    "                \n",
    "                x, y = map(*np.meshgrid(lon, lat))\n",
    "                # Adjust the text coordinates based on the actual data coordinates\n",
    "\n",
    "                if model == 'OBS':\n",
    "                    data = np.where(data==0,np.nan,data)\n",
    "                    obs = axs[axs_start].contourf(x, y, data, levels=v_obs, extend='both',\n",
    "                      transform=ccrs.PlateCarree(), cmap=cmap_obs)\n",
    "                else:\n",
    "                    data = np.where(data==0,np.nan,data)\n",
    "                    im = axs[axs_start].contourf(x, y, data, levels=v_ref, extend='both',\n",
    "                                          transform=ccrs.PlateCarree(), cmap=cmap,norm=norm_ref)\n",
    "                \n",
    "                gl = axs[axs_start].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                           linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "                gl.top_labels = False\n",
    "                gl.right_labels = False\n",
    "                \n",
    "                if axs_start in [0,4,8]:\n",
    "                    gl.left_labels = True\n",
    "                \n",
    "                gl.xlabel_style = {'fontsize': grid_label_fontsize}  # Set the fontsize for the x labels\n",
    "                gl.ylabel_style = {'fontsize': grid_label_fontsize}  # Set the fontsize for the y labels\n",
    "                   \n",
    "                gl.xformatter = LongitudeFormatter()\n",
    "                gl.yformatter = LatitudeFormatter()\n",
    "                axs[axs_start].coastlines()\n",
    "                axs[axs_start].set_aspect('equal')  # this makes the plots better\n",
    "                if model == 'OBS':\n",
    "                    axs[axs_start].set_title(f'GLEAM {lead}',fontsize=8)\n",
    "                elif model == f'{experiment}_GEFSv12':\n",
    "                    axs[axs_start].set_title(f'DL-DM_G {lead}',fontsize=8)\n",
    "                else:\n",
    "                    axs[axs_start].set_title(f'{model} {lead}',fontsize=8)\n",
    "                axs_start+=1\n",
    "\n",
    "    # Creating colorbars  [left, bottom, width, height] \n",
    "    cbar_ax_obs = fig.add_axes([0.038, -0.02, .21, .03])\n",
    "    cbar_ax_ref = fig.add_axes([0.27, -0.02, .7, .03])\n",
    "\n",
    "    cbar_obs = fig.colorbar(obs, cax=cbar_ax_obs, orientation='horizontal')\n",
    "    cbar_ref = fig.colorbar(im, cax=cbar_ax_ref, orientation='horizontal')\n",
    "\n",
    "    cbar_obs.set_label('Forecast Weeks < 20th percentile')\n",
    "    cbar_ref.set_label('Percentile Difference')\n",
    "    \n",
    "    # cbar1 = fig.colorbar(im, ax=axs[:4], orientation='horizontal', pad=0.05, fraction=0.04)\n",
    "    # cbar2 = fig.colorbar(im, ax=axs[9:], orientation='horizontal', pad=0.05, fraction=0.04)\n",
    "    # cbar_ax = fig.add_axes([0.05, 0.0, .9, .04])\n",
    "\n",
    "    # plt.text(-95, 30, 'B', fontsize=12, color='black', weight='bold')\n",
    "    # plt.text(-65, 30, 'B', fontsize=12, color='black', weight='bold')\n",
    "    # plt.text(-45, 30, 'C', fontsize=12, color='black', weight='bold')\n",
    "    # plt.text(-45, 30, 'D', fontsize=12, color='black', weight='bold')\n",
    "    \n",
    "    # Draw the colorbar\n",
    "    # cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    plt.suptitle(f'Percentile difference ( <{percentile}th percentile ) ensemble mean', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(f'Saving into {save_dir}/{experiment}_percentile_diff_{percentile}th_percentileIdist.png')\n",
    "    plt.savefig(f'{save_dir}/{experiment}_percentile_diff_{percentile}th_percentileIdist.png',bbox_inches='tight')\n",
    "\n",
    "    return(0)\n",
    "\n",
    "# plot_by_percentile_difference_for_hitRate_CONUS_with_own_distribution(percentage_dict,percentile,experiment='EX29')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73b1b8-e462-4f4d-9f0f-1078350a2f45",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb472bae-e5c5-47cc-8b15-3d9aa89a64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if region_name == 'CONUS':\n",
    "    unet_experiment='EX29'\n",
    "    percentile=20\n",
    "    percentage_dict = setup_percentile_difference_of_TP_with_ensemble_mean_with_own_distribution(region_name, \n",
    "                                                                                   test_start, test_end, unet_experiment,percentile)\n",
    "    plot_by_percentile_difference_for_hitRate_CONUS_with_own_distribution(percentage_dict,percentile,experiment='EX29')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06de96-fed6-4822-b5bf-726fdfe4c5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93e7f055-424b-425e-a12c-f749f9e72be5",
   "metadata": {},
   "source": [
    "# Plot difference for CONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b1ecd0-c3dc-476d-80a2-ec2f7e75b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot hit rate by grid cell'''\n",
    "\n",
    "'''CREATE a ROC figure for classification below the 20th percentile'''\n",
    "\n",
    "'''Only for 20th percentile'''\n",
    "percentile = 20\n",
    "\n",
    "#First save all data\n",
    "def return_difference_data_by_percentile(percentile, unet_experiment):\n",
    "\n",
    "    #where files are saved\n",
    "    dir_ = f'Outputs/hit_rate_stats/{region_name}'\n",
    "    \n",
    "    save_dir = f'Outputs/hit_rate_stats/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    percentile_list = [5,10,20,33,66,80,90,95] #These are also the indices of the percentile values\n",
    "\n",
    "\n",
    "    if percentile == 20:\n",
    "        idPercentile = 2\n",
    "    elif percentile == 10:\n",
    "        idPercentile = 1\n",
    "    elif percentile == 5:\n",
    "        idPercentile = 0\n",
    "    elif percentile == 33:\n",
    "        idPercentile = 3\n",
    "\n",
    "    anomaly_diff = {}\n",
    "\n",
    "    for week_lead in [3,4,5]:\n",
    "        all_files = sorted(glob(f'{dir_}/Wk{week_lead}_testing*{unet_experiment}*difference_ensemble_mean*'))\n",
    "        ecm_file_unet = [i for i in all_files if 'ECMWF' in i][0]\n",
    "        gef_file_unet = [i for i in all_files if 'ECMWF' not in i][0]\n",
    "        \n",
    "        anomaly_diff[f'Week {week_lead}'] = {}\n",
    "        \n",
    "        # break\n",
    "        save_ecmwf = f'{dir_}/Wk{week_lead}_ecmwf_stats_TP_difference_ensemble_mean.npy'\n",
    "        save_gefs = f'{dir_}/Wk{week_lead}_gefs_stats_TP_difference_ensemble_mean.npy'\n",
    "        save_unet_gefs = gef_file_unet\n",
    "        save_unet_ecmwf = ecm_file_unet\n",
    "        save_xg = f'{dir_}/Wk{week_lead}_xgboost_stats_TP_difference_ensemble_mean.npy'\n",
    "        save_emos = f'{dir_}/Wk{week_lead}_emos_stats_TP_difference_ensemble_mean.npy'\n",
    "        \n",
    "    \n",
    "        for ref_name, ref_file in zip(['GEFSv12', 'ECMWF', 'XGBoost', f'{unet_experiment}_GEFSv12', f'{unet_experiment}_ECMWF','EMOS' ],[save_gefs, save_ecmwf, save_xg, save_unet_gefs, save_unet_ecmwf, save_emos]):\n",
    "            # break\n",
    "            \n",
    "            out_plot = np.zeros(shape=(48,96))\n",
    "            # ax = axes[ax_start]\n",
    "            \n",
    "            # break\n",
    "            reforecast = np.load(ref_file)\n",
    "            reforecast.shape\n",
    "            ref_subset = reforecast[:,:,idPercentile]\n",
    "            ref_subset.shape\n",
    "            anomaly_diff[f'Week {week_lead}'][ref_name] = ref_subset\n",
    "     \n",
    "    return(anomaly_diff,unet_experiment,percentile)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e67445-987e-4f5d-9eb9-e921907aec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_min_Max_all_values(all_values):\n",
    "    max_ = []\n",
    "    min_ = []\n",
    "    for i in range(len(all_values)):\n",
    "        max_.append(max(all_values[i][0]))\n",
    "        min_.append(min(all_values[i][0]))\n",
    "    return(max_,min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5da6a6-b29c-4750-a52e-a2ba0ae67d15",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reorder_nested_dict(anomaly_diff, desired_order):\n",
    "    reordered_dict = {}\n",
    "    for k in anomaly_diff.keys():\n",
    "        # break\n",
    "        reordered_dict[k] = {}\n",
    "        for key in desired_order:\n",
    "            # break\n",
    "            if key in anomaly_diff[k]:\n",
    "                if isinstance(anomaly_diff[k][key], dict):\n",
    "                    reordered_dict[k][key] = reorder_nested_dict(anomaly_diff[k][key], desired_order)\n",
    "                else:\n",
    "                    reordered_dict[k][key] = anomaly_diff[k][key]\n",
    "    return reordered_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbadec8-3aef-4171-ba3b-6a99083258b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_percentile_for_difference_CONUS(anomaly_diff,hitRate,experiment,percentile):\n",
    "\n",
    "    '''The hitRate object must run a few code blocks above. This has the counts for the number of observation forecasts below the 20th percentile during the testing period'''\n",
    "    save_dir = f'Outputs/hit_rate/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    #Keep only 1 instance of the observation counts below the 20th percentile\n",
    "    _, _, omax_,omin_ = get_min_max_without_obs(hitRate)\n",
    "    v_obs = np.linspace(1, omax_, 8, endpoint=True,dtype=int)\n",
    "    cmap_obs = 'YlOrRd'\n",
    "    \n",
    "    all_values = [value for inner_dict in anomaly_diff.values() for value in inner_dict.values()]\n",
    "    \n",
    "    max_,min_ = return_min_Max_all_values(all_values)\n",
    "    global_max, global_min = max(max_), min(min_)\n",
    "    v = np.linspace(global_min, global_max, 20, endpoint=True)\n",
    "    \n",
    "    pos_vals = [i for i in v if i >0]\n",
    "    neg_vals = [i for i in v if i <0]\n",
    "\n",
    "    neg_vals.append(0)\n",
    "    v = neg_vals + pos_vals\n",
    "    norm = TwoSlopeNorm(0, vmin=v[0], vmax=v[-1])\n",
    "\n",
    "\n",
    "    anomaly_diff['Week 3']['OBS'] = hitRate['Week 3']\n",
    "    anomaly_diff['Week 4']['OBS'] = hitRate['Week 4']\n",
    "    anomaly_diff['Week 5']['OBS'] = hitRate['Week 5']\n",
    "\n",
    "    desired_order = ['OBS', 'GEFSv12', 'ECMWF', 'EX27_ECMWF', 'EX27_GEFSv12','XGBoost','EMOS']\n",
    "    anomaly_diff=reorder_nested_dict(anomaly_diff, desired_order)\n",
    "\n",
    "    cmap = 'bwr'\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows = 3, ncols= 4, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(10, 5), dpi=300)\n",
    "    \n",
    "    axs = axs.flatten()\n",
    "    grid_label_fontsize = 7\n",
    "    \n",
    "    lon = mask.X.values\n",
    "    lat = mask.Y.values\n",
    "    \n",
    "    axs_start = 0\n",
    "    for lead in anomaly_diff.keys():\n",
    "        for model in anomaly_diff[lead].keys():\n",
    "            if ('EMOS' in model) or ('XGBoost' in model) or (f'{experiment}_GEFSv12' in model):\n",
    "                pass\n",
    "            elif 'OBS' in model:\n",
    "                data = hitRate[lead]['OBS']\n",
    "                data = np.where(data==0, np.nan, data)\n",
    "                map = Basemap(projection='cyl', llcrnrlat=lat[-1] - 1.5, urcrnrlat=lat[0],\n",
    "                          llcrnrlon=(lon[0] - 360 - 6), urcrnrlon=(lon[-1] - 360 + 20), resolution='l')\n",
    "                x, y = map(*np.meshgrid(lon, lat))\n",
    "                obs = axs[axs_start].contourf(x, y, data, levels=v_obs, extend='both',\n",
    "                                  transform=ccrs.PlateCarree(), cmap=cmap_obs)\n",
    "            else:\n",
    "                data = anomaly_diff[lead][model]\n",
    "                map = Basemap(projection='cyl', llcrnrlat=lat[-1] - 1.5, urcrnrlat=lat[0],\n",
    "                              llcrnrlon=(lon[0] - 360 - 6), urcrnrlon=(lon[-1] - 360 + 20), resolution='l')\n",
    "                x, y = map(*np.meshgrid(lon, lat))\n",
    "                im = axs[axs_start].contourf(x, y, data, levels=v, extend='both',\n",
    "                                  transform=ccrs.PlateCarree(), cmap=cmap, norm =norm)\n",
    "                \n",
    "            if ('EMOS' in model) or ('XGBoost' in model) or (f'{experiment}_GEFSv12' in model):\n",
    "                pass\n",
    "            else:\n",
    "                gl = axs[axs_start].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                           linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "                gl.top_labels = False\n",
    "                gl.right_labels = False\n",
    "                if 'OBS' in model:\n",
    "                    gl.left_labels = True\n",
    "                else:\n",
    "                    gl.left_labels = False\n",
    "                    \n",
    "                gl.xlabel_style = {'fontsize': grid_label_fontsize}  # Set the fontsize for the x labels\n",
    "                gl.ylabel_style = {'fontsize': grid_label_fontsize}  # Set the fontsize for the y labels\n",
    "                    \n",
    "                gl.xformatter = LongitudeFormatter()\n",
    "                gl.yformatter = LatitudeFormatter()\n",
    "                axs[axs_start].coastlines()\n",
    "                axs[axs_start].set_aspect('equal')  # this makes the plots better\n",
    "                if 'OBS' in model:\n",
    "                    axs[axs_start].set_title(f'GLEAM {lead}',fontsize=8)\n",
    "                    \n",
    "                else:\n",
    "                    axs[axs_start].set_title(f'{model} {lead}',fontsize=8)\n",
    "                axs_start+=1\n",
    "    \n",
    "    # Creating colorbars  [left, bottom, width, height] \n",
    "    cbar_ax_obs = fig.add_axes([0.045, -0.03, .21, .03])\n",
    "    cbar_ax_ref = fig.add_axes([0.31, -0.03, .66, .03])\n",
    "\n",
    "    cbar_obs = fig.colorbar(obs, cax=cbar_ax_obs, orientation='horizontal')\n",
    "    cbar_ref = fig.colorbar(im, cax=cbar_ax_ref, orientation='horizontal')\n",
    "\n",
    "    cbar_obs.set_label('Forecast Weeks < 20th percentile')\n",
    "    cbar_ref.set_label('Anomaly Difference (m3/m3)')\n",
    "\n",
    "    plt.suptitle(f'Anomaly Difference ( <{percentile}th percentile ) ensemble mean \\n (FCST - OBS anomaly)', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'{save_dir}/{experiment}_anom_difference_{percentile}th_percentile.png',bbox_inches='tight')\n",
    "\n",
    "    return(0)\n",
    "\n",
    "\n",
    "'''RUN'''\n",
    "percentile = 20\n",
    "if region_name == 'CONUS':\n",
    "    anomaly_diff, experiment,percentile =  return_difference_data_by_percentile(percentile=percentile,unet_experiment='EX27')\n",
    "    plot_by_percentile_for_difference_CONUS(anomaly_diff,hitRate,experiment,percentile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83353d-cbf1-4f73-aad1-88134d6f04fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b2b74d-a5de-48ec-8120-51a9c3aac823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3c7d35-6124-449d-9ae1-365fe29b3041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df655280-3038-4c41-86ea-3269a8fa8ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61afa5da-edc3-4da2-aed1-795f08cd961b",
   "metadata": {},
   "source": [
    "# Extra code for plotting other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea964a2-5245-4ac6-8bc5-e8e79c57e65f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def setup_improvement_over_baseline_by_region_image(metric_dict,week_lead, region_name,metric_name,xlim_start,xlim_end):\n",
    "    #Need to make the data into an array where:\n",
    "    # ROWS = number of different realizations (so approximatley 15)\n",
    "    # COLS = values (ACC or CRPS)\n",
    "\n",
    "    save_dir = f'Outputs/joyplots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    # to return a group of the key-value\n",
    "    # # pairs in the dictionary\n",
    "    # result = metric_dict.items()\n",
    "     \n",
    "    # # Convert object to a list\n",
    "    # data = list(result)\n",
    "     \n",
    "    # # Convert list to an array\n",
    "    # numpyArray = np.array(data)\n",
    "     \n",
    "    # # print the numpy array\n",
    "    # print(numpyArray)\n",
    "    #convert to a dataframe\n",
    "    df = pd.DataFrame(metric_dict).T\n",
    "\n",
    "    if metric_name == 'CRPS':\n",
    "        joypy.joyplot(df.T,colormap=cm.autumn_r,\n",
    "                     title=f\"{metric_name} by Experiment Week {week_lead}\",\n",
    "                     x_range=(0,0.05))\n",
    "    else:\n",
    "        joypy.joyplot(df.T,colormap=cm.autumn_r,\n",
    "             title=f\"{metric_name} by Experiment Week {week_lead}\",\n",
    "                     x_range=(xlim_start,xlim_end))\n",
    "        \n",
    "    plt.savefig(f'{save_dir}/Wk{week_lead}_{metric_name}.png')\n",
    "\n",
    "    # colormap=cm.autumn_r\n",
    "    # list_df = []\n",
    "    # name_df = list(df.index)\n",
    "    \n",
    "    # for i in range(len(df)):\n",
    "    #     list_df.append(np.array(df.iloc[i,:]))\n",
    "\n",
    "    # fig = ridgeplot(samples = list_df,\n",
    "    #                labels=name_df)\n",
    "    # fig.update_layout(height=500, width=800)\n",
    "    # fig.show()\n",
    "\n",
    "    #Need to find the number of columns based on gefs\n",
    "\n",
    "    return('Completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff9888-78ec-4901-9e40-91dbf966ea55",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def ridgeplot_ACC_MEM(week_lead, region_name, test_start, test_end):\n",
    "\n",
    "#     day_num = (week_lead*7) -1\n",
    "    \n",
    "#     print('Loading observation and baseline anomaly files')\n",
    "#     obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only = open_obs_and_baseline_files(region_name=region_name, lead=week_lead, day_num=day_num,\n",
    "#                                                                                                     test_start = test_start, test_end = test_end)\n",
    "#     metric_dict = {}\n",
    "    \n",
    "#     #Test \n",
    "#     gefs = baseline_anomaly\n",
    "#     ecmwf = baseline_ecmwf\n",
    "#     obs = obs_anomaly_SubX_format\n",
    "\n",
    "#     obs = obs.mean(dim='M')\n",
    "#     gefs = gefs.mean(dim='M')\n",
    "#     ecmwf = ecmwf.mean(dim='M')\n",
    "\n",
    "    \n",
    "#     #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "#     gefs_acc = verifications.anomaly_correlation_coefficient_function_ensemble_mean(var_OUT=np.empty_like(var_OUT), \n",
    "#                                                                                     forecast_converted=gefs[putils.xarray_varname(gefs)].to_numpy(),\n",
    "#                                                                                     obs_converted=obs[putils.xarray_varname(obs)].to_numpy()).flatten()\n",
    "    \n",
    "#     ecmwf_acc = verifications.anomaly_correlation_coefficient_function_ensemble_mean(var_OUT=np.empty_like(var_OUT), \n",
    "#                                                                                     forecast_converted=ecmwf[putils.xarray_varname(ecmwf)].to_numpy(),\n",
    "#                                                                                     obs_converted=obs[putils.xarray_varname(obs)].to_numpy()).flatten()\n",
    "#     #First get any missing values from ECMWF to properly construct ridgeplot\n",
    "#     ec_not_nan = ~np.isnan(ecmwf_acc)\n",
    "\n",
    "#     gefs_acc = gefs_acc[ec_not_nan]\n",
    "#     ecmwf_acc = ecmwf_acc[ec_not_nan]\n",
    "    \n",
    "#     metric_dict.update({'GEFSv12': gefs_acc, 'ECMWF': ecmwf_acc})\n",
    "    \n",
    "#     #Now for all predictions from UNET, make the ACC\n",
    "#     unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*denseLar*'))\n",
    "\n",
    "#     #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "#     for i in unet_files:\n",
    "#         test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "#         test =  verifications.reverse_min_max_scaling(np.load(i), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "#         test = np.reshape(test,(test.shape[0]//11,11,test.shape[1],test.shape[2]))\n",
    "#         #Take the mean of models\n",
    "#         test = np.nanmean(test,axis=1)\n",
    "#         unet_acc = verifications.anomaly_correlation_coefficient_function_ensemble_mean(var_OUT=np.empty_like(var_OUT), \n",
    "#                                                                                 forecast_converted=test,\n",
    "#                                                                                 obs_converted=obs[putils.xarray_varname(obs)].to_numpy()).flatten()\n",
    "\n",
    "#         unet_acc = unet_acc[ec_not_nan]\n",
    "        \n",
    "#         metric_dict.update({test_name: unet_acc})\n",
    "    \n",
    "#     setup_ridgeplot_array(metric_dict,week_lead, region_name,metric_name='ACC')\n",
    "\n",
    "#     return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ed251-ca22-4db0-b48d-7dc826a851d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f121e1-76db-4bf0-9885-c4c691a1143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridgeplot_ACC_climpred_only_UNET(week_lead, region_name, test_start, test_end, single_experiment_or_all_experiments):\n",
    "\n",
    "    \n",
    "    day_num = (week_lead*7) -1\n",
    "    \n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    #For a single lead\n",
    "    obs, gefs, ecmwf, var_OUT_overwrite, template_testing_only_by_lead= select_data_by_lead(obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only, day_num)\n",
    "\n",
    "    print('Calculating ACC on GEFSv12 and ECMWF')\n",
    "    gefs_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "    gefs_BC_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(additive_bias_gefs_final.sel(L=day_num).expand_dims({'L':1})), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    ecmwf_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "    ecmwf_BC_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(additive_bias_ecmwf_final.sel(L=day_num).expand_dims({'L':1})), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    def flatten_files(file):\n",
    "        return(file[putils.xarray_varname(file)].values.flatten())\n",
    "\n",
    "    print('Masking files with np.nan values')\n",
    "    gefs_acc = flatten_files(gefs_acc)\n",
    "    gefs_BC_acc = flatten_files(gefs_BC_acc)\n",
    "    ecmwf_acc = flatten_files(ecmwf_acc)\n",
    "    ecmwf_BC_acc = flatten_files(ecmwf_BC_acc)\n",
    "    \n",
    "    #First get any missing values from ECMWF to properly construct ridgeplot\n",
    "    ec_not_nan = ~np.isnan(ecmwf_acc)\n",
    "\n",
    "    gefs_acc = gefs_acc[ec_not_nan]\n",
    "    gefs_BC_acc = gefs_BC_acc[ec_not_nan]\n",
    "    ecmwf_acc = ecmwf_acc[ec_not_nan]\n",
    "    ecmwf_BC_acc = ecmwf_BC_acc[ec_not_nan]\n",
    "\n",
    "    def add_to_dataframe(df, file, source_or_name):\n",
    "        a=pd.DataFrame()\n",
    "        a[source_or_name] =file\n",
    "        return(pd.concat([df,a],axis=1))\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    df = add_to_dataframe(df, ecmwf_acc, 'ECMWF')\n",
    "    df = add_to_dataframe(df, ecmwf_BC_acc, 'ECMWF_BC')\n",
    "    df = add_to_dataframe(df, gefs_acc, 'GEFSv12')\n",
    "    df = add_to_dataframe(df, gefs_BC_acc, 'GEFSv12_BC')\n",
    "\n",
    "    if single_experiment_or_all_experiments == 'all':\n",
    "        #Now for all predictions from UNET, make the ACC\n",
    "        #Can add *denseLar* for only the dense ones\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*')) #With a specific subset of data\n",
    "    else:\n",
    "        unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*{single_experiment_or_all_experiments}*')) #Will all data\n",
    "    \n",
    "   \n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in unet_files:\n",
    "        test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "        test =  verifications.reverse_min_max_scaling(np.load(i), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "        test = np.reshape(test,(test.shape[0]//11,11,test.shape[1],test.shape[2]))\n",
    "        test = np.expand_dims(test, -1)\n",
    "        #Now re-order the dimensions to match SubX\n",
    "        load_ =  np.reshape(test,(test.shape[0], test.shape[1], test.shape[-1], test.shape[2], test.shape[3]))\n",
    "        add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = load_\n",
    "        unet_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "        unet_acc = unet_acc.acc.values.flatten()\n",
    "        unet_acc = unet_acc[ec_not_nan]\n",
    "        df = add_to_dataframe(df, unet_acc, test_name)\n",
    "        \n",
    "\n",
    "    save_dir = f'Outputs/joyplots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    #Now plot all on 1 figure (Cant do it with joyplot)\n",
    "    # fig,axs = plt.subplots(nrows=1,ncols=3,figsize = (10,6))\n",
    "    # axs = axs.flatten()\n",
    "\n",
    "    '''I see that the persistence and climatology is the exact same for every dataset, so let's remove those and simply add as another'''\n",
    "    if single_experiment_or_all_experiments == 'all':\n",
    "        joypy.joyplot(df,colormap=cm.autumn_r,\n",
    "                     title=f\"ACC by Experiment Week {week_lead}\",\n",
    "                     fade=True,figsize=(10,30))\n",
    "    else:\n",
    "        joypy.joyplot(df,colormap=cm.autumn_r,\n",
    "             title=f\"ACC by Experiment Week {week_lead}\",\n",
    "             fade=True,figsize=(5,10))\n",
    "\n",
    "\n",
    "    plt.savefig(f'{save_dir}/Wk{week_lead}_ACC_climpred.png')\n",
    "\n",
    "    # for idx,i in enumerate(['Forecast','Climatology','Persistence']):\n",
    "    #     # break\n",
    "    #     plot_ = out_df[out_df['Reference'] == i]\n",
    "        \n",
    "    #     joypy.joyplot(plot_,colormap=cm.autumn_r,\n",
    "    #                  title=f\"{i} by Experiment Week {week_lead}\",\n",
    "    #                  fade=True)\n",
    "    #     plt.savefig(f'{save_dir}/Wk{week_lead}_{i}.png')\n",
    "\n",
    "        \n",
    "    # setup_ridgeplot_array(metric_dict_acc,week_lead, region_name, metric_name='ACC_climpred')\n",
    "    # setup_ridgeplot_array(metric_dict_persist,week_lead, region_name, metric_name='ACC_persistence')\n",
    "    \n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16fa6aa-1625-4c63-99e7-4be8731e9bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30493b95-5301-4674-9689-d374929f40d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca10b5-0efd-4e07-8476-b2ab20c82899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    # ridgeplot_ACC_climpred(week_lead, region_name, test_start, test_end, single_experiment_or_all_experiments = 'EX27')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fedaa-ff8e-458c-83ee-5e73c520d6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6312d14d-9c61-4934-ba44-2e7226a2a280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5a5d3-8d49-445a-a139-587c0119d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for week_lead in [3,4,5]:\n",
    "    unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*')) #With a specific subset of data\n",
    "    unet_files = [i for i in unet_files if 'final_mean_ensemble' not in i]\n",
    "    unet_files = [i for i in unet_files if 'mean' not in i]\n",
    "    for unet_file in unet_files:\n",
    "        evalute_percentiles_ensemble_mean(week_lead, region_name, test_start, test_end,unet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09958156-e5ca-4503-a072-6fb6974c331c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3921cf9b-037e-43bf-becd-294c2c76e443",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ridgeplot_CRPS(week_lead, region_name, test_start, test_end):\n",
    "\n",
    "    day_num = (week_lead*7) -1\n",
    "    \n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only = open_obs_and_baseline_files(region_name=region_name, lead=week_lead, day_num=day_num,\n",
    "                                                                                                    test_start = test_start, test_end = test_end)\n",
    "    metric_dict = {}\n",
    "    \n",
    "    #Test \n",
    "    gefs = baseline_anomaly\n",
    "    ecmwf = baseline_ecmwf\n",
    "    obs = obs_anomaly_SubX_format\n",
    "\n",
    "    #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "    gefs_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crps.values.flatten()\n",
    "    \n",
    "    #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "    ecmwf_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crps.values.flatten()\n",
    "    \n",
    "    #First get any missing values from ECMWF to properly construct ridgeplot\n",
    "    ec_not_nan = ~np.isnan(ecmwf_acc)\n",
    "\n",
    "    gefs_acc = gefs_acc[ec_not_nan]\n",
    "    ecmwf_acc = ecmwf_acc[ec_not_nan]\n",
    "    \n",
    "    metric_dict.update({'GEFSv12': gefs_acc, 'ECMWF': ecmwf_acc})\n",
    "\n",
    "\n",
    "    #Now add EMOS results\n",
    "    if region_name == 'CONUS':\n",
    "       \n",
    "        #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "        emos_acc = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').sel(lead=day_num).crps.values.flatten()\n",
    "\n",
    "        emos_final = emos_acc[ec_not_nan]\n",
    "        metric_dict.update({'EMOS':emos_final})\n",
    "    \n",
    "    #Now for all predictions from UNET, make the ACC\n",
    "    unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*denseLar*'))\n",
    "\n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in unet_files:\n",
    "        # break\n",
    "        add_to_file = gefs.copy(deep = True)\n",
    "        if 'mean' in i:\n",
    "            # break\n",
    "            if 'final_mean_ensemble' in i:\n",
    "                # break\n",
    "                #Still working here\n",
    "                test_name = i.split('testing_')[-1].split('.npy')[0].split('ensemble_')[-1]\n",
    "                load_ = np.expand_dims(np.load(i),-1)\n",
    "                load_.shape\n",
    "                load_ = np.reshape(load_,(load_.shape[0],load_.shape[-2], load_.shape[-1], load_.shape[1], load_.shape[2]))\n",
    "                load_.shape\n",
    "                load_ = np.where(load_ == 0,np.nan,load_)\n",
    "                if 'ECMWF' in i:\n",
    "                    new_source = 'ECMWF'\n",
    "                else:\n",
    "                    new_source = 'GEFSv12'\n",
    "                load_ = verifications.reverse_min_max_scaling(load_, region_name, day_num,new_source,test_year)\n",
    "                add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = load_\n",
    "                \n",
    "                unet_persist = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crps.values.flatten()\n",
    "\n",
    "                unet_persist = unet_persist[ec_not_nan]\n",
    "                metric_dict.update({test_name:unet_persist})\n",
    "\n",
    "        else:\n",
    "        \n",
    "            test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "            test =  verifications.reverse_min_max_scaling(np.load(i), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "            test = np.reshape(test,(test.shape[0]//11,11,test.shape[1],test.shape[2]))\n",
    "            test = np.expand_dims(test, -1)\n",
    "            #Now re-order the dimensions to match SubX\n",
    "            load_ =  np.reshape(test,(test.shape[0], test.shape[1], test.shape[-1], test.shape[2], test.shape[3]))\n",
    "            add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = load_\n",
    "            unet_persist = verifications.create_climpred_CRPS(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crps.values.flatten()\n",
    "\n",
    "            unet_persist = unet_persist[ec_not_nan]\n",
    "            metric_dict.update({test_name:unet_persist})\n",
    "    \n",
    "    setup_ridgeplot_array(metric_dict,week_lead, region_name, metric_name='CRPS')\n",
    "\n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb6876-4929-4797-b43f-88f7bd48ee5f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ridgeplot_standard_deviation(week_lead, region_name, test_start, test_end):\n",
    "\n",
    "    day_num = (week_lead*7) -1\n",
    "    \n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only = open_obs_and_baseline_files(region_name=region_name, lead=week_lead, day_num=day_num,\n",
    "                                                                                                    test_start = test_start, test_end = test_end)\n",
    "    metric_dict = {}\n",
    "    \n",
    "    #Test \n",
    "    gefs = baseline_anomaly\n",
    "    ecmwf = baseline_ecmwf\n",
    "    obs = obs_anomaly_SubX_format\n",
    "\n",
    "    #Find the standard deviation of the ensemble forecast for each day\n",
    "    gefs_std = gefs.std(dim='M',skipna=True).mean(dim='S').isel(L=0).RZSM.values.flatten()\n",
    "    #check the output (looks good)\n",
    "    # gefs.RZSM[0,:,0,10,10].values\n",
    "    # np.nanmean(gefs.RZSM[0,:,0,10,10].values)\n",
    "    # np.nanstd(gefs.RZSM[0,:,0,10,10].values)\n",
    "    # gefs_std.RZSM[0,0,10,10].values\n",
    "\n",
    "    ecmwf_std = ecmwf.std(dim='M',skipna=True).mean(dim='S').isel(L=0).RZSM.values.flatten()\n",
    "\n",
    "    #First get any missing values from ECMWF to properly construct ridgeplot\n",
    "    ec_not_nan = ecmwf_std!=0\n",
    "\n",
    "    gefs_std = gefs_std[ec_not_nan]\n",
    "    ecmwf_std = ecmwf_std[ec_not_nan]\n",
    "    \n",
    "    metric_dict.update({'GEFSv12': gefs_std, 'ECMWF': ecmwf_std})\n",
    "\n",
    "    #Now add EMOS results\n",
    "    if region_name == 'CONUS':\n",
    "       \n",
    "        #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "        emos_acc = emos_.std(dim='M',skipna=True).mean(dim='S').sel(L=day_num).RZSM.values.flatten()\n",
    "\n",
    "        emos_final = emos_acc[ec_not_nan]\n",
    "        metric_dict.update({'EMOS':emos_final})\n",
    "    \n",
    "    #Now for all predictions from UNET, make the ACC\n",
    "    unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*denseLar*'))\n",
    "\n",
    "\n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in unet_files:\n",
    "        # break\n",
    "        add_to_file = gefs.copy(deep = True)\n",
    "        if 'mean' in i:\n",
    "            # break\n",
    "            if 'final_mean_ensemble' in i:\n",
    "                # break\n",
    "                #Still working here\n",
    "                test_name = i.split('testing_')[-1].split('.npy')[0].split('ensemble_')[-1]\n",
    "                load_ = np.expand_dims(np.load(i),-1)\n",
    "                load_.shape\n",
    "                load_ = np.reshape(load_,(load_.shape[0],load_.shape[-2], load_.shape[-1], load_.shape[1], load_.shape[2]))\n",
    "                load_.shape\n",
    "                load_ = np.where(load_ == 0,np.nan,load_)\n",
    "                if 'ECMWF' in i:\n",
    "                    new_source = 'ECMWF'\n",
    "                else:\n",
    "                    new_source = 'GEFSv12'\n",
    "                load_ = verifications.reverse_min_max_scaling(load_, region_name, day_num,new_source,test_year)\n",
    "                add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = load_\n",
    "                \n",
    "                unet_std =  add_to_file.std(dim='M',skipna=True).mean(dim='S').isel(L=0).RZSM.values.flatten()\n",
    "                unet_std = unet_std[ec_not_nan]\n",
    "                metric_dict.update({test_name:unet_std})\n",
    "\n",
    "        else:\n",
    "        \n",
    "            test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "            if 'ECMWF' in i:\n",
    "                new_source = 'ECMWF'\n",
    "            else:\n",
    "                new_source = 'GEFSv12'\n",
    "            test =  verifications.reverse_min_max_scaling(np.load(i), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "            test = np.reshape(test,(test.shape[0]//11,11,test.shape[1],test.shape[2]))\n",
    "            test = np.expand_dims(test, -1)\n",
    "            #Now re-order the dimensions to match SubX\n",
    "            load_ =  np.reshape(test,(test.shape[0], test.shape[1], test.shape[-1], test.shape[2], test.shape[3]))\n",
    "            add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = load_\n",
    "                \n",
    "            unet_std =  add_to_file.std(dim='M',skipna=True).mean(dim='S').isel(L=0).RZSM.values.flatten()\n",
    "            unet_std = unet_std[ec_not_nan]\n",
    "            metric_dict.update({test_name:unet_std})\n",
    "\n",
    "    xlim_start, xlim_end = 0,0.04\n",
    "    setup_ridgeplot_array(metric_dict,week_lead, region_name, metric_name='Standard Deviation',xlim_start=xlim_start,xlim_end=xlim_end)\n",
    "\n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb239d8-b6f3-44f7-987b-57080efef367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_percentage_of_forecasts_that_contain_the_observation(week_lead, region_name, test_start, test_end):\n",
    "\n",
    "    day_num = (week_lead*7) -1\n",
    "    \n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only = open_obs_and_baseline_files(region_name=region_name, lead=week_lead, day_num=day_num,\n",
    "                                                                                                    test_start = test_start, test_end = test_end)\n",
    "    metric_dict = {}\n",
    "    \n",
    "    #Test \n",
    "    gefs = baseline_anomaly\n",
    "    ecmwf = baseline_ecmwf\n",
    "    obs = obs_anomaly_SubX_format\n",
    "\n",
    "    obs_value = obs_anomaly_SubX_format.mean(dim='M')\n",
    "\n",
    "    #Find if forecast ensemble contains the value\n",
    "\n",
    "    def find_if_forecast_contains_obs(forecast,obs):\n",
    "        \n",
    "        max_ = forecast.max(dim='M')\n",
    "        min_ = forecast.min(dim='M')\n",
    "\n",
    "        correct = xr.where((obs_value<max_) & (obs_value>min_),1,0)\n",
    "        percent = (correct.sum(dim='S')/len(correct.S.values)).isel(L=0).RZSM.values.flatten()\n",
    "        \n",
    "        return(percent)\n",
    "    \n",
    "    \n",
    "    gefs_perc = find_if_forecast_contains_obs(gefs,obs_value)\n",
    "\n",
    "    ecmwf_perc = find_if_forecast_contains_obs(ecmwf,obs_value)\n",
    "\n",
    "    #First get any missing values from ECMWF to properly construct ridgeplot\n",
    "    ec_not_nan = ecmwf_perc!=0\n",
    "\n",
    "    gefs_perc = gefs_perc[ec_not_nan]\n",
    "    ecmwf_perc = ecmwf_perc[ec_not_nan]\n",
    "    \n",
    "    metric_dict.update({'GEFSv12': gefs_perc, 'ECMWF': ecmwf_perc})\n",
    "    \n",
    "    #Now for all predictions from UNET, make the ACC\n",
    "    unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*denseLar*'))\n",
    "\n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in unet_files:\n",
    "        # break\n",
    "        add_to_file = gefs.copy(deep = True)\n",
    "        test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "        test =  verifications.reverse_min_max_scaling(np.load(i), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "        test = np.reshape(test,(test.shape[0]//11,11,test.shape[1],test.shape[2]))\n",
    "        test = np.expand_dims(test, -1)\n",
    "        #Now re-order the dimensions to match SubX\n",
    "        test =  np.reshape(test,(test.shape[0], test.shape[1], test.shape[-1], test.shape[2], test.shape[3]))\n",
    "        \n",
    "        add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = test\n",
    "\n",
    "        unet_perc = find_if_forecast_contains_obs(add_to_file,obs_value)\n",
    "        \n",
    "        unet_perc = unet_perc[ec_not_nan]\n",
    "        \n",
    "        metric_dict.update({test_name: unet_perc})\n",
    "    \n",
    "    setup_ridgeplot_array(metric_dict,week_lead, region_name, metric_name='Percent of Inits within Ensemble Spread')\n",
    "\n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ed519-2ad5-4ff1-beb7-dcb3558cb979",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ridgeplot_crpss_climpred(week_lead, region_name, test_start, test_end):\n",
    "   \n",
    "    day_num = (week_lead*7) -1\n",
    "    \n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only = open_obs_and_baseline_files(region_name=region_name, lead=week_lead, day_num=day_num,\n",
    "                                                                                                    test_start = test_start, test_end = test_end)\n",
    "    metric_dict_acc = {}\n",
    "    metric_dict_persist = {}\n",
    "    \n",
    "    #Test \n",
    "    gefs = baseline_anomaly\n",
    "    ecmwf = baseline_ecmwf\n",
    "    obs = obs_anomaly_SubX_format\n",
    "\n",
    "\n",
    "    #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "    # gefs_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original)).acc.values.flatten()\n",
    "    gefs_c = verifications.create_climpred_crpss_skill_score(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crpss.values.flatten()\n",
    "\n",
    "    #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "    ecmwf_c = verifications.create_climpred_crpss_skill_score(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crpss.values.flatten()\n",
    "\n",
    "    #First get any missing values from ECMWF to properly construct ridgeplot\n",
    "    ec_not_nan = ~np.isnan(ecmwf_c)\n",
    "\n",
    "    gefs_c = gefs_c[ec_not_nan]\n",
    "    ecmwf_c = ecmwf_c[ec_not_nan]\n",
    "\n",
    "\n",
    "    #Now stack each of the them in a dataframe \n",
    "\n",
    "    def add_to_dataframe(df, file, source_or_name):\n",
    "        a=pd.DataFrame()\n",
    "        a[source_or_name] =file\n",
    "        return(pd.concat([df,a],axis=1))\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    df = add_to_dataframe(df, ecmwf_c, 'ECMWF')\n",
    "    df = add_to_dataframe(df, gefs_c, 'GEFSv12')\n",
    "    \n",
    "    #Now for all predictions from UNET, make the ACC\n",
    "    unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*denseLar*'))\n",
    "    # unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*'))\n",
    "    \n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in unet_files:\n",
    "        if 'denseLargeMean_RZSM' in i:\n",
    "            pass\n",
    "        else:\n",
    "            # break\n",
    "            add_to_file = gefs.copy(deep = True)\n",
    "            test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "            test =  verifications.reverse_min_max_scaling(np.load(i), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "            test = np.reshape(test,(test.shape[0]//11,11,test.shape[1],test.shape[2]))\n",
    "            test = np.expand_dims(test, -1)\n",
    "            #Now re-order the dimensions to match SubX\n",
    "            test =  np.reshape(test,(test.shape[0], test.shape[1], test.shape[-1], test.shape[2], test.shape[3]))\n",
    "\n",
    "            add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = test\n",
    "            \n",
    "            unet_c = verifications.create_climpred_crpss_skill_score(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crpss.values.flatten()\n",
    "    \n",
    "            unet_c = unet_c[ec_not_nan]\n",
    "    \n",
    "            # out_df = pd.concat([out_df.drop('Reference',axis=1),unet_vals],axis=1)\n",
    "            df = add_to_dataframe(df, unet_c, test_name)\n",
    "\n",
    "    save_dir = f'Outputs/joyplots/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    #Now plot all on 1 figure (Cant do it with joyplot)\n",
    "    # fig,axs = plt.subplots(nrows=1,ncols=3,figsize = (10,6))\n",
    "    # axs = axs.flatten()\n",
    "\n",
    "    '''I see that the persistence and climatology is the exact same for every dataset, so let's remove those and simply add as another'''\n",
    "    joypy.joyplot(df,colormap=cm.autumn_r,\n",
    "                 title=f\"CRPSS by Experiment Week {week_lead}\",\n",
    "                 fade=True,\n",
    "                 x_range=(-5,1.1))\n",
    "    plt.savefig(f'{save_dir}/Wk{week_lead}_CRPSS_climpred.png')\n",
    "\n",
    "    # for idx,i in enumerate(['Forecast','Climatology','Persistence']):\n",
    "    #     # break\n",
    "    #     plot_ = out_df[out_df['Reference'] == i]\n",
    "        \n",
    "    #     joypy.joyplot(plot_,colormap=cm.autumn_r,\n",
    "    #                  title=f\"{i} by Experiment Week {week_lead}\",\n",
    "    #                  fade=True)\n",
    "    #     plt.savefig(f'{save_dir}/Wk{week_lead}_{i}.png')\n",
    "\n",
    "        \n",
    "    # setup_ridgeplot_array(metric_dict_acc,week_lead, region_name, metric_name='ACC_climpred')\n",
    "    # setup_ridgeplot_array(metric_dict_persist,week_lead, region_name, metric_name='ACC_persistence')\n",
    "    \n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2900e39-36d5-4a46-8828-68fe26f4f5bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ridgeplot_crpss_ensemble_spread_climpred(week_lead, region_name, test_start, test_end):\n",
    "\n",
    "\n",
    "    metric_dict = {}\n",
    "    \n",
    "    day_num = (week_lead*7) -1\n",
    "    \n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only = open_obs_and_baseline_files(region_name=region_name, lead=week_lead, day_num=day_num,\n",
    "                                                                                                    test_start = test_start, test_end = test_end)\n",
    "    \n",
    "    #Test \n",
    "    gefs = baseline_anomaly\n",
    "    ecmwf = baseline_ecmwf\n",
    "    obs = obs_anomaly_SubX_format\n",
    "\n",
    "\n",
    "    #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "    # gefs_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original)).acc.values.flatten()\n",
    "    gefs_c = verifications.create_climpred_crpss_ensemble_spread(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crpss.values.flatten()\n",
    "\n",
    "    #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "    ecmwf_c = verifications.create_climpred_crpss_ensemble_spread(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crpss.values.flatten()\n",
    "\n",
    "    #First get any missing values from ECMWF to properly construct ridgeplot\n",
    "    ec_not_nan = ~np.isnan(ecmwf_c)\n",
    "\n",
    "    gefs_c = gefs_c[ec_not_nan]\n",
    "    ecmwf_c = ecmwf_c[ec_not_nan]\n",
    "\n",
    "    metric_dict.update({'GEFSv12': gefs_c, 'ECMWF': ecmwf_c})\n",
    "\n",
    "    #Now add EMOS results\n",
    "    if region_name == 'CONUS':\n",
    "       \n",
    "        #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "        emos_acc = verifications.create_climpred_crpss_ensemble_spread(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').sel(lead=day_num).crpss.values.flatten()\n",
    "\n",
    "        emos_final = emos_acc[ec_not_nan]\n",
    "        metric_dict.update({'EMOS':emos_final})\n",
    "    \n",
    "    \n",
    "    #Now for all predictions from UNET, make the ACC\n",
    "    unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*denseLar*'))\n",
    "    # unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*'))\n",
    "    \n",
    "    #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "    for i in unet_files:\n",
    "        # break\n",
    "        add_to_file = gefs.copy(deep = True)\n",
    "        if 'mean' in i:\n",
    "            # break\n",
    "            if 'final_mean_ensemble' in i:\n",
    "                # break\n",
    "                #Still working here\n",
    "                test_name = i.split('testing_')[-1].split('.npy')[0].split('ensemble_')[-1]\n",
    "                load_ = np.expand_dims(np.load(i),-1)\n",
    "                load_.shape\n",
    "                load_ = np.reshape(load_,(load_.shape[0],load_.shape[-2], load_.shape[-1], load_.shape[1], load_.shape[2]))\n",
    "                load_.shape\n",
    "                load_ = np.where(load_ == 0,np.nan,load_)\n",
    "                load_ = verifications.reverse_min_max_scaling(load_, region_name, day_num,source,test_year)\n",
    "                add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = load_\n",
    "                \n",
    "                unet_persist = verifications.create_climpred_crpss_ensemble_spread(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crpss.values.flatten()\n",
    "\n",
    "                unet_persist = unet_persist[ec_not_nan]\n",
    "                metric_dict.update({test_name:unet_persist})\n",
    "\n",
    "        else:\n",
    "        \n",
    "            test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "            test =  verifications.reverse_min_max_scaling(np.load(i), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "            test = np.reshape(test,(test.shape[0]//11,11,test.shape[1],test.shape[2]))\n",
    "            test = np.expand_dims(test, -1)\n",
    "            #Now re-order the dimensions to match SubX\n",
    "            load_ =  np.reshape(test,(test.shape[0], test.shape[1], test.shape[-1], test.shape[2], test.shape[3]))\n",
    "            add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = load_\n",
    "            unet_persist = verifications.create_climpred_crpss_ensemble_spread(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crpss.values.flatten()\n",
    "\n",
    "            unet_persist = unet_persist[ec_not_nan]\n",
    "            metric_dict.update({test_name:unet_persist})\n",
    "\n",
    "\n",
    "    xlim_start,xlim_end = -0.75,0.25\n",
    "    \n",
    "    setup_ridgeplot_array(metric_dict,week_lead, region_name, metric_name='CRPSS_ensemble_spread_climpred',xlim_start=xlim_start,xlim_end=xlim_end)\n",
    "\n",
    "    return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5a99c-fbe1-4c08-9833-5ec80ba65736",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def ridgeplot_realiability(week_lead, region_name, test_start, test_end):\n",
    "   \n",
    "#     day_num = (week_lead*7) -1\n",
    "    \n",
    "#     print('Loading observation and baseline anomaly files')\n",
    "#     obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only = open_obs_and_baseline_files(region_name=region_name, lead=week_lead, day_num=day_num,\n",
    "#                                                                                                     test_start = test_start, test_end = test_end)\n",
    "#     metric_dict_acc = {}\n",
    "#     metric_dict_persist = {}\n",
    "    \n",
    "#     #Test \n",
    "#     gefs = baseline_anomaly\n",
    "#     ecmwf = baseline_ecmwf\n",
    "#     obs = obs_anomaly_SubX_format\n",
    "\n",
    "\n",
    "#     #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "#     # gefs_acc = verifications.create_climpred_ACC(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original)).acc.values.flatten()\n",
    "#     gefs_c = verifications.create_realiability_forecasts(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crpss.values.flatten()\n",
    "\n",
    "#     #First get the ACC values of GEFS and ECMWF relative to observations\n",
    "#     ecmwf_c = verifications.create_climpred_crpss_ensemble_spread(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crpss.values.flatten()\n",
    "\n",
    "#     #First get any missing values from ECMWF to properly construct ridgeplot\n",
    "#     ec_not_nan = ~np.isnan(ecmwf_c)\n",
    "\n",
    "#     gefs_c = gefs_c[ec_not_nan]\n",
    "#     ecmwf_c = ecmwf_c[ec_not_nan]\n",
    "\n",
    "\n",
    "#     #Now stack each of the them in a dataframe \n",
    "\n",
    "#     def add_to_dataframe(df, file, source_or_name):\n",
    "#         a=pd.DataFrame()\n",
    "#         a[source_or_name] =file\n",
    "#         return(pd.concat([df,a],axis=1))\n",
    "    \n",
    "#     df = pd.DataFrame()\n",
    "\n",
    "#     df = add_to_dataframe(df, ecmwf_c, 'ECMWF')\n",
    "#     df = add_to_dataframe(df, gefs_c, 'GEFSv12')\n",
    "    \n",
    "#     #Now for all predictions from UNET, make the ACC\n",
    "#     unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*denseLar*'))\n",
    "#     # unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*'))\n",
    "    \n",
    "#     #Now loop through and open file, convert to anomaly and compute ACC score\n",
    "#     for i in unet_files:\n",
    "#         add_to_file = gefs.copy(deep = True)\n",
    "#         if 'final_mean_ensemble' in i:\n",
    "#             #Still working here\n",
    "#             test_name = i.split('testing_')[-1].split('.npy')[0].split('ensemble_')[-1]\n",
    "#             load_ = np.load(i)\n",
    "#             load_ = np.reshape(load_,(load_.shape[0],load_.shape[-1], load_.shape[1], load_.shape[2]))\n",
    "#             load_ = np.where(load_ == 0,np.nan,load_)\n",
    "#             load_ = verifications.reverse_min_max_scaling(load_, region_name, day_num,source,test_year)\n",
    "#             break\n",
    "#         else:\n",
    "        \n",
    "#             test_name = i.split('testing_')[-1].split('.npy')[0]\n",
    "#             test =  verifications.reverse_min_max_scaling(np.load(i), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "#             test = np.reshape(test,(test.shape[0]//11,11,test.shape[1],test.shape[2]))\n",
    "#             test = np.expand_dims(test, -1)\n",
    "#             #Now re-order the dimensions to match SubX\n",
    "#             load_ =  np.reshape(test,(test.shape[0], test.shape[1], test.shape[-1], test.shape[2], test.shape[3]))\n",
    "            \n",
    "\n",
    "#         add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = load_\n",
    "        \n",
    "#         unet_c = verifications.create_climpred_crpss_ensemble_spread(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original)).mean(dim='init').crpss.values.flatten()\n",
    "\n",
    "#         unet_c = unet_c[ec_not_nan]\n",
    "\n",
    "#         # out_df = pd.concat([out_df.drop('Reference',axis=1),unet_vals],axis=1)\n",
    "#         df = add_to_dataframe(df, unet_c, test_name)\n",
    "\n",
    "#     save_dir = f'Outputs/joyplots/{region_name}'\n",
    "#     os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "#     #Now plot all on 1 figure (Cant do it with joyplot)\n",
    "#     # fig,axs = plt.subplots(nrows=1,ncols=3,figsize = (10,6))\n",
    "#     # axs = axs.flatten()\n",
    "\n",
    "#     '''I see that the persistence and climatology is the exact same for every dataset, so let's remove those and simply add as another'''\n",
    "#     joypy.joyplot(df,colormap=cm.autumn_r,\n",
    "#                  title=f\"CRPSS Ensemble Spread by Experiment Week {week_lead}\",\n",
    "#                  fade=True,\n",
    "#                  x_range=(-1,1))\n",
    "#     plt.savefig(f'{save_dir}/Wk{week_lead}_CRPSS_ensemble_spread_climpred.png')\n",
    "\n",
    "#     # for idx,i in enumerate(['Forecast','Climatology','Persistence']):\n",
    "#     #     # break\n",
    "#     #     plot_ = out_df[out_df['Reference'] == i]\n",
    "        \n",
    "#     #     joypy.joyplot(plot_,colormap=cm.autumn_r,\n",
    "#     #                  title=f\"{i} by Experiment Week {week_lead}\",\n",
    "#     #                  fade=True)\n",
    "#     #     plt.savefig(f'{save_dir}/Wk{week_lead}_{i}.png')\n",
    "\n",
    "        \n",
    "#     # setup_ridgeplot_array(metric_dict_acc,week_lead, region_name, metric_name='ACC_climpred')\n",
    "#     # setup_ridgeplot_array(metric_dict_persist,week_lead, region_name, metric_name='ACC_persistence')\n",
    "    \n",
    "#     return('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b21c44-426e-4cab-a9d7-a764bd2898ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def rank_histogram(week_lead, region_name, test_start, test_end):\n",
    "\n",
    "    save_dir = f'Outputs/rank_histogram/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    day_num = (week_lead*7) -1\n",
    "    \n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only = open_obs_and_baseline_files(region_name=region_name, lead=week_lead, day_num=day_num,\n",
    "                                                                                                    test_start = test_start, test_end = test_end)\n",
    "    metric_dict_acc = {}\n",
    "    metric_dict_persist = {}\n",
    "\n",
    "    #Must apply a mask because some have 0s and others have np.nan\n",
    "    mask = baseline_ecmwf.copy(deep=True)\n",
    "    \n",
    "    #Test \n",
    "    gefs = baseline_anomaly\n",
    "    ecmwf = baseline_ecmwf\n",
    "    obs = obs_anomaly_SubX_format\n",
    "\n",
    "    gefs = xr.where(mask == 0,np.nan,gefs)\n",
    "    obs = xr.where(mask == 0,np.nan,obs)\n",
    "    ecmwf = xr.where(mask == 0,np.nan,gefs)\n",
    "\n",
    "    gefs_r = verifications.create_rank_histogram(verifications.rename_subx_for_climpred(gefs), verifications.rename_obs_for_climpred(obs_original))\n",
    "    ecmwf_r = verifications.create_rank_histogram(verifications.rename_subx_for_climpred(ecmwf), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "    #Now add EMOS results\n",
    "    if region_name == 'CONUS':\n",
    "        emos_r= verifications.create_rank_histogram(verifications.rename_subx_for_climpred(emos_), verifications.rename_obs_for_climpred(obs_original)).sel(lead=day_num)\n",
    "        \n",
    "    add_to_file = gefs.copy(deep = True)\n",
    "\n",
    "\n",
    "    #Only choose a single UNET file\n",
    "    unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*EX27_denseLarge_RZSM*'))[0]\n",
    "    unet= verifications.reverse_min_max_scaling(np.load(unet_files), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "    \n",
    "    test_name = unet_files.split('testing_')[-1].split('.npy')[0]\n",
    "\n",
    "    unet = np.reshape(unet,(unet.shape[0]//11,11,unet.shape[1],unet.shape[2]))\n",
    "    unet = np.expand_dims(unet, -1)\n",
    "    #Now re-order the dimensions to match SubX\n",
    "    unet =  np.reshape(unet,(unet.shape[0], unet.shape[1], unet.shape[-1], unet.shape[2], unet.shape[3]))\n",
    "    add_to_file[putils.xarray_varname(add_to_file)][:,:,:,:,:] = unet\n",
    "\n",
    "    add_to_file = xr.where(mask == 0,np.nan,add_to_file)\n",
    "    \n",
    "    unet_r= verifications.create_rank_histogram(verifications.rename_subx_for_climpred(add_to_file), verifications.rename_obs_for_climpred(obs_original))\n",
    "\n",
    "            \n",
    "\n",
    "    if region_name == 'CONUS':\n",
    "        num_rows = 4\n",
    "    else:\n",
    "        num_rows = 3\n",
    "        \n",
    "    fig, axs = plt.subplots(1,num_rows, figsize=(20, 7))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    \n",
    "    if region_name == 'CONUS':\n",
    "        for ax,(data,name) in enumerate(zip([gefs_r, ecmwf_r, emos_r, unet_r], ['GEFSv12','ECMWF', 'EMOS','UNET_EX27'])):\n",
    "            # break\n",
    "            # ax+=1\n",
    "                   \n",
    "            to_df = data.rank_histogram[:].to_dataframe()\n",
    "            to_df['rank_histogram'] = to_df['rank_histogram'] / \\\n",
    "                to_df['rank_histogram'].sum()\n",
    "            to_df['rank'] = to_df.index\n",
    "            to_df['rank'] = to_df['rank'].astype(int)\n",
    "            to_df.index = to_df['rank']\n",
    "            del to_df['lead']\n",
    "            del to_df['skill']\n",
    "            del to_df['rank']\n",
    "            \n",
    "            print(f'Shape of to_df : {to_df.rank().shape[0]}')\n",
    "            # axs[ax].plot(to_df)\n",
    "            axs[ax].bar(np.arange(1,to_df.rank().shape[0]+1),to_df.rank_histogram)\n",
    "            axs[ax].set_xlim(1, 12)\n",
    "            axs[ax].set_ylim(0,0.7)\n",
    "    \n",
    "            # Optionally, adjust tick marks\n",
    "            axs[ax].set_xticks(np.arange(1, 13))\n",
    "            axs[ax].set_title(name)\n",
    "            axs[ax].set_xticklabels(axs[ax].get_xticklabels(), rotation=0)\n",
    "            axs[ax].set_ylabel('Relative Frequency', rotation=90)\n",
    "            \n",
    "    plt.suptitle(f'Week {week_lead} Rank Histogram', fontsize=30)\n",
    "    plt.tight_layout()\n",
    "    out_dir_save = f'{save_dir}/Week{week_lead}_rank_histogram.png'\n",
    "    plt.savefig(out_dir_save, dpi=300)\n",
    "\n",
    "    return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ecb36d-f810-4e04-b100-8d4dafe63bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561f3d1-425e-445f-8929-c963e428b511",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for week_lead in [3,4,5]:\n",
    "    ridgeplot_ACC_climpred(week_lead, region_name, test_start, test_end)\n",
    "    # ridgeplot_CRPS(week_lead, region_name, test_start, test_end)\n",
    "    # ridgeplot_standard_deviation(week_lead, region_name, test_start, test_end)\n",
    "    # find_percentage_of_forecasts_that_contain_the_observation(week_lead, region_name, test_start, test_end)\n",
    "    # ridgeplot_crpss_climpred(week_lead, region_name, test_start, test_end)\n",
    "    # ridgeplot_crpss_ensemble_spread_climpred(week_lead, region_name, test_start, test_end)\n",
    "    # rank_histogram(week_lead, region_name, test_start, test_end)\n",
    "    # evalute_percentiles_ensemble_mean(week_lead, region_name, test_start, test_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1eb4a-d1a6-4a82-8e9c-95e4836ab591",
   "metadata": {},
   "source": [
    "# ROC analysis\n",
    "\n",
    "### Reason why the plots don't have a curve:\n",
    "\n",
    "### Threshold Granularity: If the model outputs have limited granularity (i.e., a small number of unique score values), the ROC curve can appear as a series of straight lines, as each unique value creates a step change in the TPR and FPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3713ed85-de79-4a46-ae60-8bbf0e293bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup_percentiles_for_ROC_plot(week_lead, region_name, test_start, test_end):\n",
    "\n",
    "#     #Test \n",
    "#     # week_lead=1\n",
    "#     # percentile_eval = 20\n",
    "\n",
    "#     #Save dir\n",
    "#     save_dir = f'Data/correct_anomaly_percentile_statistics/{region_name}'\n",
    "#     os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "#     save_ecmwf = f'{save_dir}/Wk{week_lead}_ecmwf_stats_ROC.npy'\n",
    "#     save_gefs = f'{save_dir}/Wk{week_lead}_gefs_stats_ROC.npy'\n",
    "#     save_unet = f'{save_dir}/Wk{week_lead}_unet_stats_ROC.npy'\n",
    "#     save_xg = f'{save_dir}/Wk{week_lead}_xgboost_stats_ROC.npy'\n",
    "#     save_obs_binary = f'{save_dir}/Wk{week_lead}_obs_binary_stats_ROC.npy'\n",
    "\n",
    "#     if (os.path.exists(save_ecmwf) and os.path.exists(save_ecmwf) and os.path.exists(save_ecmwf) and os.path.exists(save_xg) and os.path.exists(save_obs_binary)):\n",
    "#         pass\n",
    "#     else:\n",
    "    \n",
    "#         day_num = (week_lead*7) -1\n",
    "        \n",
    "#         print('Loading observation and baseline anomaly files')\n",
    "#         obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only= open_obs_and_baseline_files(region_name=region_name, lead=week_lead, day_num=day_num,\n",
    "#                                                                                                         test_start = test_start, test_end = test_end)\n",
    "#         #Test \n",
    "#         gefs = baseline_anomaly\n",
    "#         ecmwf = baseline_ecmwf\n",
    "#         obs = obs_anomaly_SubX_format\n",
    "    \n",
    "        \n",
    "#         obs_percent = obs_anom_percentile.sel(L=day_num).sel(M=0)\n",
    "#         obs_percent['95th_percentile'].shape #(104, 48, 96)\n",
    "    \n",
    "#         #Get UNET prediction (single file)\n",
    "#         #Now for all predictions from UNET, make the ACC\n",
    "#         unet_files = sorted(glob(f'predictions/{region_name}/Wk{week_lead}_testing/*EX27_regular_RZSM*'))[0]\n",
    "        \n",
    "#         test_name = unet_files.split('testing_')[-1].split('.npy')[0].split('ensemble_')[-1]\n",
    "#         test =  verifications.reverse_min_max_scaling(np.load(unet_files), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "#         test = np.reshape(test,(test.shape[0]//11,11,test.shape[1],test.shape[2]))\n",
    "#         test.shape\n",
    "    \n",
    "#         #Now mask the input\n",
    "#         test_unet = np.where(mask_anom == 1,test,np.nan)\n",
    "\n",
    "#         #XGBoost\n",
    "#         #Load the XGBoost data\n",
    "#         xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{week_lead}_testing/*EX28*'))[0]\n",
    "\n",
    "#         # break\n",
    "#         #Still working here\n",
    "#         test_name = xgboost_files.split('testing_')[-1].split('.npy')[0]\n",
    "#         load_ = np.expand_dims(np.load(xgboost_files),-1)\n",
    "#         load_.shape\n",
    "#         load_ = np.where(load_ == 0,np.nan,load_)\n",
    "#         load_ =  verifications.reverse_min_max_scaling(load_, region_name, day_num,source,test_year)#We only want the last channel\n",
    "        \n",
    "#         xg = np.empty(shape=(load_.shape[0],11,load_.shape[1],load_.shape[2],load_.shape[3])) #This will help with climpred functions\n",
    "#         for j in range(11):\n",
    "#             xg[:,j,:,:,:] = load_\n",
    "\n",
    "#         xg = xg.squeeze()\n",
    "#         xg.shape\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "#         #Check if the predicted anomaly is below each threshold\n",
    "    \n",
    "#         file = baseline_anomaly\n",
    "#         file.RZSM.shape\n",
    "#         out_check_gefs_base = np.zeros(shape=(104,11,48,96,8)) #Adding 8 channels for the different anomaly spreads\n",
    "        \n",
    "#         out_check_gefs_base[:,:,:,:] = np.nan\n",
    "#         out_check_ecmwf_base = out_check_gefs_base.copy()\n",
    "#         out_check_unet = out_check_gefs_base.copy()\n",
    "#         out_check_xg = out_check_gefs_base.copy()\n",
    "        \n",
    "#         obs_binary_out =np.zeros(shape=(104,11,48,96,8)) #Adding 8 channels for the different anomaly spreads\n",
    "\n",
    "#         final_perc_gefs = np.zeros(shape=(104,48,96,8)) #Adding 8 channels for the different anomaly spreads\n",
    "#         final_perc_gefs[:,:,:] = np.nan\n",
    "#         final_perc_ecmwf = final_perc_gefs.copy()\n",
    "#         final_perc_unet = final_perc_gefs.copy()\n",
    "#         final_perc_xg = final_perc_gefs.copy()\n",
    "        \n",
    "#         #Test\n",
    "#         # idx = 0\n",
    "#         # mx = 0\n",
    "#         # ix = 10\n",
    "#         # iy =10 #NEGATIVE ANOMALY VALUE\n",
    "#         # iy =5 #POSITIVE ANOMALY VALUE\n",
    "\n",
    "#         #Use np.where to find the values of the percentile\n",
    "\n",
    "#         #Take ensemble mean\n",
    "#         # o_vals = np.nanmean(obs.RZSM[:,:,0,:,:].values,axis=1)\n",
    "#         # g_vals =  np.nanmean(gefs.RZSM[:,:,0,:,:].values,axis=1)\n",
    "#         # e_vals =  np.nanmean(ecmwf.RZSM[:,:,0,:,:].values,axis=1)\n",
    "#         # u_vals = np.nanmean(test_unet,axis=1)\n",
    "#         # x_vals = np.nanmean(xg,axis=1)\n",
    "\n",
    "#         #Keep all models the same\n",
    "#         o_vals = obs.RZSM[:,:,0,:,:].values\n",
    "#         g_vals =  gefs.RZSM[:,:,0,:,:].values\n",
    "#         e_vals =  ecmwf.RZSM[:,:,0,:,:].values\n",
    "#         u_vals = test_unet\n",
    "#         x_vals = xg\n",
    "\n",
    "#         u_vals.shape\n",
    "#         x_vals.shape\n",
    "\n",
    "#         def check_if_below_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_vals, x_vals):\n",
    "#             perc_= obs_percent[f'{percentile_num}th_percentile'].values\n",
    "#             #Just expand the percentiles to match the data\n",
    "#             perc_expanded = np.zeros(shape=(104,11,48,96))\n",
    "#             perc_expanded[:,:,:,:] = np.nan\n",
    "#             for i in range(11):\n",
    "#                 perc_expanded[:,i,:,:]  = perc_\n",
    "            \n",
    "            \n",
    "#             def find_percentage(perc_expanded,o_vals,fcst):\n",
    "#                 correct =  np.where((o_vals<perc_expanded)&(fcst<perc_expanded),1,0)\n",
    "#                 obs_binary = np.where((o_vals<perc_expanded),1,0)\n",
    "#                 '''Take the mean to find the probability of correct'''\n",
    "#                 correct = np.nanmean(correct,axis=1)\n",
    "#                 correct.shape #(104, 48, 96)\n",
    "#                 #Now mask the input of CONUS/region\n",
    "#                 correct = np.where(mask_anom == 1,correct,np.nan)\n",
    "\n",
    "#                 return(correct,obs_binary)\n",
    "\n",
    "#             g_perc,obs_binary = find_percentage(perc_expanded,o_vals,g_vals)\n",
    "#             g_perc.shape\n",
    "#             e_perc,obs_binary = find_percentage(perc_expanded,o_vals,e_vals)\n",
    "#             u_perc,obs_binary  = find_percentage(perc_expanded,o_vals,u_vals)\n",
    "#             x_perc,obs_binary  = find_percentage(perc_expanded,o_vals,x_vals)\n",
    "#             return(g_perc,e_perc,u_perc,x_perc,obs_binary )\n",
    "\n",
    "#         for idx,percentile_num in enumerate([5,10,20,33]):\n",
    "#             final_perc_gefs[:,:,:,idx], final_perc_ecmwf[:,:,:,idx], final_perc_unet[:,:,:,idx],final_perc_xg[:,:,:,idx],obs_binary_out[:,:,:,:,idx] = check_if_below_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_vals, x_vals)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#         def check_if_above_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_vals, x_vals):\n",
    "#             perc_= obs_percent[f'{percentile_num}th_percentile'].values\n",
    "#             #Just expand the percentiles to match the data\n",
    "#             perc_expanded = np.zeros(shape=(104,11,48,96))\n",
    "\n",
    "            \n",
    "#             perc_expanded[:,:,:,:] = np.nan\n",
    "#             for i in range(11):\n",
    "#                 perc_expanded[:,i,:,:]  = perc_\n",
    "            \n",
    "            \n",
    "#             def find_percentage(perc_expanded,o_vals,fcst):\n",
    "#                 correct =  np.where((o_vals>perc_expanded)&(fcst>perc_expanded),1,0)\n",
    "#                 '''Take the mean to find the probability of correct'''\n",
    "#                 obs_binary = np.where((o_vals>perc_expanded),1,0)\n",
    "                \n",
    "#                 correct = np.nanmean(correct,axis=1)\n",
    "#                 correct.shape #(104, 48, 96)\n",
    "#                 #Now mask the input of CONUS/region\n",
    "#                 correct = np.where(mask_anom == 1,correct,np.nan)\n",
    "\n",
    "#                 return(correct,obs_binary )\n",
    "\n",
    "#             g_perc,obs_binary  = find_percentage(perc_expanded,o_vals,g_vals)\n",
    "#             g_perc.shape\n",
    "#             e_perc,obs_binary  = find_percentage(perc_expanded,o_vals,e_vals)\n",
    "#             u_perc,obs_binary  = find_percentage(perc_expanded,o_vals,u_vals)\n",
    "#             x_perc,obs_binary  = find_percentage(perc_expanded,o_vals,x_vals)\n",
    "            \n",
    "#             return(g_perc,e_perc,u_perc,x_perc,obs_binary )\n",
    "        \n",
    "#         for idx,percentile_num in enumerate([66,80,90,95]):\n",
    "#             '''We are adding 4 to make sure that we get the indices correct, we already added data from below percentiles'''\n",
    "#             final_perc_gefs[:,:,:,idx+4], final_perc_ecmwf[:,:,:,idx+4], final_perc_unet[:,:,:,idx+4], final_perc_xg[:,:,:,idx+4],obs_binary_out[:,:,:,:,idx+4] = check_if_above_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_vals, x_vals)\n",
    "\n",
    "\n",
    "#         #Just save the obs_binary_out as the (init,lat,lon)\n",
    "#         obs_binary_out = np.nanmean(obs_binary_out,axis=1)\n",
    "        \n",
    "#         #Save files for later use\n",
    "#         np.save(save_ecmwf,final_perc_ecmwf)\n",
    "#         np.save(save_gefs, final_perc_gefs)\n",
    "#         np.save(save_unet, final_perc_unet)\n",
    "#         np.save(save_xg, final_perc_xg)\n",
    "#         np.save(save_obs_binary, obs_binary_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed480f81-84f8-498e-8eba-bc48c0407ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for week_lead in [1,2,3,4,5]:\n",
    "#     setup_percentiles_for_ROC_plot(week_lead, region_name, test_start, test_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352ac2b-be94-48d3-b6fb-cd65e4a73811",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# '''CREATE a ROC figure for classification below the 20th percentile'''\n",
    "\n",
    "# '''Only for 20th percentile'''\n",
    "# percentile = 5\n",
    "\n",
    "# if percentile == 20:\n",
    "#     idx = 2\n",
    "# elif percentile == 10:\n",
    "#     idx = 1\n",
    "# elif percentile == 5:\n",
    "#     idx = 0\n",
    "# elif percentile == 33:\n",
    "#     idx = 3\n",
    "\n",
    "\n",
    "# #where files are saved\n",
    "# dir_ = f'Data/correct_anomaly_percentile_statistics/{region_name}'\n",
    "\n",
    "# save_dir = f'Outputs/ROC/{region_name}'\n",
    "# os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "# percentile_list = [5,10,20,33,66,80,90,95] #These are also the indices of the percentile values\n",
    "\n",
    "# for week_lead in [3,4,5]:\n",
    "#     # break\n",
    "#     save_ecmwf = f'{dir_}/Wk{week_lead}_ecmwf_stats_ROC.npy'\n",
    "#     save_gefs = f'{dir_}/Wk{week_lead}_gefs_stats_ROC.npy'\n",
    "#     save_unet = f'{dir_}/Wk{week_lead}_unet_stats_ROC.npy'\n",
    "#     save_xg = f'{dir_}/Wk{week_lead}_xgboost_stats_ROC.npy'\n",
    "#     obs_binary = np.load(f'{dir_}/Wk{week_lead}_obs_binary_stats_ROC.npy')\n",
    "    \n",
    "\n",
    "#     num_regions = 6\n",
    "#     num_models = 4\n",
    "#     colors = ['b', 'g', 'r', 'c', 'm', 'y']  # Different colors for each region\n",
    "    \n",
    "#     fig, axes = plt.subplots(1, num_models, figsize=(15, 5), dpi=300) \n",
    "#     # axs = axes.flatten()\n",
    "    \n",
    "#     ax_start = 0\n",
    "#     for ref_name, ref_file in zip(['GEFSv12', 'ECMWF','UNET', 'XGBoost'],[save_gefs,save_ecmwf, save_unet, save_xg]):\n",
    "#         ax = axes[ax_start]\n",
    "        \n",
    "#         # Placeholder for ROC curve data\n",
    "#         fpr = {}\n",
    "#         tpr = {}\n",
    "#         roc_auc = {}\n",
    "        \n",
    "#         # break\n",
    "#         reforecast = np.load(ref_file)\n",
    "#         reforecast.shape\n",
    "#         ref_subset = reforecast[:,:,:,idx]\n",
    "#         ref_subset.shape\n",
    "\n",
    "#         obs_subset_binary = obs_binary[:,:,:,idx]\n",
    "        \n",
    "#         for region_num,name in CONUS_region_names.items():\n",
    "#             # break\n",
    "#             region_subset = np.where(region_mask==region_num,ref_subset,np.nan)\n",
    "#             obs_region_subset = np.where(region_mask==region_num,obs_subset_binary,np.nan)\n",
    "            \n",
    "#             predicted_probs_flat = region_subset.flatten()\n",
    "#             observed_events_flat = obs_region_subset.flatten()\n",
    "\n",
    "#             #Remove nans\n",
    "#             mask_nan =~np.isnan(observed_events_flat)\n",
    "#             observed_events_flat = observed_events_flat[mask_nan]\n",
    "#             predicted_probs_flat = predicted_probs_flat[mask_nan]\n",
    "#             #Set a threshold where at least 50 percent of the models must agree\n",
    "#             predicted_probs_flat = [0 if i <0.5 else i for i in predicted_probs_flat]\n",
    "            \n",
    "#             #Make a random probability sample\n",
    "#             # predicted_probs_flat = np.random.uniform(size=len(observed_events_flat))\n",
    "\n",
    "#             fpr[name], tpr[name], _ = roc_curve(observed_events_flat, predicted_probs_flat,pos_label=1)\n",
    "#             roc_auc[name] = auc(fpr[name], tpr[name])\n",
    "\n",
    "#         colors = iter(plt.cm.rainbow(np.linspace(0, 1, len(CONUS_region_names.values()))))\n",
    "#         for k,name in CONUS_region_names.items():\n",
    "#             # break\n",
    "#             ax.plot(fpr[name], tpr[name], color=next(colors), lw=2, label=f'{name} (area = {roc_auc[name]:.2f})')\n",
    "#             # plt.show()\n",
    "#         ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "#         # ax.xlim([0.0, 1.0])\n",
    "#         # ax.ylim([0.0, 1.05])\n",
    "#         ax.set_xlabel('False Positive Rate')\n",
    "#         ax.set_ylabel('True Positive Rate')\n",
    "#         ax.set_title(f'{ref_name}')\n",
    "#         ax.legend(loc=\"lower right\")\n",
    "#         ax_start+=1\n",
    "        \n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.suptitle(f'Week {week_lead}')\n",
    "#     plt.savefig(f'{save_dir}/Wk{week_lead}_ROC_{percentile}th_percentile.png')\n",
    "                        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deba99c1-746d-46a2-b689-20394a8fd992",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #Plot a single grid cell ROC\n",
    "\n",
    "# lat,lon = 10,10\n",
    "\n",
    "# reforecast = np.load(save_unet)\n",
    "# reforecast.shape\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(10,5,figsize=(10,20))\n",
    "# axs = axs.flatten()\n",
    "\n",
    "# count=0\n",
    "# for lon in np.arange(20,25):\n",
    "#     for lat in np.arange(10,20):\n",
    "        \n",
    "#         ref_subset = reforecast[:,lat,lon,idx]\n",
    "#         ref_subset.shape\n",
    "\n",
    "#         if np.count_nonzero(np.isnan(ref_subset)) == 104:\n",
    "#             pass\n",
    "#         else:\n",
    "        \n",
    "#             obs_subset_binary = obs_binary[:,lat,lon,idx]\n",
    "            \n",
    "#             ref_subset.shape\n",
    "            \n",
    "#             predicted_probs_flat = ref_subset.flatten()\n",
    "#             observed_events_flat = obs_subset_binary.flatten()\n",
    "            \n",
    "#             # Calculate ROC curve\n",
    "#             fpr, tpr, thresholds = roc_curve(observed_events_flat, predicted_probs_flat)\n",
    "            \n",
    "#             # Calculate AUC\n",
    "#             roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "#             axs[count].plot(fpr, tpr)\n",
    "#             axs[count].plot([0, 1], [0, 1], 'k--')\n",
    "#             axs[count].set_title(f'Lat: {lat} Lon: {lon}')\n",
    "#             axs[count].set_xlabel('False Positive Rate')\n",
    "#             axs[count].set_ylabel('True Positive Rate')\n",
    "#             axs[count].legend(loc=\"lower right\")\n",
    "#             count +=1\n",
    "\n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbe82f5-0344-4be0-94d1-13b9d75b2a42",
   "metadata": {},
   "source": [
    "# End of ROC analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500ad28-3323-4930-9065-a4561cac55a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf212gpu]",
   "language": "python",
   "name": "conda-env-tf212gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
