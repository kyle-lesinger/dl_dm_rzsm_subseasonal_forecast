{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7302f239-5aba-4164-8125-86df134c9bd5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mt\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxr\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "# import pandas as pd\n",
    "from glob import glob\n",
    "#import climpredNEW.climpred \n",
    "#from climpredNEW.climpred.options import OPTIONS\n",
    "from climpred.options import OPTIONS\n",
    "import climpred\n",
    "import pickle\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from numpy import meshgrid\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "import matplotlib.colors as mcolors\n",
    "import cartopy.feature as cfeature\n",
    "import itertools\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter, LatitudeLocator\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, TwoSlopeNorm\n",
    "from scipy.stats import rankdata\n",
    "import bottleneck as bn\n",
    "import scipy.stats as stats\n",
    "\n",
    "from function import preprocessUtils as putils\n",
    "from function import masks\n",
    "from function import verifications\n",
    "from function import funs as f\n",
    "from function import conf\n",
    "from function import loadbias\n",
    "from function import dataLoad\n",
    "from function import quikplot as qp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a9f743-2b10-4ed6-8037-d2ff7bcfd9b0",
   "metadata": {},
   "source": [
    "# Save ACC values for the later plots. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151bb10-b78b-472d-9a60-e062e22bbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = 'china' #or ['australia', 'CONUS', 'china']\n",
    "obs_source = 'GLEAM' #['ERA5','GLEAM']\n",
    "\n",
    "if obs_source == 'ERA5':\n",
    "    soil_dir = conf.era_data\n",
    "elif obs_source == 'GLEAM':\n",
    "    soil_dir = conf.gleam_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04abf3b7-3bda-4abe-b1d5-405f32a05f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "global obs_original,obs_raw\n",
    "obs_original,obs_raw = dataLoad.load_rzsm_observations(soil_dir, region_name)\n",
    "obs_original[\"time\"] = obs_original[\"time\"].dt.floor(\"D\")\n",
    "obs_raw[\"time\"] = obs_raw[\"time\"].dt.floor(\"D\")\n",
    "\n",
    "obs_anom_climp = verifications.rename_obs_for_climpred(obs_original)\n",
    "\n",
    "mask, mask_anom = masks.load_mask_vals(region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612950b1-1fcf-4522-b79c-7d85f090e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start_obs = '2000-01-01' #Beginning of observation period for analysis. We actually have data starting from 1999 so that we could have a 7-day rolling mean applied to the data and have up to 12 weeks lags for RZSM\n",
    "end_obs = '2020-12-31' #end of observations for ERA5 and GLEAM. We actually needed data through 2020-02-15 since we have an initialization on 2019-12-25\n",
    "start_testing = '2018-01-01' #Beginning of testing period\n",
    "end_testing = '2019-12-31'\n",
    "train_end_string = '2015-12-31' #last string date for training\n",
    "train_end = 2015 #last year of training dates\n",
    "\n",
    "\n",
    "global RZSM_or_Tmax_or_both\n",
    "RZSM_or_Tmax_or_both = 'RZSM' # for getting the predictor from either RZSM and Tmax ('both') or only RZSM ('RZSM')\n",
    "\n",
    "lead_select = [6,13,20,27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898fb0e0-f8ee-4f17-bc53-1398e00403b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_min_max(file,name):\n",
    "    print(name)\n",
    "    print(f'Maximum value in file is {np.nanmax(file[putils.xarray_varname(file)])}')\n",
    "    print(f'Minimum value in file is {np.nanmin(file[putils.xarray_varname(file)])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33006b80-df8e-4d6a-93ca-4ab7fb432eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "global verification_var\n",
    "verification_var = f'soilw_bgrnd_{obs_source}' #this is for what we are verifying with DL outputs\n",
    "\n",
    "#Gleam observations\n",
    "gleam_dir = f'{soil_dir}/{region_name}'\n",
    "\n",
    "#Forecast predictions\n",
    "gefsv12_fcst_dir = f'{conf.gefsv12_data}/{region_name}'\n",
    "\n",
    "#ERA5 observations\n",
    "era5_dir = f'{conf.era_data}/{region_name}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288d7ab-6c7c-47b5-b251-6a5067c7512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load observation anomaly\n",
    "gleam_anom = verifications.load_RZSM_anomaly_obs(region_name, soil_dir).load()\n",
    "ecmwf_anom = verifications.load_ECMWF_baseline_anomaly(region_name).load()\n",
    "gefs_anom = verifications.load_GEFSv12_baseline_anomaly(region_name).load()\n",
    "\n",
    "'''Load a base file to serve as the xarray template to add our predictions from UNET into.'''\n",
    "base_file_testing = gefs_anom.copy(deep=True).sel(S=slice(start_testing, None)).load()\n",
    "\n",
    "global lat, lon\n",
    "lat = base_file_testing.Y.values  # for plotting later\n",
    "lon = base_file_testing.X.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969c264e-626d-4153-b516-05b88f7b3445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load the percentile files from \n",
    "# ecmwf_perc = verifications.load_ECMWF_percentile_anomaly(region_name).load()\n",
    "# gefs_perc = verifications.load_GEFSv12_percentile_anomaly(region_name).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2506f7a-0e55-4576-a221-cd611731ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bias corrected data\n",
    "gef_bc_crpss, ecm_bc_crpss = loadbias.load_additive_bias_corrected_data_CRPSS(lead_select,region_name,obs_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b733db-dc18-4c90-b0d0-2e458932e706",
   "metadata": {},
   "source": [
    "# Now create baseline anomalies of files to not have to re-compute later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897f7c3-e10a-44a6-a5b6-a8936bc7ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First\n",
    "#Create seasonal anomaly from observations\n",
    "print(f'Creating the seasonal anomalies for observational data and then subsetting for everything after {start_testing} date.')\n",
    "save_anomaly_dir = f'Data/anomaly/{region_name}'\n",
    "os.system(f'mkdir -p {save_anomaly_dir}')\n",
    "\n",
    "obs_RZSM_save = f'{save_anomaly_dir}/obs_RZSM_anomaly_testing_{obs_source}.nc'\n",
    "ref_RZSM_save = f'{save_anomaly_dir}/reforecast_RZSM_anomaly_testing_{obs_source}.nc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf3c95-e085-40ff-96f6-4edc71b69559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file_create_seasonal_anomaly(path,train_end):\n",
    "    #Must subset by lead first (because we actually have data previously from past lag weeks)\n",
    "    return(create_seasonal_anomaly(xr.open_mfdataset(path).sel(L=slice(0,34)).rolling(L=7, min_periods=7,center=False).mean(),train_end=train_end,source='reforecast'))\n",
    "\n",
    "def check_values_in_file(file,lead):\n",
    "    '''Just print some values to verify the files aren't identical when comparing with other results'''\n",
    "    name_file = list(file.keys())[0]\n",
    "    return(print(file[name_file].isel(L=lead).isel(M=10).isel(S=0).values))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa259c4d-646c-4c4a-9901-21cc495dde82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_experiment_predictions_and_observations(lead,experiment,region_name,obs_source):\n",
    "    # #Test\n",
    "    # experiment='EX0'\n",
    "    day_num = (lead*7)-1\n",
    "    min_max_dir = f'Data/min_max_values/{region_name}'\n",
    "    verification_directory = f'Data/model_npy_input_data/{region_name}/Verification_data' #For observation verification\n",
    "    # bias_correction_dir = f'Data/bias_mean_values/Wk_{lead}'\n",
    "\n",
    "    ex_name = experiment\n",
    "\n",
    "\n",
    "    #Load the actual observations (used for the Mean Absolute Error calculation)\n",
    "    obs_final_train,obs_final_validation,obs_final_testing = f.load_verification_observations_updated(lead,verification_directory,obs_source)\n",
    "    obs_RZSM = np.array(obs_final_testing) #anomaly\n",
    "\n",
    "    #Convert observations 0 values to nan (only for the RZSM observations). These values had a zero where there is no land soil moisture\n",
    "    obs_RZSM = np.where(obs_RZSM == 0,np.nan,obs_RZSM)\n",
    "    \n",
    "    obs_RZSM =verifications.reverse_min_max_scaling(obs_RZSM,region_name,day_num,obs_source,2019)\n",
    "\n",
    "    predictions_directory = f'predictions/{region_name}/Wk{lead}_testing'\n",
    "\n",
    "    cont=False\n",
    "    ecmwf_present=False\n",
    "    if obs_source == 'GLEAM':\n",
    "        prediction_GEFS = np.load(f'{predictions_directory}/Wk{lead}_testing_{ex_name}_regular_RZSM.npy')\n",
    "        try:\n",
    "            prediction_ECMWF = np.load(f'{predictions_directory}/Wk{lead}_testing_{ex_name}_ECMWF_regular_RZSM.npy')\n",
    "            ecmwf_present=True\n",
    "        except FileNotFoundError:\n",
    "            prediction_ECMWF = np.empty_like(prediction_GEFS)\n",
    "            \n",
    "        cont = True\n",
    "    elif obs_source == 'ERA5':\n",
    "        prediction_GEFS = np.load(f'{predictions_directory}/Wk{lead}_testing_{ex_name}_regular_ERA5_RZSM.npy')\n",
    "        try:\n",
    "            prediction_ECMWF = np.load(f'{predictions_directory}/Wk{lead}_testing_{ex_name}_ECMWF_regular_ERA5_RZSM.npy')\n",
    "            ecmwf_present=True\n",
    "        except:\n",
    "            prediction_ECMWF = np.empty_like(prediction_GEFS)\n",
    "            \n",
    "        cont = True\n",
    "\n",
    "\n",
    "    if cont:\n",
    "        print(f'Test prediction shape: {prediction_GEFS.shape}')\n",
    "        prediction_RZSM_GEFS = verifications.reverse_min_max_scaling(prediction_GEFS[-1,:,:,:],region_name, day_num, 'GEFSv12',2019)\n",
    "        if ecmwf_present:\n",
    "            prediction_RZSM_ECMWF = verifications.reverse_min_max_scaling(prediction_ECMWF[-1,:,:,:],region_name, day_num, 'ECMWF',2019)\n",
    "        else:\n",
    "            prediction_RZSM_ECMWF = np.zeros_like(prediction_RZSM_GEFS)\n",
    "            prediction_RZSM_ECMWF[:,:,:,:] = np.nan\n",
    "        print(f'Shape of prediction RZSM: {prediction_RZSM_GEFS.shape}')\n",
    "    \n",
    "        #Convert back to np.nan values for the ocean and other water bodies\n",
    "        prediction_RZSM_GEFS = np.where(np.isnan(obs_RZSM),np.nan,prediction_RZSM_GEFS.squeeze())\n",
    "        if ecmwf_present:\n",
    "            prediction_RZSM_ECMWF = np.where(np.isnan(obs_RZSM),np.nan,prediction_RZSM_ECMWF.squeeze())\n",
    "    \n",
    "        return(prediction_RZSM_GEFS, prediction_RZSM_ECMWF, obs_RZSM)\n",
    "    else:\n",
    "        return(np.zeros(shape=obs_RZSM.shape),np.zeros(shape=obs_RZSM.shape), obs_RZSM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4033f-958d-4bc3-b0e7-29f9e98e0a3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def setup_binary_for_hit_rate_with_ensemble_mean(week_lead, region_name, test_start, test_end, unet_file):\n",
    "\n",
    "    #Test \n",
    "    # week_lead=1\n",
    "    # percentile_eval = 20\n",
    "\n",
    "    #Save dir\n",
    "    save_dir = f'Data/correct_anomaly_percentile_statistics/{region_name}'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "\n",
    "    save_ecmwf = f'{save_dir}/Wk{week_lead}_ecmwf_stats_TP_FP_ensemble_mean.npy'\n",
    "    save_gefs = f'{save_dir}/Wk{week_lead}_gefs_stats_TP_FP_ensemble_mean.npy'\n",
    "    save_xg = f'{save_dir}/Wk{week_lead}_xgboost_stats_TP_FP_ensemble_mean.npy'\n",
    "    save_obs_binary = f'{save_dir}/Wk{week_lead}_obs_stats_TP_FP_ensemble_mean.npy'\n",
    "\n",
    "    day_num = (week_lead*7) -1\n",
    "        \n",
    "    print('Loading observation and baseline anomaly files')\n",
    "    obs, gefs, ecmwf, var_OUT_overwrite, template_testing_only_by_lead= select_data_by_lead(obs_anomaly_SubX_format, baseline_anomaly, baseline_ecmwf, var_OUT, template_testing_only, day_num)\n",
    "    \n",
    "    obs_percent = obs_anom_percentile.sel(L=day_num).sel(M=0)\n",
    "    obs_percent['95th_percentile'].shape #(104, 48, 96)\n",
    "\n",
    "\n",
    "    file = baseline_anomaly\n",
    "    file.RZSM.shape\n",
    "    out_check_gefs_base = np.zeros(shape=(104,11,48,96,8)) #Adding 8 channels for the different anomaly spreads\n",
    "    \n",
    "    out_check_gefs_base[:,:,:,:] = np.nan\n",
    "    out_check_ecmwf_base = out_check_gefs_base.copy()\n",
    "    out_check_unet = out_check_gefs_base.copy()\n",
    "    \n",
    "    \n",
    "    obs_binary_out =np.zeros(shape=(104,48,96,8)) #Adding 8 channels for the different anomaly spreads\n",
    "\n",
    "    final_perc_gefs = np.zeros(shape=(104,48,96,8)) #Adding 8 channels for the different anomaly spreads\n",
    "    final_perc_gefs[:,:,:] = np.nan\n",
    "    final_perc_ecmwf = final_perc_gefs.copy()\n",
    "    final_perc_unet = final_perc_gefs.copy()\n",
    "    \n",
    "    \n",
    "\n",
    "    test_name = unet_file.split('testing_')[-1].split('.npy')[0].split('ensemble_')[-1]\n",
    "    save_unet = f'{save_dir}/Wk{week_lead}_unet_stats_{test_name}_TP_FP_ensemble_mean.npy'\n",
    "    \n",
    "    test =  verifications.reverse_min_max_scaling(np.load(unet_file), region_name, day_num)[2,:,:,:,0] #We only want the last channel\n",
    "    test = np.reshape(test,(test.shape[0]//11,11,test.shape[1],test.shape[2]))\n",
    "    test.shape\n",
    "\n",
    "    #Now mask the input\n",
    "    test_unet = np.where(mask_anom == 1,test,np.nan)\n",
    "\n",
    "    #XGBoost\n",
    "    #Load the XGBoost data\n",
    "    if region_name == 'CONUS':\n",
    "        out_check_xg = out_check_gefs_base.copy()\n",
    "        final_perc_xg = final_perc_gefs.copy()\n",
    "        \n",
    "        xgboost_files = sorted(glob(f'predictions_XGBOOST/{region_name}/Wk{week_lead}_testing/*EX28*'))[0]\n",
    "    \n",
    "        # break\n",
    "        #Still working here\n",
    "        test_name = xgboost_files.split('testing_')[-1].split('.npy')[0]\n",
    "        load_ = np.expand_dims(np.load(xgboost_files),-1)\n",
    "        load_.shape\n",
    "        load_ = np.where(load_ == 0,np.nan,load_)\n",
    "        load_ =  verifications.reverse_min_max_scaling(load_, region_name, day_num)#We only want the last channel\n",
    "        \n",
    "        xg = np.empty(shape=(load_.shape[0],11,load_.shape[1],load_.shape[2],load_.shape[3])) #This will help with climpred functions\n",
    "        for j in range(11):\n",
    "            xg[:,j,:,:,:] = load_\n",
    "    \n",
    "        xg = xg.squeeze()\n",
    "        xg.shape\n",
    "        x_vals = np.nanmean(xg,axis=1)\n",
    "    else:\n",
    "        x_vals = out_check_gefs_base.copy()\n",
    "    #Check if the predicted anomaly is below each threshold\n",
    "\n",
    "\n",
    "    \n",
    "    #Test\n",
    "    # idx = 0\n",
    "    # mx = 0\n",
    "    # ix = 10\n",
    "    # iy =10 #NEGATIVE ANOMALY VALUE\n",
    "    # iy =5 #POSITIVE ANOMALY VALUE\n",
    "\n",
    "    #Use np.where to find the values of the percentile\n",
    "\n",
    "    #Take ensemble mean\n",
    "    o_vals = np.nanmean(obs.RZSM[:,:,0,:,:].values,axis=1)\n",
    "    g_vals =  np.nanmean(gefs.RZSM[:,:,0,:,:].values,axis=1)\n",
    "    e_vals =  np.nanmean(ecmwf.RZSM[:,:,0,:,:].values,axis=1)\n",
    "    u_vals = np.nanmean(test_unet,axis=1)\n",
    "    \n",
    "\n",
    "    #Keep all models the same\n",
    "    # o_vals = obs.RZSM[:,:,0,:,:].values\n",
    "    # g_vals =  gefs.RZSM[:,:,0,:,:].values\n",
    "    # e_vals =  ecmwf.RZSM[:,:,0,:,:].values\n",
    "    # u_vals = test_unet\n",
    "    # x_vals = xg\n",
    "\n",
    "    u_vals.shape\n",
    "    x_vals.shape\n",
    "\n",
    "    def check_if_below_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_vals, x_vals):\n",
    "        perc_= obs_percent[f'{percentile_num}th_percentile'].values\n",
    "        \n",
    "        def find_percentage(perc_,o_vals,fcst):\n",
    "            fcst =  np.where(fcst<perc_,1,0)\n",
    "            obs_binary = np.where((o_vals<perc_),1,0)\n",
    "            '''Take the mean to find the probability of correct'''\n",
    "            fcst.shape #(104, 48, 96)\n",
    "            #Now mask the input of CONUS/region\n",
    "            fcst = np.where(mask_anom == 1,fcst,np.nan)\n",
    "            obs_binary = np.where(mask_anom == 1,obs_binary,np.nan)\n",
    "\n",
    "            return(fcst,obs_binary)\n",
    "\n",
    "        g_perc,obs_binary = find_percentage(perc_,o_vals,g_vals)\n",
    "        g_perc.shape\n",
    "        e_perc,obs_binary = find_percentage(perc_,o_vals,e_vals)\n",
    "        u_perc,obs_binary  = find_percentage(perc_,o_vals,u_vals)\n",
    "        x_perc,obs_binary  = find_percentage(perc_,o_vals,x_vals)\n",
    "        return(g_perc,e_perc,u_perc,x_perc,obs_binary )\n",
    "\n",
    "    for idx,percentile_num in enumerate([5,10,20,33]):\n",
    "        final_perc_gefs[:,:,:,idx], final_perc_ecmwf[:,:,:,idx], final_perc_unet[:,:,:,idx],final_perc_xg[:,:,:,idx],obs_binary_out[:,:,:,idx] = check_if_below_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_vals, x_vals)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def check_if_above_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_vals, x_vals):\n",
    "        perc_= obs_percent[f'{percentile_num}th_percentile'].values\n",
    "       \n",
    "        def find_percentage(perc_,o_vals,fcst):\n",
    "            fcst =  np.where(fcst>perc_,1,0)\n",
    "            obs_binary = np.where((o_vals>perc_),1,0)\n",
    "            '''Take the mean to find the probability of correct'''\n",
    "            fcst.shape #(104, 48, 96)\n",
    "            #Now mask the input of CONUS/region\n",
    "            fcst = np.where(mask_anom == 1,fcst,np.nan)\n",
    "            obs_binary = np.where(mask_anom == 1,obs_binary,np.nan)\n",
    "\n",
    "            return(fcst,obs_binary)\n",
    "\n",
    "        g_perc,obs_binary  = find_percentage(perc_,o_vals,g_vals)\n",
    "        g_perc.shape\n",
    "        e_perc,obs_binary  = find_percentage(perc_,o_vals,e_vals)\n",
    "        u_perc,obs_binary  = find_percentage(perc_,o_vals,u_vals)\n",
    "        x_perc,obs_binary  = find_percentage(perc_,o_vals,x_vals)\n",
    "        \n",
    "        return(g_perc,e_perc,u_perc,x_perc,obs_binary )\n",
    "    \n",
    "    for idx,percentile_num in enumerate([66,80,90,95]):\n",
    "        '''We are adding 4 to make sure that we get the indices correct, we already added data from below percentiles'''\n",
    "        final_perc_gefs[:,:,:,idx+4], final_perc_ecmwf[:,:,:,idx+4], final_perc_unet[:,:,:,idx+4], final_perc_xg[:,:,:,idx+4],obs_binary_out[:,:,:,idx+4] = check_if_above_percentile(obs_percent, percentile_num, o_vals, g_vals, e_vals, u_vals, x_vals)\n",
    "\n",
    "    \n",
    "    #Save files for later use\n",
    "    np.save(save_ecmwf,final_perc_ecmwf)\n",
    "    np.save(save_gefs, final_perc_gefs)\n",
    "    np.save(save_unet, final_perc_unet)\n",
    "    np.save(save_xg, final_perc_xg)\n",
    "    np.save(save_obs_binary, obs_binary_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9adef1-6c22-4222-aee4-b9e7e99ba4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_non_post_processed_forecasts(lead,dim_order):\n",
    "    '''We are selecting a single lead time, so use this code'''\n",
    "    if lead == 0:\n",
    "        index_sel = 0\n",
    "    else:\n",
    "        index_sel = (lead*7)-1\n",
    "    \n",
    "\n",
    "    if region_name == 'CONUS':\n",
    "        RZSM_base_reforecast_climpred_GEF = f.restrict_to_CONUS_bounding_box(gefs_anom,mask).sel(L=(lead*7)-1).expand_dims({'L': 1}).transpose(*dim_order)\n",
    "        RZSM_base_reforecast_climpred_ECM = f.restrict_to_CONUS_bounding_box(ecmwf_anom,mask).sel(L=(lead*7)-1).expand_dims({'L': 1}).transpose(*dim_order)\n",
    "    else:\n",
    "        RZSM_base_reforecast_climpred_GEF = gefs_anom.sel(L=(lead*7)-1).expand_dims({'L': 1}).transpose(*dim_order)\n",
    "        RZSM_base_reforecast_climpred_ECM = ecmwf_anom.sel(L=(lead*7)-1).expand_dims({'L': 1}).transpose(*dim_order)\n",
    "        \n",
    "    print_min_max(RZSM_base_reforecast_climpred_GEF,'GEFS RZSM baseline value from reforecast (training, validation, testing) (no pre-processing other than anomaly computed.)')\n",
    "    print_min_max(RZSM_base_reforecast_climpred_ECM,'ECMWF RZSM baseline value from reforecast (training, validation, testing) (no pre-processing other than anomaly computed.)')\n",
    "\n",
    "    if RZSM_or_Tmax_or_both == 'both':\n",
    "        return(RZSM_base_reforecast_climpred_GEF, RZSM_base_reforecast_climpred_ECM,tmax_base_reforecast_climpred)\n",
    "    else:\n",
    "        return(RZSM_base_reforecast_climpred_GEF, RZSM_base_reforecast_climpred_ECM)\n",
    "\n",
    "    '''Then load the actual UNET predictions and keep the observations that are already in a good format for comparison\n",
    "    This data has been loaded, converted back to anomalies, and masked for RZSM for non-land regions'''\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf5a49-59b2-4f3b-9157-747aac9e5f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prediction_to_SubX_format(file,lead,dim_order):\n",
    "\n",
    "    if region_name == 'CONUS':\n",
    "        cp_base = f.restrict_to_CONUS_bounding_box(base_file_testing.copy(deep=True).sel(L=(lead*7)-1),mask).expand_dims({'L': 1})\n",
    "    else:\n",
    "        cp_base = base_file_testing.copy(deep=True).sel(L=(lead*7)-1).expand_dims({'L': 1})\n",
    "\n",
    "    \n",
    "    #reshape data back to original format (testing shape)\n",
    "    \n",
    "    cp_base = cp_base.transpose(*dim_order)\n",
    "    \n",
    "    #Reshape prediction file\n",
    "    file = file.reshape((104,11,1,48,96))\n",
    "    \n",
    "    var_OUT = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                file_name = (['S','M','L','Y','X'],  file[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "                S = cp_base.S.values,\n",
    "                X = cp_base.X.values,\n",
    "                Y = cp_base.Y.values,\n",
    "                L = cp_base.L.values,\n",
    "                M = cp_base.M.values,\n",
    "\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'New data added to file'),\n",
    "        )  \n",
    "\n",
    "    return(var_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739bf45-b227-47ea-84a8-0bfd172673b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crpss_(lead,experiment,ACC_dictionary,obs_anom_climp,gef_bc_acc,ecm_bc_acc,obs_source):\n",
    "    output_dictionary = {}\n",
    "    out_name = ''\n",
    "    \n",
    "    #Must re-add back L as a date\n",
    "    dim_order = ['S','M','L','Y','X']\n",
    "\n",
    "    RZSM_base_reforecast_climpred_GEF, RZSM_base_reforecast_climpred_ECM = return_non_post_processed_forecasts(lead,dim_order) #Returns the original reforecasts\n",
    "    \n",
    "    prediction_RZSM_GEFS, prediction_RZSM_ECMWF, obs_RZSM = load_experiment_predictions_and_observations(lead,experiment,region_name,obs_source) #Returns the UNET prediction and observations\n",
    "\n",
    "\n",
    "    #Change name for climpred processing\n",
    "    #Reforecast prediction\n",
    "    prediction_RZSM_climpred_GEF = verifications.rename_subx_for_climpred(convert_prediction_to_SubX_format(file=prediction_RZSM_GEFS,lead=lead,dim_order = dim_order))\n",
    "    prediction_RZSM_climpred_ECM = verifications.rename_subx_for_climpred(convert_prediction_to_SubX_format(file=prediction_RZSM_ECMWF,lead=lead,dim_order = dim_order))\n",
    "\n",
    "    prediction_RZSM_climpred_GEF  = prediction_RZSM_climpred_GEF.rename({'file_name':'RZSM'})\n",
    "    prediction_RZSM_climpred_ECM  = prediction_RZSM_climpred_ECM.rename({'file_name':'RZSM'})\n",
    "    \n",
    "    print_min_max(prediction_RZSM_climpred_ECM,'ECMWF RZSM anomaly prediction value from UNET')\n",
    "    print_min_max(prediction_RZSM_climpred_GEF,'GEFSv12 RZSM anomaly prediction value from UNET')\n",
    "    \n",
    "    unet_acc_gef = verifications.create_climpred_CRPSS_no_chunk(prediction_RZSM_climpred_GEF, obs_anom_climp)\n",
    "    unet_acc_ecm = verifications.create_climpred_CRPSS_no_chunk(prediction_RZSM_climpred_ECM, obs_anom_climp)\n",
    "    ACC_dictionary[f'Wk{lead}_{experiment}_MEM_RZSM_CRPSS'] = np.nanmedian(unet_acc_gef[putils.xarray_varname(unet_acc_gef)].values)\n",
    "    ACC_dictionary[f'Wk{lead}_{experiment}_ECMWF_MEM_RZSM_CRPSS'] = np.nanmedian(unet_acc_ecm[putils.xarray_varname(unet_acc_ecm)].values)\n",
    "\n",
    "    #Base reforecast (before post-processing)\n",
    "    base_RZSM_climpred_GEFS  = verifications.rename_subx_for_climpred(RZSM_base_reforecast_climpred_GEF).sel(init=slice(start_testing, None))\n",
    "    print_min_max(base_RZSM_climpred_GEFS,'GEFS RZSM baseline anomaly from reforecast. No post-processing. \\n')\n",
    "\n",
    "    base_RZSM_climpred_ECMWF  = verifications.rename_subx_for_climpred(RZSM_base_reforecast_climpred_ECM).sel(init=slice(start_testing, None))\n",
    "    print_min_max(base_RZSM_climpred_ECMWF,'ECMWF RZSM baseline anomaly from reforecast. No post-processing. \\n')\n",
    "    \n",
    "    gefs_acc = verifications.create_climpred_CRPSS_no_chunk(base_RZSM_climpred_GEFS, obs_anom_climp)\n",
    "    ecm_acc = verifications.create_climpred_CRPSS_no_chunk(base_RZSM_climpred_ECMWF, obs_anom_climp)\n",
    "    \n",
    "\n",
    "    ACC_dictionary[f'Wk{lead}_GEFS_MEM_baseline_RZSM_CRPSS'] = np.nanmedian(gefs_acc[putils.xarray_varname(gefs_acc)].values)\n",
    "    ACC_dictionary[f'Wk{lead}_ECMWF_MEM_baseline_RZSM_CRPSS'] = np.nanmedian(ecm_acc[putils.xarray_varname(ecm_acc)].values)\n",
    "    ACC_dictionary[f'Wk{lead}_GEFS_MEM_BC_baseline_RZSM_CRPSS'] = np.nanmedian(gef_bc_acc[putils.xarray_varname(gef_bc_acc)].values[lead-1,:,:])\n",
    "    ACC_dictionary[f'Wk{lead}_ECMWF_MEM_BC_baseline_RZSM_CRPSS'] = np.nanmedian(ecm_bc_acc[putils.xarray_varname(ecm_bc_acc)].values[lead-1,:,:])\n",
    "    \n",
    "\n",
    "    return(ACC_dictionary)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d12bc30-168a-44b4-90d7-9d9fd0f29889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crpss_by_season(lead, experiment, ACC_dictionary, obs_anom_climp, gef_bc_acc, ecm_bc_acc, obs_source):\n",
    "    output_dictionary = {}\n",
    "    out_name = ''\n",
    "    \n",
    "    # Must re-add back L as a date\n",
    "    dim_order = ['S','M','L','Y','X']\n",
    "\n",
    "    # Get the original reforecasts\n",
    "    RZSM_base_reforecast_climpred_GEF, RZSM_base_reforecast_climpred_ECM = return_non_post_processed_forecasts(lead, dim_order)\n",
    "    \n",
    "    # Get the UNET predictions and observations\n",
    "    prediction_RZSM_GEFS, prediction_RZSM_ECMWF, obs_RZSM = load_experiment_predictions_and_observations(lead, experiment, region_name, obs_source)\n",
    "\n",
    "    # Convert predictions to climpred format\n",
    "    prediction_RZSM_climpred_GEF = verifications.rename_subx_for_climpred(\n",
    "        convert_prediction_to_SubX_format(file=prediction_RZSM_GEFS, lead=lead, dim_order=dim_order)\n",
    "    ).rename({'file_name':'RZSM'})\n",
    "\n",
    "    prediction_RZSM_climpred_ECM = verifications.rename_subx_for_climpred(\n",
    "        convert_prediction_to_SubX_format(file=prediction_RZSM_ECMWF, lead=lead, dim_order=dim_order)\n",
    "    ).rename({'file_name':'RZSM'})\n",
    "    \n",
    "    # Print stats\n",
    "    print_min_max(prediction_RZSM_climpred_ECM, 'ECMWF RZSM anomaly prediction value from UNET')\n",
    "    print_min_max(prediction_RZSM_climpred_GEF, 'GEFSv12 RZSM anomaly prediction value from UNET')\n",
    "\n",
    "    def add_single_month(months, season):\n",
    "        vals = months[season]\n",
    "        #This is to ensure that we have all of the data correctly forecasted within the distribution\n",
    "        next_number = vals[-1] + 1  # Get the last number and add 1\n",
    "        vals.append(next_number)\n",
    "        return vals\n",
    "        \n",
    "    # Function to filter data by season\n",
    "    def filter_by_season(data, season, obs_fcst):\n",
    "        months = {\n",
    "            'DJF': [12, 1, 2],\n",
    "            'MAM': [3, 4, 5],\n",
    "            'JJA': [6, 7, 8],\n",
    "            'SON': [9, 10, 11]\n",
    "        }\n",
    "        if obs_fcst=='forecast':\n",
    "            return data.sel(init=data['init'].dt.month.isin(months[season]))\n",
    "        else:\n",
    "            return data.sel(time=data['time'].dt.month.isin(months[season]))\n",
    "\n",
    "    \n",
    "\n",
    "    seasons = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "\n",
    "    for season in seasons:\n",
    "        # Filter by season\n",
    "        pred_season_GEF = filter_by_season(prediction_RZSM_climpred_GEF, season, 'forecast')\n",
    "        pred_season_ECM = filter_by_season(prediction_RZSM_climpred_ECM, season, 'forecast')\n",
    "        obs_season = filter_by_season(obs_anom_climp, season, 'obs')\n",
    "        \n",
    "        # Compute ACC for each season\n",
    "        unet_acc_gef = verifications.create_climpred_CRPSS_no_chunk(pred_season_GEF, obs_season)\n",
    "        unet_acc_ecm = verifications.create_climpred_CRPSS_no_chunk(pred_season_ECM, obs_season)\n",
    "        \n",
    "        ACC_dictionary[f'{season}_Wk{lead}_{experiment}_MEM_RZSM_CRPSS'] = np.nanmedian(unet_acc_gef[putils.xarray_varname(unet_acc_gef)].values)\n",
    "        ACC_dictionary[f'{season}_Wk{lead}_{experiment}_ECMWF_MEM_RZSM_CRPSS'] = np.nanmedian(unet_acc_ecm[putils.xarray_varname(unet_acc_ecm)].values)\n",
    "        \n",
    "        # Baseline reforecasts\n",
    "        base_RZSM_climpred_GEFS = verifications.rename_subx_for_climpred(RZSM_base_reforecast_climpred_GEF).sel(init=slice(start_testing, None))\n",
    "        base_RZSM_climpred_ECMWF = verifications.rename_subx_for_climpred(RZSM_base_reforecast_climpred_ECM).sel(init=slice(start_testing, None))\n",
    "        \n",
    "        # Filter baseline forecasts by season\n",
    "        base_season_GEF = filter_by_season(base_RZSM_climpred_GEFS, season, 'forecast')\n",
    "        base_season_ECM = filter_by_season(base_RZSM_climpred_ECMWF, season, 'forecast')\n",
    "        \n",
    "        gefs_acc = verifications.create_climpred_CRPSS_no_chunk(base_season_GEF, obs_season)\n",
    "        ecm_acc = verifications.create_climpred_CRPSS_no_chunk(base_season_ECM, obs_season)\n",
    "        \n",
    "        # Store baseline ACC values for each season\n",
    "        ACC_dictionary[f'{season}_Wk{lead}_GEFS_MEM_baseline_RZSM_CRPSS'] = np.nanmedian(gefs_acc[putils.xarray_varname(gefs_acc)].values)\n",
    "        ACC_dictionary[f'{season}_Wk{lead}_ECMWF_MEM_baseline_RZSM_CRPSS'] = np.nanmedian(ecm_acc[putils.xarray_varname(ecm_acc)].values)\n",
    "        \n",
    "        # Bias-corrected ACC values for each season\n",
    "        ACC_dictionary[f'{season}_Wk{lead}_GEFS_MEM_BC_baseline_RZSM_CRPSS'] = np.nanmedian(gef_bc_acc[putils.xarray_varname(gef_bc_acc)].values[lead-1,:,:])\n",
    "        ACC_dictionary[f'{season}_Wk{lead}_ECMWF_MEM_BC_baseline_RZSM_CRPSS'] = np.nanmedian(ecm_bc_acc[putils.xarray_varname(ecm_bc_acc)].values[lead-1,:,:])\n",
    "\n",
    "    return ACC_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecc019d-e460-4016-8bd0-75b040172514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CRPSS(lead,region_name,obs_source):\n",
    "    print(f'Working on lead {lead}')\n",
    "    # lead=1\n",
    "\n",
    "    # save_dict_dir = f'Outputs/crps_mae/{region_name}/Wk_{lead}'\n",
    "    # os.system(f'mkdir -p {save_dict_dir}')\n",
    "    \n",
    "    if lead <=4:\n",
    "        if region_name == 'CONUS':\n",
    "            if obs_source == 'GLEAM':\n",
    "                \n",
    "                experiment_list = [f'EX{i}' for i in range(0,30)]\n",
    "                experiment_list.remove('EX26')\n",
    "                if lead <=2:\n",
    "                    experiment_list.remove('EX18')\n",
    "                    experiment_list.remove('EX19')\n",
    "                    experiment_list.remove('EX20')\n",
    "                    experiment_list.remove('EX21')\n",
    "            elif obs_source == 'ERA5':\n",
    "                experiment_list = ['EX29']\n",
    "        else:\n",
    "            experiment_list = ['EX29']\n",
    "    elif lead ==5:\n",
    "        experiment_list = ['EX26']\n",
    "\n",
    "    \n",
    "    ACC_dictionary = {}\n",
    "    ACC_season = {}\n",
    "    for experiment in experiment_list:\n",
    "        ACC_dictionary.update(crpss_(lead=lead,experiment=experiment,\n",
    "                                                              ACC_dictionary=ACC_dictionary, \n",
    "                                                              obs_anom_climp=obs_anom_climp,\n",
    "                                                              gef_bc_acc=gef_bc_crpss, ecm_bc_acc=ecm_bc_crpss,\n",
    "                                                     obs_source=obs_source))\n",
    "        ACC_season.update(crpss_by_season(lead=lead,experiment=experiment,\n",
    "                                                      ACC_dictionary=ACC_season, \n",
    "                                                      obs_anom_climp=obs_anom_climp,\n",
    "                                                      gef_bc_acc=gef_bc_crpss, ecm_bc_acc=ecm_bc_crpss,\n",
    "                                                     obs_source=obs_source))\n",
    "\n",
    "        \n",
    "    return(ACC_dictionary,ACC_season)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca117d1-ce0c-43cc-89a7-7d53c514738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ACC_dictionary.keys())\n",
    "\n",
    "def save_CRPSS_tests(var, ACC_dictionary, region_name,obs_source,season):\n",
    "\n",
    "    t1 = grab_ACC_from_dict(dict_ = ACC_dictionary, var = var)\n",
    "    # print(acc.keys()) \n",
    "\n",
    "    ############################### SINGLE PREDICTION, NO BIAS CORRECTION ##################################################################\n",
    "    # acc = grab_ACC_from_dict(dict_ = ACC_dictionary, var = var)\n",
    "    # t1 = subset_delete(dict_ = acc, subset = 'bias_corrected')\n",
    "    # print(t1.keys())\n",
    "\n",
    "    #Save the average ACC values to a dictionary for later plotting\n",
    "    # t_base = subset_keep(dict_ = t1, subset = 'baseline')\n",
    "    # t_unet= subset_delete(dict_ = t1, subset = 'baseline')\n",
    "    # print(t_base.keys())\n",
    "    # print(t_unet.keys())\n",
    "    \n",
    "    file_path = f'Outputs/permutation_tests/{region_name}/Wk_{lead}'\n",
    "    if season:\n",
    "        file_save = f'{file_path}/CRPSS_vals_{obs_source}.pkl'\n",
    "    else:\n",
    "        file_save = f'{file_path}/CRPSS_vals_{obs_source}_season.pkl'\n",
    "    \n",
    "    os.system(f'mkdir -p {file_path}')\n",
    "    \n",
    "    with open(file_save, 'wb') as file:\n",
    "        pickle.dump(t1, file)\n",
    "\n",
    "    # plot_files_ACC(test_file = t1, var = var, name_of_test = f'{var} Single prediction ACC - No bias correction')\n",
    "\n",
    " \n",
    "    return(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04232d7-ea4f-4380-9d3b-6a6492f70cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_ACC_from_dict(dict_,var):\n",
    "    crpss = {key: value for key, value in dict_.items() if f'{var}_CRPSS' in key}\n",
    "    return(crpss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd061e-5ef2-4d32-9e01-2ddb19daad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lead in [1,2,3,4]:\n",
    "    CRPSS_dictionary,CRPSS_season = run_CRPSS(lead,region_name,obs_source)\n",
    "    print(CRPSS_dictionary)\n",
    "    '''As a note, week 1 doesn't ever have any experimental runs for EX18-EX21, also EX26 is not a model that I ran'''\n",
    "    save_CRPSS_tests(var = 'RZSM', ACC_dictionary=CRPSS_dictionary, region_name = region_name,obs_source=obs_source, season=False)\n",
    "    save_CRPSS_tests(var = 'RZSM', ACC_dictionary=CRPSS_season, region_name = region_name,obs_source=obs_source, season=True)\n",
    "\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f816d-6b3c-48b2-950e-34232f894440",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c64f3b-82d6-49ed-9a07-fd76046bbfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "# many_testing_predictions = True\n",
    "# experiment = 'EX0'\n",
    "# lead=1\n",
    "# bias_correction = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ea3e1-b12b-4a6b-9633-b526bad79742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf0d63-3835-4001-9e97-966d7b717694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_lead(file,dim_order,lead):\n",
    "    return(file.sel(L=(lead*7)-1).expand_dims({'L': 1}).transpose(*dim_order))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b6d592-5d00-4164-a29d-9732336ee864",
   "metadata": {},
   "source": [
    "# Plot data\n",
    "## Have not completed this yet with new ERA5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6c342-4a4a-4be3-992f-9916275c8915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_min_max_of_files(file):\n",
    "    min_ =  []\n",
    "    max_ = []\n",
    "    try:\n",
    "        for f in list(file.keys()):\n",
    "            min_.append(np.nanmin(file[f]))\n",
    "            max_.append(np.nanmax(file[f]))\n",
    "    except TypeError:\n",
    "         for f in list(file.keys()):\n",
    "            min_.append(np.nanmin(file[f].crps))\n",
    "            max_.append(np.nanmax(file[f].crps))\n",
    "            \n",
    "    return(min(min_),max(max_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222496b8-48b2-4b11-9739-fa46f31403af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "   \n",
    "# cmap = 'coolwarm'\n",
    "\n",
    "def plot_files_ACC(test_file, var, name_of_test):\n",
    "    cmap = plt.get_cmap('bwr')    \n",
    "    \n",
    "    save_dir = f'Outputs/crps_mae/Wk_{lead}/{var}_ACC'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    if lead == 0:\n",
    "        row=3\n",
    "        column=5\n",
    "        width = 20\n",
    "        height=10\n",
    "        ex_size = 12\n",
    "    else:\n",
    "        row=6\n",
    "        column=5\n",
    "        width = 30\n",
    "        height=25\n",
    "        ex_size = 16\n",
    "        \n",
    "    fig, axs = plt.subplots(\n",
    "        row, column, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(width, height))\n",
    "    axs = axs.flatten()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    min_,max_ = get_min_max_of_files(test_file)\n",
    "    # test_file = mae_rzsm_keys\n",
    "    # for Subx original data\n",
    "    \n",
    "    #For the ACC values\n",
    "    if lead == 0:\n",
    "        text_x = -84  # You may need to adjust this value based on your data\n",
    "        text_y = 27  # You may need to adjust this value based on your data\n",
    "        font_size = 12\n",
    "    else:\n",
    "        text_x = -84  # You may need to adjust this value based on your data\n",
    "        text_y = 27  # You may need to adjust this value based on your data\n",
    "        font_size = 16\n",
    "    \n",
    "    for idx,experiment in enumerate(experiment_list):\n",
    "        data = {key: value for key, value in test_file.items() if experiment in key}\n",
    "        data = list(data.values())[0]\n",
    "        \n",
    "        \n",
    "        mean_vals = round(np.nanmean(data),4)\n",
    "        print(f'Mean value: {mean_vals}')\n",
    "        \n",
    "        v = np.linspace(min_, max_, 30, endpoint=True)\n",
    "\n",
    "        map = Basemap(projection='cyl', llcrnrlat=25, urcrnrlat=50,\n",
    "                      llcrnrlon=-128, urcrnrlon=-60, resolution='l')\n",
    "        x, y = map(*np.meshgrid(lon, lat))\n",
    "        # Adjust the text coordinates based on the actual data coordinates\n",
    "\n",
    "\n",
    "        \n",
    "        # ax.drawmeridians()\n",
    "        try:\n",
    "            norm = TwoSlopeNorm(vmin=min_, vcenter=0, vmax=max_)\n",
    "        except ValueError:\n",
    "            norm = None\n",
    "        \n",
    "        try:\n",
    "            im = axs[idx].contourf(x, y, data, levels=v, extend='both',\n",
    "                                  transform=ccrs.PlateCarree(), cmap=cmap,norm=norm)\n",
    "        except TypeError:\n",
    "            data = np.nanmean(data.crps.values,axis=0)\n",
    "            im = axs[idx].contourf(x, y, data, levels=v, extend='both',\n",
    "                      transform=ccrs.PlateCarree(), cmap=cmap,norm=norm)\n",
    "            \n",
    "    \n",
    "        # axs[idx].title.set_text(f'SubX Lead {lead*7}')\n",
    "        gl = axs[idx].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                   linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "        gl.xlabels_top = False\n",
    "        gl.ylabels_right = False\n",
    "        if lead != 1:\n",
    "            gl.ylabels_left = False\n",
    "        gl.xformatter = LongitudeFormatter()\n",
    "        gl.yformatter = LatitudeFormatter()\n",
    "        axs[idx].coastlines()\n",
    "        # plt.colorbar(im)\n",
    "        # axs[idx].set_aspect('auto', adjustable=None)\n",
    "        axs[idx].set_aspect('equal')  # this makes the plots better\n",
    "        axs[idx].set_title(experiment,fontsize=ex_size)\n",
    "        axs[idx].text(text_x, text_y, mean_vals, ha='right', va='bottom', fontsize=font_size, color='blue', weight = 'bold')\n",
    "\n",
    "        # Add a colorbar axis at the bottom of the graph\n",
    "        # left, bottom, width, height\n",
    "    \n",
    "    data_non_EX = {key: value for key, value in test_file.items() if 'EX' not in key}\n",
    "    #Don't worry about additive bias right now. I can't figure out why it doesn't work\n",
    "    data_non_EX = {key: value for key, value in data_non_EX.items() if 'no_BC' in key}\n",
    "    \n",
    "    for non_used,data_key in enumerate(data_non_EX):\n",
    "        idx+=1\n",
    "        # break\n",
    "        data = {key: value for key, value in test_file.items() if data_key in key}\n",
    "        data = list(data.values())[0]\n",
    "        mean_vals = round(np.nanmean(data),4)\n",
    "        \n",
    "        v = np.linspace(min_, max_, 30, endpoint=True)\n",
    "\n",
    "        map = Basemap(projection='cyl', llcrnrlat=25, urcrnrlat=50,\n",
    "                      llcrnrlon=-128, urcrnrlon=-60, resolution='l')\n",
    "        x, y = map(*np.meshgrid(lon, lat))\n",
    "\n",
    "\n",
    "        # ax.drawmeridians()\n",
    "        try:\n",
    "            norm = TwoSlopeNorm(vmin=min_, vcenter=0, vmax=max_)\n",
    "        except ValueError:\n",
    "            norm = None\n",
    "        \n",
    "        try:\n",
    "            im = axs[idx].contourf(x, y, data, levels=v, extend='both',\n",
    "                                  transform=ccrs.PlateCarree(), cmap=cmap,norm=norm)\n",
    "        except TypeError:\n",
    "            data = np.nanmean(data.crps.values,axis=0)\n",
    "            im = axs[idx].contourf(x, y, data, levels=v, extend='both',\n",
    "                      transform=ccrs.PlateCarree(), cmap=cmap,norm=norm)\n",
    "        # axs[idx].title.set_text(f'SubX Lead {lead*7}')\n",
    "        gl = axs[idx].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                   linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "        gl.xlabels_top = False\n",
    "        gl.ylabels_right = False\n",
    "        if lead != 1:\n",
    "            gl.ylabels_left = False\n",
    "        gl.xformatter = LongitudeFormatter()\n",
    "        gl.yformatter = LatitudeFormatter()\n",
    "        axs[idx].coastlines()\n",
    "        # plt.colorbar(im)\n",
    "        # axs[idx].set_aspect('auto', adjustable=None)\n",
    "        axs[idx].set_aspect('equal')  # this makes the plots better\n",
    "        axs[idx].set_title(data_key)\n",
    "        axs[idx].text(text_x, text_y, mean_vals, ha='right', va='bottom', fontsize=font_size, color='blue', weight = 'bold')\n",
    "        \n",
    "        # Add a colorbar axis at the bottom of the graph\n",
    "        # left, bottom, width, height\n",
    "    cbar_ax = fig.add_axes([0.05, -0.05, .9, .04])\n",
    "\n",
    "\n",
    "    \n",
    "    # Draw the colorbar\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    plt.tight_layout()\n",
    "    fig.suptitle(name_of_test, fontsize=30)\n",
    "    plt.savefig(f'{save_dir}/{name_of_test}.png',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c5b561-37c3-4948-94f7-0008e8034b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_rank_histogram(baseline_file,prediction_file, name_of_file):\n",
    "    # prediction_file = rzsm_Rank_histogram_predictions\n",
    "    # baseline_file = rzsm_Rank_histogram_baseline\n",
    "    \n",
    "    # baseline_file = baseline_file.values()\n",
    "    baseline_file = list(baseline_file.values())[0]\n",
    "    \n",
    "    fig, axs = plt.subplots(3,5, figsize=(20, 7))\n",
    "\n",
    "#     fig, axs = plt.subplots(\n",
    "#         3, 5, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(20, 7), gridspec_kw={'height_ratios': [2,2,2]})\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "\n",
    "    # rank_file=rank_histogram_unet\n",
    "    to_df = baseline_file.rank_histogram[:].to_dataframe()\n",
    "    to_df['rank_histogram'] = to_df['rank_histogram'] / \\\n",
    "        to_df['rank_histogram'].sum()\n",
    "    to_df['rank'] = to_df.index\n",
    "    to_df['rank'] = to_df['rank'].astype(int)\n",
    "    to_df.index = to_df['rank']\n",
    "    del to_df['lead']\n",
    "    del to_df['skill']\n",
    "    del to_df['rank']\n",
    "    \n",
    "    print(f'Shape of to_df : {to_df.rank().shape[0]}')\n",
    "    # axs[ax].plot(to_df)\n",
    "    axs[0].bar(np.arange(1,to_df.rank().shape[0]+1),to_df.rank_histogram)\n",
    "    axs[0].set_xlim(1, 12)\n",
    "\n",
    "    # Optionally, adjust tick marks\n",
    "    axs[0].set_xticks(np.arange(1, 13))\n",
    "    # to_df.bar(ax=axs[0], kind='bar', width=1.4)\n",
    "    axs[0].set_title(f'Baseline Rank Histogram')\n",
    "    axs[0].set_xticklabels(axs[0].get_xticklabels(), rotation=0)\n",
    "    axs[0].set_ylabel('Relative Frequency', rotation=90)\n",
    "\n",
    "    \n",
    "    for ax,experiment in enumerate(experiment_list):\n",
    "        ax+=1\n",
    "        data = {key: value for key, value in prediction_file.items() if experiment in key}\n",
    "        data = list(data.values())[0]\n",
    "\n",
    "        # rank_file=rank_histogram_unet\n",
    "        to_df = data.rank_histogram[:].to_dataframe()\n",
    "        to_df['rank_histogram'] = to_df['rank_histogram'] / \\\n",
    "            to_df['rank_histogram'].sum()\n",
    "        to_df['rank'] = to_df.index\n",
    "        to_df['rank'] = to_df['rank'].astype(int)\n",
    "        to_df.index = to_df['rank']\n",
    "        del to_df['lead']\n",
    "        del to_df['skill']\n",
    "        del to_df['rank']\n",
    "        \n",
    "        print(f'Shape of to_df : {to_df.rank().shape[0]}')\n",
    "        # axs[ax].plot(to_df)\n",
    "        axs[ax].bar(np.arange(1,to_df.rank().shape[0]+1),to_df.rank_histogram)\n",
    "        axs[ax].set_xlim(1, 12)\n",
    "\n",
    "        # Optionally, adjust tick marks\n",
    "        axs[ax].set_xticks(np.arange(1, 13))\n",
    "        axs[ax].set_title(experiment)\n",
    "        axs[ax].set_xticklabels(axs[ax].get_xticklabels(), rotation=0)\n",
    "        axs[ax].set_ylabel('Relative Frequency', rotation=90)\n",
    "    plt.suptitle(f'{name_of_file} Rank Histogram', fontsize=30)\n",
    "    plt.tight_layout()\n",
    "    out_dir_save = f'{save_dir}/{name_of_file}.png'\n",
    "    plt.savefig(out_dir_save, dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf212gpu_new]",
   "language": "python",
   "name": "conda-env-tf212gpu_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
