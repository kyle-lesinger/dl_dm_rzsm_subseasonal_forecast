{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7302f239-5aba-4164-8125-86df134c9bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "#import climpredNEW.climpred \n",
    "#from climpredNEW.climpred.options import OPTIONS\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from numpy import meshgrid\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "import matplotlib.colors as mcolors\n",
    "import cartopy.feature as cfeature\n",
    "import itertools\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter, LatitudeLocator\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, TwoSlopeNorm\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.stats import percentileofscore as pos\n",
    "from scipy.stats import rankdata\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "from multiprocessing import Pool\n",
    "from numba import njit,prange\n",
    "import numba\n",
    "\n",
    "\n",
    "from function import preprocessUtils as putils\n",
    "from function import masks\n",
    "from function import verifications\n",
    "from function import funs as f\n",
    "from function import conf\n",
    "from function import loadbias\n",
    "from function import percentile as per\n",
    "from function import caseUtils as cutils\n",
    "from function import masks\n",
    "\n",
    "\n",
    "'''For looping through the GEFS and ECMWF data for percentile distributions'''\n",
    "date_index_start = 210 \n",
    "# [0,15,30,45,60,75,90,105,120,135,150,165,180,195,210,225,240,255,270,285,300,315,330,345,360]\n",
    "\n",
    "global n_processes,day_to_grab\n",
    "n_processes = day_to_grab = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874a15b-c11a-468d-b6dd-110f5f3952b4",
   "metadata": {},
   "source": [
    "# This script will only look at the testing period and include the bias corrected subseasonal reforecasts\n",
    "\n",
    "## Only do this for GLEAM observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "612950b1-1fcf-4522-b79c-7d85f090e726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set script parameters\n",
    "\n",
    "start_obs = '2000-01-01' #Beginning of observation period for analysis. We actually have data starting from 1999 so that we could have a 7-day rolling mean applied to the data and have up to 12 weeks lags for RZSM\n",
    "end_obs = '2020-12-31' #end of observations for ERA5 and GLEAM. We actually needed data through 2020-02-15 since we have an initialization on 2019-12-25\n",
    "start_testing = '2018-01-01' #Beginning of testing period\n",
    "end_testing = '2019-12-31'\n",
    "train_end_string = '2015-12-30' #last string date for training\n",
    "train_end = 2015 #last year of training dates\n",
    "\n",
    "#For creation of percentiles. Get +/- all days from observation within the window\n",
    "window = 45\n",
    "\n",
    "\n",
    "#Unet final experiment name (week 5)\n",
    "experiment_name='EX29_regular_RZSM'\n",
    "\n",
    "region_name = 'CONUS'\n",
    "\n",
    "\n",
    "global mask, mask_anom\n",
    "mask,mask_anom = masks.load_mask_vals(region_name) #Load the mask xarry and mask numpy files. Values of 1 = land \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc269be9-e318-440f-98ad-c0f3a336bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #template for adding data\n",
    "# global template,template_ECM\n",
    "template = xr.open_mfdataset(f'{conf.gefsv12_data}/{region_name}/baseline_RZSM_anomaly/soilw_bgrnd*',combine='nested',concat_dim=['S'],).isel(L=[6,13,20,27,34]).sel(S=slice(start_testing,end_testing)).load()\n",
    "                                                                                                                                                                     \n",
    "# # ECMWF anomaly\n",
    "# ecm_anom = f'{conf.ecmwf_data}/{region_name}/baseline_RZSM_anomaly'\n",
    "# template_ECM = xr.open_mfdataset(f'{ecm_anom}/soilw_bgrnd*',combine='nested',concat_dim=['S'],).sel(L=[6,13,20,27,34]).sel(S=slice(start_testing,end_testing)).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ca21ad54-ead2-4a27-85a1-20f1f7087a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecmwf_bias_corrected = xr.open_dataset(conf.return_bias_corrected_anomaly('CONUS', 'ECMWF')).isel(lead=[6,13,20,27,34]).rename({'init':'S','member':'M','lead': 'L','lat':'Y','lon':'X'})\n",
    "gefs_bias_corrected = xr.open_dataset(conf.return_bias_corrected_anomaly('CONUS', 'GEFSv12')).isel(lead=[6,13,20,27,34]).rename({'init':'S','member':'M','lead': 'L','lat':'Y','lon':'X'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "def54ebf-98f2-4613-9294-8db1d630fc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Observations_open_file_restrict_to_CONUS_apply_7day_rolling_mean(path,start_obs,end_obs,CONUS_mask):\n",
    "    open_file = f.restrict_to_CONUS_bounding_box(xr.open_dataset(path),CONUS_mask)\n",
    "    try:\n",
    "        open_file = open_file.drop('time_bnds')\n",
    "    except ValueError:\n",
    "        open_file = open_file\n",
    "    open_file = open_file.rolling(time=7, min_periods=7,center=False).mean().sel(time = slice(start_obs,end_obs))\n",
    "    return(open_file)\n",
    "\n",
    "def Reforecast_open_file_restrict_to_CONUS_apply_7day_rolling_mean(path,CONUS_mask):\n",
    "    return(f.restrict_to_CONUS_bounding_box(xr.open_mfdataset(path).rolling(L=7, min_periods=7,center=False).mean(),CONUS_mask))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0b38f064-8d7c-4a0f-9c65-06349b10dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_julian_dates_ECMWF(reforecast_file):\n",
    "    dim_order = ['S','M','L','Y','X']\n",
    "    \n",
    "    new_save_dir = f'{conf.ecmwf_data}/{region_name}/soilw_bgrnd/RZSM_anomaly_with_julian_dates_testing_distribution'\n",
    "    os.system(f'mkdir -p {new_save_dir}')\n",
    "    \n",
    "    #Make the julian dates for easier processing\n",
    "    for idx,date in enumerate(reforecast_file.S.values):\n",
    "        # break\n",
    "        date_out = f'{pd.to_datetime(date).year}-{pd.to_datetime(date).month:02}-{pd.to_datetime(date).day:02}'\n",
    "        out_name = f'{new_save_dir}/soilw_bgrnd_{date_out}.nc'\n",
    "        \n",
    "        if os.path.exists(out_name):\n",
    "            pass\n",
    "        else:\n",
    "            single_file = reforecast_file.sel(S=date).expand_dims({'S': 1}).transpose(*dim_order)\n",
    "            date_list = [single_file.S.values + np.timedelta64(i, 'D') for i in single_file.L.values]\n",
    "\n",
    "            julian_dates = [datestdtojd(pd.to_datetime(i).year[0], pd.to_datetime(i).month[0], pd.to_datetime(i).day[0]) for i in date_list]\n",
    "            single_file['L'] = julian_dates\n",
    "            single_file.to_netcdf(out_name)\n",
    "\n",
    "    print('Loading the ECMWF julian day anomaly dataset')\n",
    "    \n",
    "    return(xr.open_mfdataset(f'{new_save_dir}/soil*',combine='nested',concat_dim=['S']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "49b84273-71c6-4f2d-a438-c3da68c14643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datestdtojd (before_year, before_month, before_day):\n",
    "    # convert dates to julian date to allow for window selection\n",
    "    fmt='%Y-%m-%d %H:%M:%S'\n",
    "    target_date = str(datetime(before_year, before_month, before_day))\n",
    "    sdtdate = datetime.strptime(target_date, fmt)\n",
    "    sdtdate = sdtdate.timetuple()\n",
    "    jdate = sdtdate.tm_yday\n",
    "    return(jdate)\n",
    "\n",
    "def jdtodatestd (jdate):\n",
    "    #convert julian date to datetime \n",
    "    fmt = '%Y%j'\n",
    "    datestd = datetime.datetime.strptime(jdate, fmt).date()\n",
    "    return(datestd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd273d98-22c0-4020-ad89-1edcffa61bce",
   "metadata": {},
   "source": [
    "# Create the percentile dataset first. Need to have the percentiles from within a +/- 45 day window of the observation day over all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd7f6f22-626d-44ca-8a24-1984102681ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def window_select(file,time,X,Y,window):\n",
    "    #Grab all time samples from within the window size\n",
    "    #Test \n",
    "    # file = obs_RZSM_anomaly\n",
    "    # time = time\n",
    "    # window=45\n",
    "\n",
    "    before_month =  pd.to_datetime(time - np.timedelta64(45,'D')).month\n",
    "    before_day = pd.to_datetime(time - np.timedelta64(45,'D')).day\n",
    "    before_year = pd.to_datetime(time - np.timedelta64(45,'D')).year\n",
    "    \n",
    "    julian_date_before =  datestdtojd (before_year, before_month, before_day)\n",
    "    \n",
    "    after_month =  pd.to_datetime(time + np.timedelta64(45,'D')).month\n",
    "    after_day = pd.to_datetime(time + np.timedelta64(45,'D')).day\n",
    "    after_year =  pd.to_datetime(time + np.timedelta64(45,'D')).year\n",
    "    \n",
    "    julian_date_after =  datestdtojd (after_year, after_month, after_day)\n",
    "    \n",
    "    #Now we have created the before and after julian dates, now subset all data\n",
    "    if julian_date_before < julian_date_after:\n",
    "        selected_data = file.sel(time=(file['day_of_year'] > julian_date_before) & (file['day_of_year'] < julian_date_after)).isel(longitude=X,latitude=Y)[putils.xarray_varname(file)].values\n",
    "    else:\n",
    "        selected_data = file.sel(time=(file['day_of_year'] > julian_date_before) | (file['day_of_year'] < julian_date_after)).isel(longitude=X,latitude=Y)[putils.xarray_varname(file)].values\n",
    "    \n",
    "    #Remove any bad data points\n",
    "    selected_data = selected_data[selected_data != 0]\n",
    "    selected_data = selected_data[~np.isnan(selected_data)]\n",
    "    \n",
    "    return(selected_data)\n",
    "\n",
    "\n",
    "\n",
    "def window_select_OBS(file,time,window):\n",
    "    #Grab all time samples from within the window size\n",
    "    #Test \n",
    "    # file = obs_RZSM_anomaly\n",
    "    # time = time\n",
    "    # window=45\n",
    "\n",
    "    before_month =  pd.to_datetime(time - np.timedelta64(45,'D')).month\n",
    "    before_day = pd.to_datetime(time - np.timedelta64(45,'D')).day\n",
    "    before_year = pd.to_datetime(time - np.timedelta64(45,'D')).year\n",
    "    \n",
    "    julian_date_before =  datestdtojd (before_year, before_month, before_day)\n",
    "    \n",
    "    after_month =  pd.to_datetime(time + np.timedelta64(45,'D')).month\n",
    "    after_day = pd.to_datetime(time + np.timedelta64(45,'D')).day\n",
    "    after_year =  pd.to_datetime(time + np.timedelta64(45,'D')).year\n",
    "    \n",
    "    julian_date_after =  datestdtojd (after_year, after_month, after_day)\n",
    "\n",
    "    #Now we have created the before and after julian dates, now subset all data\n",
    "    if julian_date_before < julian_date_after:\n",
    "        selected_data = file.sel(time=(file['day_of_year'] > julian_date_before) & (file['day_of_year'] < julian_date_after))[putils.xarray_varname(file)].values\n",
    "    else:\n",
    "        selected_data = file.sel(time=(file['day_of_year'] > julian_date_before) | (file['day_of_year'] < julian_date_after))[putils.xarray_varname(file)].values\n",
    "\n",
    "    return(selected_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533e609-a774-4b73-95c1-064b53a3d480",
   "metadata": {},
   "source": [
    "# Now convert the observation percentiles to the same format as GEFSv12 reforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2bb9c778-0f01-4adf-b8a2-e931d64657fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dates = gefs_bias_corrected.init.values\n",
    "init_date_list = [pd.to_datetime(i) for i in dates]\n",
    "# init_date_list = sorted([i.split('_')[-1][:-3] for i in dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a6857960-9baf-4d3a-b0d5-f7a2674ebea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "global percentile_ranges\n",
    "percentile_ranges = [5,10,20,33,66,80,90,95]\n",
    "\n",
    "save_percentile_observations = f'{conf.gleam_data}/{region_name}/anomaly_percentile_RZSM_full_distribution_with_different_thresholds_testing_distribution.nc4'\n",
    "\n",
    "if os.path.exists(save_percentile_observations):\n",
    "    obs_RZSM_percentile = xr.open_dataset(save_percentile_observations).load()\n",
    "    obs_RZSM_percentile.close()\n",
    "else:\n",
    "    obs_RZSM_anomaly = xr.open_dataset(f'{conf.gleam_data}/{region_name}/RZSM_anomaly.nc').sel(time=slice(start_testing,'2020-03-30')).rename({'SMsurf':'RZSM'}).load()\n",
    "    obs_RZSM_anomaly \n",
    "    # Calculate day of the year (Julian day)\n",
    "    obs_RZSM_anomaly['day_of_year'] = obs_RZSM_anomaly['time'].dt.dayofyear\n",
    "\n",
    "    #Convert to array \n",
    "    # obs_RZSM_arr = np.array(obs_RZSM_anomaly.RZSM.values)\n",
    "\n",
    "    obs_RZSM_percentile = obs_RZSM_anomaly.copy(deep=True)\n",
    "\n",
    "\n",
    "    \n",
    "    for i in percentile_ranges:\n",
    "        obs_RZSM_percentile[f'{i}th_percentile'] = xr.DataArray(np.empty(obs_RZSM_percentile.RZSM.shape, dtype='float'), dims=obs_RZSM_percentile.RZSM.dims)\n",
    "    # Assuming obs_RZSM_percentile is an xarray Dataset\n",
    "    # if 'serialized_percentiles' not in obs_RZSM_percentile:\n",
    "    #     obs_RZSM_percentile['serialized_percentiles'] = xr.DataArray(np.empty(obs_RZSM_percentile.RZSM.shape, dtype='object'), dims=obs_RZSM_percentile.RZSM.dims)\n",
    "\n",
    "    del obs_RZSM_percentile['RZSM']\n",
    "    del obs_RZSM_percentile['day_of_year']\n",
    "    \n",
    "    #Find percentile of score for each grid cell\n",
    "    \n",
    "\n",
    "    for idx,time in enumerate(obs_RZSM_anomaly.time.values[0:367]):\n",
    "        '''We only want to do the first year. Then we will add back the values later for other days'''\n",
    "        print(f'Working on date index {idx} out of 367')\n",
    "        \n",
    "        all_values = window_select_OBS(obs_RZSM_anomaly,time,window)\n",
    "        all_values.shape\n",
    "\n",
    "        all_value_percentiles = {i:np.nanpercentile(all_values,i,axis=0) for i in percentile_ranges}\n",
    "        \n",
    "        for i in percentile_ranges:\n",
    "            obs_RZSM_percentile[f'{i}th_percentile'][idx,:,:] = all_value_percentiles[i]\n",
    "    \n",
    "\n",
    "    #Now we need to add the data from each day of the first year to the next years\n",
    "\n",
    "    #First get the base values for the 366 days that we computed\n",
    "    time_dates = obs_RZSM_anomaly.time.values[0:367]\n",
    "\n",
    "    dict_base = {}\n",
    "    for date_ in time_dates:\n",
    "        month_day = f'{pd.to_datetime(date_).month:02}-{pd.to_datetime(date_).day:02}'\n",
    "        dict_base[month_day] = {}\n",
    "        for i in percentile_ranges:\n",
    "            # break\n",
    "            dict_base[month_day][f'{i}th_percentile'] = obs_RZSM_percentile[f'{i}th_percentile'].sel(time=date_).values\n",
    "\n",
    "    #Now we need to add everything to each dataset for each date to have all the values\n",
    "    for idx,date_ in enumerate(obs_RZSM_percentile.time.values):\n",
    "        # break\n",
    "        month_day = f'{pd.to_datetime(date_).month:02}-{pd.to_datetime(date_).day:02}'\n",
    "\n",
    "        for i in percentile_ranges:\n",
    "            if month_day == '02-29':\n",
    "                pass\n",
    "            else:\n",
    "                obs_RZSM_percentile[f'{i}th_percentile'][idx,:,:] =  dict_base[month_day][f'{i}th_percentile']\n",
    "\n",
    "    obs_RZSM_percentile = obs_RZSM_percentile.astype(np.float32)\n",
    "    \n",
    "    obs_RZSM_percentile.to_netcdf(save_percentile_observations)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ca59da08-0969-4573-9798-ac9f53fe8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_percentile_of_score_observations = f'{conf.gleam_data}/{region_name}/anomaly_percentile_of_score_RZSM_testing_distribution.nc4'\n",
    "obs_RZSM_anomaly = xr.open_dataset(f'{conf.gleam_data}/{region_name}/RZSM_anomaly.nc').sel(time=slice(start_testing,'2020-03-30')).rename({'SMsurf':'RZSM'}).load()\n",
    "obs_RZSM_anomaly \n",
    "\n",
    "# Calculate day of the year (Julian day)\n",
    "obs_RZSM_anomaly['day_of_year'] = obs_RZSM_anomaly['time'].dt.dayofyear\n",
    "obs_RZSM_output = obs_RZSM_anomaly.copy(deep=True)\n",
    "obs_RZSM_output.RZSM[:,:,:] = np.nan\n",
    "\n",
    "obs_RZSM_pos = np.empty(shape=obs_RZSM_anomaly.RZSM.shape)\n",
    "obs_RZSM_pos[:,:,:] = np.nan\n",
    "\n",
    "obs_RZSM_anomaly_ARR = obs_RZSM_anomaly.RZSM.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d8aa210-1373-4449-b94e-d50d4340e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    obs_RZSM_output = xr.open_dataset(save_percentile_of_score_observations)\n",
    "except FileNotFoundError:\n",
    "    for idx,time in enumerate(obs_RZSM_anomaly.time.values):\n",
    "        # break\n",
    "        a = window_select_OBS(obs_RZSM_anomaly,time,window) #All of the values within the windows\n",
    "    \n",
    "        print(f'Working on day {idx} out of {len(obs_RZSM_anomaly.time.values)}')\n",
    "        for Y in range(obs_RZSM_anomaly.latitude.shape[0]):\n",
    "    \n",
    "            for X in range(obs_RZSM_anomaly.longitude.shape[0]):\n",
    "                if mask_anom[Y,X] == 1:\n",
    "                    obs_RZSM_pos[idx,Y,X] =  pos(a[:,Y,X],obs_RZSM_anomaly_ARR[idx,Y,X])\n",
    "    \n",
    "    obs_RZSM_output.RZSM[:,:,:] = obs_RZSM_pos\n",
    "    \n",
    "    obs_RZSM_output.to_netcdf(save_percentile_of_score_observations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "21345d44-f7ae-4431-aaf5-eb6b8816b067",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-01-03 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-01-10 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-01-17 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-01-24 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-01-31 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-02-07 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-02-14 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-02-21 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-02-28 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-03-07 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-03-14 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-03-21 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-03-28 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-04-04 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-04-11 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-04-18 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-04-25 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-05-02 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-05-09 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-05-16 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-05-23 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-05-30 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-06-06 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-06-13 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-06-20 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-06-27 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-07-04 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-07-11 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-07-18 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-07-25 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-08-01 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-08-08 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-08-15 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-08-22 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-08-29 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-09-05 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-09-12 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-09-19 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-09-26 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-10-03 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-10-10 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-10-17 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-10-24 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-10-31 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-11-07 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-11-14 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-11-21 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-11-28 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-12-05 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-12-12 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-12-19 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2018-12-26 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-01-02 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-01-09 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-01-16 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-01-23 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-01-30 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-02-06 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-02-13 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-02-20 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-02-27 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-03-06 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-03-13 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-03-20 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-03-27 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-04-03 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-04-10 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-04-17 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-04-24 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-05-01 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-05-08 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-05-15 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-05-22 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-05-29 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-06-05 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-06-12 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-06-19 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-06-26 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-07-03 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-07-10 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-07-17 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-07-24 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-07-31 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-08-07 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-08-14 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-08-21 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-08-28 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-09-04 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-09-11 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-09-18 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-09-25 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-10-02 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-10-09 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-10-16 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-10-23 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-10-30 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-11-06 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-11-13 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-11-20 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-11-27 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-12-04 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-12-11 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-12-18 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n",
      "Test stop 1\n",
      "Test stop 2\n",
      "Test stop 3\n",
      "Test stop 4\n",
      "Test stop 5\n",
      "Test stop 6\n",
      "Working on initialized day 2019-12-25 00:00:00 to find values integrating with SubX models, leads, & coordinates and saving data into /glade/work/klesinger/FD_RZSM_deep_learning/Data/reanalysis/GLEAM/CONUS/RZSM_percentile_reformat_testing_distribution.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_OBS_percentiles_to_SubX_format(_date):  \n",
    "# for _date in init_date_list:\n",
    "    # break\n",
    "    # _date=init_date_list[0]\n",
    "    \n",
    "    ref_dir =f'{conf.gefsv12_data}/{region_name}/soilw_bgrnd' #Just use a single reference directory to serve as the template for file creation\n",
    "    save_dir = f'{conf.gleam_data}/{region_name}/RZSM_percentile_reformat_testing_distribution'\n",
    "    print('Test stop 1')\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    \n",
    "    '''We are going to create new leads that are different than reforecast. The reasoning for this is that we want the actual weekly lags (and 1 day lag) and this will\n",
    "    assist with future predictions within the deep learning model'''\n",
    "    print('Test stop 2')\n",
    "    #Grab a single SubX to use as the template. Doesn't matter if it is the same variable or not or the same date\n",
    "    fcst_file = glob(f'{ref_dir}/*2000-01-05*')[0]\n",
    "    fcst_file = xr.open_dataset(fcst_file).load() #(1, 11, 35, 48, 96)\n",
    "    print('Test stop 3')\n",
    "\n",
    "    if region_name == 'CONUS':\n",
    "        new_X_coords = [i+360 if i < 0 else i for i in fcst_file.X.values]\n",
    "        fcst_file = fcst_file.assign_coords({'X':new_X_coords})\n",
    "    print('Test stop 4')\n",
    "    fcst_file = putils.restrict_to_bounding_box(fcst_file,mask)\n",
    "    print('Test stop 5')\n",
    "    #Create a file to overwrite\n",
    "    out_file = xr.zeros_like(fcst_file)\n",
    "\n",
    "    obs_file_name = f'RZSM_percentile_reformat_{_date}.nc4'\n",
    "    save_file = f'{save_dir}/{obs_file_name}'\n",
    "    print('Test stop 6')\n",
    "    if os.path.exists(save_file):\n",
    "        pass\n",
    "    else:\n",
    "        print(f'Working on initialized day {_date} to find values integrating with SubX models, leads, & coordinates and saving data into {save_dir}.')\n",
    "\n",
    "        for idx,i_lead in enumerate(fcst_file.L.values):\n",
    "            # break\n",
    "\n",
    "            date_val = pd.to_datetime(pd.to_datetime(_date) + dt.timedelta(days=int(i_lead)+0)) #Adding +1 may be suitable for other forecasts which predict the next day. But GEFSv12 predicts lead 0 as 12 UTC on the same date it is initialized\n",
    "            #But be careful if you adapt this code to a new script. We are looking backwards in time from the first date.\n",
    "\n",
    "            date_val = f'{date_val.year}-{date_val.month:02}-{date_val.day:02}'\n",
    "\n",
    "            out_file[putils.xarray_varname(out_file)][0,:, idx, :, :] = \\\n",
    "                obs_file[putils.xarray_varname(obs_file)].sel(time = date_val).values\n",
    "\n",
    "        var_OUT = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                RZSM_percentile = (['S','M','L','Y','X'],    out_file[list(out_file.keys())[0]].values),\n",
    "            ),\n",
    "            coords = dict(\n",
    "                S = np.atleast_1d(_date),\n",
    "                X = out_file.X.values,\n",
    "                Y = out_file.Y.values,\n",
    "                L = list(out_file.L.values[:]),\n",
    "                M = out_file.M.values,\n",
    "\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = f'RZSM percentiles values on the exact same date and grid \\\n",
    "                cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "        )                    \n",
    "        var_OUT.to_netcdf(save_file)\n",
    "\n",
    "    return(0)\n",
    "\n",
    "global obs_file\n",
    "obs_file = xr.open_dataset(f'{conf.gleam_data}/{region_name}/anomaly_percentile_of_score_RZSM_testing_distribution.nc4').drop('day_of_year').load()\n",
    "\n",
    "\n",
    "\n",
    "####### RUN FUNCTION #######\n",
    "for _date in init_date_list:\n",
    "    \n",
    "    convert_OBS_percentiles_to_SubX_format(_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8779a-de45-47a7-a8e5-8ce41ca5ece2",
   "metadata": {},
   "source": [
    "# Create a new RZSM reforecast set of data which has the julian dates only and not the lead times\n",
    "\n",
    "## Create dataset from predictions for each week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fe5ec8b5-161c-4e2d-bfce-8003d7fad532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def window_select_reforecast_by_realization(file,time,window,add_window):\n",
    "    #Grab all time samples from within the window size\n",
    "    #Test \n",
    "    # file = obs_RZSM_anomaly\n",
    "    # time = time\n",
    "    # window=45\n",
    "\n",
    "    before_month =  pd.to_datetime(time - np.timedelta64(45+add_window,'D')).month\n",
    "    before_day = pd.to_datetime(time - np.timedelta64(45+add_window,'D')).day\n",
    "    before_year = pd.to_datetime(time - np.timedelta64(45+add_window,'D')).year\n",
    "    \n",
    "    julian_date_before =  datestdtojd (before_year, before_month, before_day)\n",
    "    \n",
    "    after_month =  pd.to_datetime(time + np.timedelta64(45+add_window,'D')).month\n",
    "    after_day = pd.to_datetime(time + np.timedelta64(45+add_window,'D')).day\n",
    "    after_year =  pd.to_datetime(time + np.timedelta64(45+add_window,'D')).year\n",
    "    \n",
    "    julian_date_after =  datestdtojd (after_year, after_month, after_day)\n",
    "    \n",
    "    #Now we have created the before and after julian dates, now subset all data\n",
    "    #We are adding 15 becuase otherwise we have to load each day individually and it takes a long time, this should shorten the waiting time.\n",
    "    if julian_date_before < julian_date_after:\n",
    "        selected_data = file.sel(L=(file['L'] > julian_date_before) & (file['L'] < julian_date_after)).load()\n",
    "    else:\n",
    "        selected_data = file.sel(L=(file['L'] > julian_date_before) | (file['L'] < julian_date_after)).load()\n",
    "    \n",
    "    \n",
    "    return(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cf6206f2-7fe2-4b40-9dc8-2daab1f2aa3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def window_select_reforecast(file,time,window):\n",
    "    #Grab all time samples from within the window size\n",
    "    #Test \n",
    "    # file = obs_RZSM_anomaly\n",
    "    # time = time\n",
    "    # window=45\n",
    "\n",
    "    before_month =  pd.to_datetime(time - np.timedelta64(45,'D')).month\n",
    "    before_day = pd.to_datetime(time - np.timedelta64(45,'D')).day\n",
    "    before_year = pd.to_datetime(time - np.timedelta64(45,'D')).year\n",
    "    \n",
    "    julian_date_before =  datestdtojd (before_year, before_month, before_day)\n",
    "    \n",
    "    after_month =  pd.to_datetime(time + np.timedelta64(45,'D')).month\n",
    "    after_day = pd.to_datetime(time + np.timedelta64(45,'D')).day\n",
    "    after_year =  pd.to_datetime(time + np.timedelta64(45,'D')).year\n",
    "    \n",
    "    julian_date_after =  datestdtojd (after_year, after_month, after_day)\n",
    "    \n",
    "    #Now we have created the before and after julian dates, now subset all data\n",
    "    #We are adding 15 becuase otherwise we have to load each day individually and it takes a long time, this should shorten the waiting time.\n",
    "    if julian_date_before < julian_date_after:\n",
    "        selected_data = file.sel(L=(file['L'] > julian_date_before) & (file['L'] < julian_date_after)).load()\n",
    "    else:\n",
    "        selected_data = file.sel(L=(file['L'] > julian_date_before) | (file['L'] < julian_date_after)).load()\n",
    "    \n",
    "    \n",
    "    return(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7171a59c-765e-4dcc-a44f-e9fd0221da2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def return_julian_before_after(time):\n",
    "    #Grab all time samples from within the window size\n",
    "    #Test \n",
    "    # file = obs_RZSM_anomaly\n",
    "    # time = time\n",
    "    # window=45\n",
    "\n",
    "    before_month =  pd.to_datetime(time - np.timedelta64(45,'D')).month\n",
    "    before_day = pd.to_datetime(time - np.timedelta64(45,'D')).day\n",
    "    before_year = pd.to_datetime(time - np.timedelta64(45,'D')).year\n",
    "    \n",
    "    julian_date_before =  datestdtojd (before_year, before_month, before_day)\n",
    "    \n",
    "    after_month =  pd.to_datetime(time + np.timedelta64(45,'D')).month\n",
    "    after_day = pd.to_datetime(time + np.timedelta64(45,'D')).day\n",
    "    after_year =  pd.to_datetime(time + np.timedelta64(45,'D')).year\n",
    "    \n",
    "    julian_date_after =  datestdtojd (after_year, after_month, after_day)\n",
    " \n",
    "    \n",
    "    return(julian_date_before,julian_date_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1bc7320d-e898-44d9-ba63-290fa36428bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_day_list(month_day):\n",
    "    final_dates = []\n",
    "    for i in month_day:\n",
    "        if i not in final_dates:\n",
    "            final_dates.append(i)\n",
    "        else:\n",
    "            pass\n",
    "    return(final_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ffb0ca1c-2432-4520-8f76-d9703793df03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def percentile_of_score_by_ensemble_mean(anomaly_file, julian_file, percentile_output,save_dir):\n",
    "     \n",
    "    \n",
    "#     #Test \n",
    "#     # anomaly_file = base_reforecast_anomaly\n",
    "#     # julian_file = base_reforecast_anomaly_julian\n",
    "#     # save_dir = 'Data/GEFSv12_reforecast/soilw_bgrnd/percentiles_baseline'\n",
    "    \n",
    "#     #first find out what the unique dates in the file\n",
    "#     all_dates = anomaly_file.S.values\n",
    "    \n",
    "#     month_day = sorted([f'{pd.to_datetime(i).month:02}-{pd.to_datetime(i).day:02}' for i in all_dates])\n",
    "    \n",
    "#     final_dates = []\n",
    "#     for i in month_day:\n",
    "#         if i not in final_dates:\n",
    "#             final_dates.append(i)\n",
    "#         else:\n",
    "#             pass\n",
    "    \n",
    "#     for i in final_dates:\n",
    "#         dates = i.split('-')\n",
    "#         month = int(dates[0])\n",
    "#         day = int(dates[1])\n",
    "#         # break\n",
    "#         #Now loop through each of the month_days and create the distribution\n",
    "#         run_dates = anomaly_file.sel(S=(anomaly_file['S.month'] == month) & (anomaly_file['S.day'] == day ))\n",
    "        \n",
    "#         #Now check if files exists\n",
    "#         saved_dates1 = run_dates.S.values\n",
    "#         saved_dates2 = [f'RZSM_percentiles_MEM_{pd.to_datetime(i).year}-{pd.to_datetime(i).month:02}-{pd.to_datetime(i).day:02}.nc' for i in saved_dates1]\n",
    "        \n",
    "#         completed_or_not = []\n",
    "#         for i in saved_dates2:\n",
    "#             if os.path.exists(f'{save_dir}/{i}'):\n",
    "#                 completed_or_not.append(True)\n",
    "#             else:\n",
    "#                 completed_or_not.append(False)\n",
    "        \n",
    "#         if len(completed_or_not) == sum(completed_or_not):\n",
    "#             #All files are already completed\n",
    "#             pass\n",
    "#         else:\n",
    "            \n",
    "#             print(f'Starting percentiles on:')\n",
    "#             print(saved_dates2)\n",
    "#             def run_selected_window_to_save_memory():\n",
    "#                 #first choose a single date for the window\n",
    "#                 single_date = saved_dates1[0]\n",
    "\n",
    "#                 #Grab the distribution\n",
    "#                 selected_window = window_select_reforecast(file=julian_file, time=single_date,window=window)\n",
    "\n",
    "#                 #Now loop through each of the actual files and create the percentile distribution\n",
    "#                 for idx,date_run in enumerate(saved_dates1):\n",
    "#                     # break\n",
    "#                     file_run  = anomaly_file.sel(S=date_run)\n",
    "\n",
    "#                     for X in range(anomaly_file.X.shape[0]):\n",
    "#                         for Y in range(anomaly_file.Y.shape[0]):\n",
    "#                             all_leads = anomaly_file.isel(X=X,Y=Y).sel(S=date_run).RZSM.values\n",
    "#                             all_values = selected_window.isel(X=X,Y=Y).RZSM.values.flatten()\n",
    "#                             all_values = all_values[~np.isnan(all_values)]\n",
    "#                             all_values = all_values[all_values != 0]\n",
    "#                             #percentile of score\n",
    "#                             percentile = pos(all_values, all_leads)\n",
    "\n",
    "#                             #Now add back to the dataset\n",
    "#                             #get index\n",
    "#                             index = int(np.where(anomaly_file['S'] == pd.to_datetime(date_run))[0])\n",
    "#                             percentile_output.RZSM[index,:, Y, X] = percentile\n",
    "                    \n",
    "#                     percentile_output.sel(S=date_run).to_netcdf(f'{save_dir}/{saved_dates2[idx]}')\n",
    "                \n",
    "#                 del selected_window\n",
    "                \n",
    "#                 return(0)\n",
    "            \n",
    "#             #Run function\n",
    "#             run_selected_window_to_save_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5277af11-bc20-4de6-9f69-b83700762693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reforecast_with_predictions(experiment_name):\n",
    "    #Load previous predictions from experiments to create a percentile distribution\n",
    "    temp_cp = template.copy(deep=True)\n",
    "\n",
    "    for idx,lead in enumerate([1,2,3,4]):\n",
    "        day_num = (lead*7)-1\n",
    "\n",
    "        test = verifications.reverse_min_max_scaling(np.load(f'predictions/{region_name}/Wk{lead}_testing/Wk{lead}_testing_{experiment_name}.npy')[-1,:,:,:,0], region_name, day_num,'GEFSv12',2019)\n",
    "        test = np.reshape(test,(test.shape[0]//11,11,test.shape[1],test.shape[2]))\n",
    "\n",
    "        #Add data to file\n",
    "        temp_cp.RZSM[0:,:,idx,:,:] = test\n",
    "    \n",
    "    return(temp_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97825e40-d7ef-4464-8456-89aeb06929fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bfbb99fb-81b0-4bba-bfbf-ec391a141255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_data_julian_dates(experiment_list):\n",
    "    dim_order = ['S','M','L','Y','X']\n",
    "    lead = int(list(experiment_list.keys())[0])\n",
    "    day_num = (lead*7)-1\n",
    "    \n",
    "    print('Saving the anomaly prediction without julian dates')\n",
    "    no_julian_dir = f'predictions/{region_name}/anomaly_no_julian_dates_bias_corrected_testing_distribution'\n",
    "    os.system(f'mkdir -p {no_julian_dir}')\n",
    "    reforecast_RZSM = create_reforecast_with_predictions(experiment_list,day_num)\n",
    "    \n",
    "    for date in reforecast_RZSM.S.values:\n",
    "        # break\n",
    "        save_no_julian_file = f'{no_julian_dir}/{experiment_name}_{pd.to_datetime(date).year}-{pd.to_datetime(date).month:02}-{pd.to_datetime(date).day:02}.nc'\n",
    "        if os.path.exists(save_no_julian_file):\n",
    "            pass\n",
    "        else:\n",
    "            reforecast_RZSM.sel(S=date).expand_dims({'S': 1}).transpose(*dim_order).to_netcdf(save_no_julian_file)\n",
    "\n",
    "    new_save_dir = 'predictions/anomaly_experiment_RZSM_julian_dates_bias_corrected'\n",
    "    os.system(f'mkdir -p {new_save_dir}')\n",
    "    \n",
    "    #Make the julian dates for easier processing\n",
    "    for idx,date in enumerate(reforecast_RZSM.S.values):\n",
    "        # break\n",
    "        date_out = f'{pd.to_datetime(date).year}-{pd.to_datetime(date).month:02}-{pd.to_datetime(date).day:02}'\n",
    "        out_name = f'{new_save_dir}/RZSM_{experiment_name}_{date_out}.nc'\n",
    "        \n",
    "        if os.path.exists(out_name):\n",
    "            pass\n",
    "        else:\n",
    "            single_file = reforecast_RZSM.sel(S=date).expand_dims({'S': 1}).transpose(*dim_order)\n",
    "            date_list = [single_file.S.values + np.timedelta64(i, 'D') for i in single_file.L.values]\n",
    "\n",
    "            julian_dates = [datestdtojd(pd.to_datetime(i).year[0], pd.to_datetime(i).month[0], pd.to_datetime(i).day[0]) for i in date_list]\n",
    "            single_file['L'] = julian_dates\n",
    "            single_file.to_netcdf(out_name)\n",
    "            \n",
    "    return('Completed dataset setup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cbcd0df9-e122-4799-9439-be0a399921b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_of_score_by_realization_GEFS(anomaly_file, julian_file, save_dir, MEM_or_by_model):\n",
    "    percentile_output = template.copy(deep=True)\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    if MEM_or_by_model == 'MEM':\n",
    "        RZSM_name = 'soilw_bgrnd_percentiles_MEM'\n",
    "        anomaly_file = anomaly_file.mean(dim='M')\n",
    "        julian_file = julian_file.mean(dim='M')\n",
    "        percentile_output = percentile_output.mean(dim='M')\n",
    "    else:\n",
    "        RZSM_name = 'soilw_bgrnd_percentiles'\n",
    "\n",
    "    #first find out what the unique dates in the file\n",
    "    all_dates = anomaly_file.S.values\n",
    "    \n",
    "    month_day = sorted([f'{pd.to_datetime(i).month:02}-{pd.to_datetime(i).day:02}' for i in all_dates])\n",
    "    final_dates = create_final_day_list(month_day)\n",
    "    final_dates.reverse()\n",
    "    \n",
    "    #run through several days at a time because it's a slow process to keep re-loading the same file when they are only 1 julian day apart\n",
    "    total_days = len(final_dates)\n",
    "    day_to_grab = 15\n",
    "    \n",
    "    for date_index in range(0, total_days, day_to_grab):\n",
    "        # date_index=0\n",
    "        # break\n",
    "        non_completed_dates = per.get_non_completed_days(final_dates,date_index,day_to_grab, anomaly_file, RZSM_name, save_dir)\n",
    "        print(f'Non completed dates:')\n",
    "        print(non_completed_dates) \n",
    "           \n",
    "        #Now loop through all the non-completed days\n",
    "        def run_subset(date_to_run,selected_window):\n",
    "            dattt = date_to_run.split('-')\n",
    "            m = int(dattt[0])\n",
    "            d = int(dattt[1])\n",
    "        \n",
    "            run_dates_final = anomaly_file.sel(S=(anomaly_file['S.month'] == m) & (anomaly_file['S.day'] == d))\n",
    "\n",
    "            #Create the actual names of the files\n",
    "            saved_dates1 = run_dates_final.S.values\n",
    "            saved_dates2 = [f'{RZSM_name}_{pd.to_datetime(i).year}-{pd.to_datetime(i).month:02}-{pd.to_datetime(i).day:02}.nc' for i in saved_dates1]\n",
    "\n",
    "            print(f'Starting percentiles on:')\n",
    "            print(saved_dates2)\n",
    "            #Now get the julian dates to properly subset\n",
    "            julian_date_before,julian_date_after = return_julian_before_after(saved_dates1[0])\n",
    "\n",
    "            #grab subset of julian dates\n",
    "            if julian_date_before < julian_date_after:\n",
    "                julian_date_before_subset = selected_window.sel(L=(selected_window['L'] > julian_date_before) & (selected_window['L'] < julian_date_after))\n",
    "            else:\n",
    "                julian_date_before_subset = selected_window.sel(L=(selected_window['L'] > julian_date_before) | (selected_window['L'] < julian_date_after))\n",
    "\n",
    "            #Now loop through each of the actual files and create the percentile distribution\n",
    "\n",
    "            # index = int(np.where(anomaly_file['S'] == pd.to_datetime(date_run))[0])\n",
    "            if MEM_or_by_model == 'model':\n",
    "                '''For this, we want to include the entire distribution when comparing with each realization. Each realization\n",
    "                is not considered its own model'''\n",
    "                all_leads1 = anomaly_file.RZSM.values\n",
    "                all_leads1.shape\n",
    "                'Perform the operation over all inits and realizations for each coordinate'''\n",
    "                for X in range(anomaly_file.X.shape[0]):\n",
    "                    for Y in range(anomaly_file.Y.shape[0]):\n",
    "                        print(f'X: {X}, Y: {Y}')\n",
    "                        if mask_anom[Y,X]==0 :\n",
    "                            pass\n",
    "                        else:\n",
    "                            all_leads = all_leads1[:,:,:,Y,X]\n",
    "                            all_values = julian_date_before_subset.isel(Y=Y,X=X).RZSM.values.flatten()\n",
    "                            all_values = all_values[~np.isnan(all_values)]\n",
    "                            all_values = all_values[all_values != 0]\n",
    "                            #percentile of score\n",
    "                            percentile = pos(all_values, all_leads)\n",
    "    \n",
    "                            #Now add back to the dataset\n",
    "                            #get index\n",
    "                            \n",
    "                            percentile_output.RZSM[:,:,:, Y, X] = percentile\n",
    "                \n",
    "                for idx,date_run in enumerate(saved_dates1):\n",
    "                    percentile_output.sel(S=date_run).to_netcdf(f'{save_dir}/{saved_dates2[idx]}')\n",
    "\n",
    "            del julian_date_before_subset\n",
    "\n",
    "            return(0)\n",
    "\n",
    "        def run_selected_window_to_save_memory(non_completed_dates):\n",
    "            #first get the first date to only have to load all the files once\n",
    "            dates_init = non_completed_dates[0].split('-')\n",
    "            month_init = int(dates_init[0])\n",
    "            day_init = int(dates_init[1])\n",
    "\n",
    "            #Select the anomaly file dates just to have it to access later dates\n",
    "            run_dates = anomaly_file.sel(S=(anomaly_file['S.month'] == month_init) & (anomaly_file['S.day'] == day_init))\n",
    "            saved_dates1 = run_dates.S.values\n",
    "\n",
    "            #first choose a single date for the window\n",
    "            single_date = saved_dates1[0]\n",
    "\n",
    "            #Grab the distribution (getting even days 15 after)\n",
    "            selected_window = window_select_reforecast_by_realization(file=julian_file, time=single_date,window=window,add_window=day_to_grab)\n",
    "            selected_window = selected_window.load()\n",
    "            \n",
    "            #Now loop through each of the non_completed dates\n",
    "            for date_to_run in non_completed_dates:\n",
    "                # break\n",
    "                # date_to_run = '12-31'\n",
    "                run_subset(date_to_run,selected_window)\n",
    "\n",
    "            del selected_window\n",
    "            return(0)\n",
    "\n",
    "        #Run function\n",
    "        if len(non_completed_dates) > 0 :\n",
    "            run_selected_window_to_save_memory(non_completed_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ef34e526-901e-43dc-9393-f1b0671e8f2f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def percentileofscoreFUNC(a, score):\n",
    "    # a,score = all_values, all_leads1[:,Y,X]\n",
    "    \"\"\"Compute the percentile rank of a score relative to a list of scores.\n",
    "\n",
    "    A `percentileofscore` of, for example, 80% means that 80% of the\n",
    "    scores in `a` are below the given score. In the case of gaps or\n",
    "    ties, the exact definition depends on the optional keyword, `kind`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : array_like\n",
    "        A 1-D array to which `score` is compared.\n",
    "    score : array_like\n",
    "        Scores to compute percentiles for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pcos : float\n",
    "        Percentile-position of score (0-100) relative to `a`.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    numpy.percentile\n",
    "    scipy.stats.scoreatpercentile, scipy.stats.rankdata\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Three-quarters of the given values lie below a given score:\n",
    "\n",
    "    >>> import numpy as np\n",
    "    >>> from scipy import stats\n",
    "    >>> stats.percentileofscore([1, 2, 3, 4], 3)\n",
    "    75.0\n",
    "\n",
    "    With multiple matches, note how the scores of the two matches, 0.6\n",
    "    and 0.8 respectively, are averaged:\n",
    "\n",
    "    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3)\n",
    "    70.0\n",
    "\n",
    "    Only 2/5 values are strictly less than 3:\n",
    "\n",
    "    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='strict')\n",
    "    40.0\n",
    "\n",
    "    But 4/5 values are less than or equal to 3:\n",
    "\n",
    "    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='weak')\n",
    "    80.0\n",
    "\n",
    "    The average between the weak and the strict scores is:\n",
    "\n",
    "    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='mean')\n",
    "    60.0\n",
    "\n",
    "    Score arrays (of any dimensionality) are supported:\n",
    "\n",
    "    >>> stats.percentileofscore([1, 2, 3, 3, 4], [2, 3])\n",
    "    array([40., 70.])\n",
    "\n",
    "    The inputs can be infinite:\n",
    "\n",
    "    >>> stats.percentileofscore([-np.inf, 0, 1, np.inf], [1, 2, np.inf])\n",
    "    array([75., 75., 100.])\n",
    "\n",
    "    If `a` is empty, then the resulting percentiles are all `nan`:\n",
    "\n",
    "    >>> stats.percentileofscore([], [1, 2])\n",
    "    array([nan, nan])\n",
    "    \"\"\"\n",
    "    # a,score = all_values, all_leads1[:,Y,X]\n",
    "    score_out = score.copy()\n",
    "    \n",
    "    good_index = [idx for idx,_ in enumerate(score) if ~np.isnan(_)]\n",
    "    #remove missing values\n",
    "    score2 = score[good_index]\n",
    "    \n",
    "    a = np.asarray(a)\n",
    "    n = len(a)\n",
    "    score2 = np.asarray(score2)\n",
    "    score2 = score2[..., None]\n",
    "    \n",
    "    def count(x):\n",
    "        return np.count_nonzero(x,-1)\n",
    "\n",
    "        # Main computations/logic\n",
    "\n",
    "    left = count(a < score2)\n",
    "    right = count(a <= score2)\n",
    "    plus1 = left < right\n",
    "    perct = (left + right + plus1) * (50.0 / n)\n",
    "\n",
    "    #Now add back to original score\n",
    "    score[good_index] = perct\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3872d99a-afe0-4678-8fc1-31f35cd2243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_1_return_info(anomaly_file,julian_file,save_dir,MEM_or_by_model):\n",
    "\n",
    "    percentile_output = anomaly_file.copy(deep=True)\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    if MEM_or_by_model == 'MEM':\n",
    "        RZSM_name = 'soilw_bgrnd_percentiles_MEM'\n",
    "        anomaly_file = anomaly_file.mean(dim='M')\n",
    "        julian_file = julian_file.mean(dim='M')\n",
    "        percentile_output = percentile_output.mean(dim='M')\n",
    "    else:\n",
    "        RZSM_name = 'soilw_bgrnd_percentiles'\n",
    "\n",
    "    #first find out what the unique dates in the file\n",
    "    all_dates = anomaly_file.S.values\n",
    "    \n",
    "    month_day = sorted([f'{pd.to_datetime(i).month:02}-{pd.to_datetime(i).day:02}' for i in all_dates])\n",
    "    final_dates = create_final_day_list(month_day)\n",
    "    final_dates.reverse()\n",
    "    \n",
    "    #run through several days at a time because it's a slow process to keep re-loading the same file when they are only 1 julian day apart\n",
    "    total_days = len(final_dates)\n",
    "\n",
    "    return(total_days,final_dates, RZSM_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "917834b5-0690-4dd2-9417-e950da20b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_1_return_info_UNET_experiment(anomaly_file,julian_file,save_dir,MEM_or_by_model):\n",
    "\n",
    "    percentile_output = anomaly_file.copy(deep=True)\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    if MEM_or_by_model == 'MEM':\n",
    "        RZSM_name = 'soilw_bgrnd_percentiles_MEM'\n",
    "        anomaly_file = anomaly_file.mean(dim='M')\n",
    "        percentile_output = percentile_output.mean(dim='M')\n",
    "    else:\n",
    "        RZSM_name = 'soilw_bgrnd_percentiles'\n",
    "\n",
    "    #first find out what the unique dates in the file\n",
    "    all_dates = anomaly_file.S.values\n",
    "    \n",
    "    month_day = sorted([f'{pd.to_datetime(i).month:02}-{pd.to_datetime(i).day:02}' for i in all_dates])\n",
    "    final_dates = create_final_day_list(month_day)\n",
    "    final_dates.reverse()\n",
    "    \n",
    "    #run through several days at a time because it's a slow process to keep re-loading the same file when they are only 1 julian day apart\n",
    "    total_days = len(final_dates)\n",
    "\n",
    "    return(total_days,final_dates, RZSM_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "87c79ed4-2299-4810-a21a-7e571b7646aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_selected_window_to_save_memory(non_completed_dates):\n",
    "    #first get the first date to only have to load all the files once\n",
    "    dates_init = non_completed_dates[0].split('-')\n",
    "    month_init = int(dates_init[0])\n",
    "    day_init = int(dates_init[1])\n",
    "\n",
    "    #Select the anomaly file dates just to have it to access later dates\n",
    "    run_dates = anomaly_file.sel(S=(anomaly_file['S.month'] == month_init) & (anomaly_file['S.day'] == day_init))\n",
    "    saved_dates1 = run_dates.S.values\n",
    "\n",
    "    #first choose a single date for the window\n",
    "    single_date = saved_dates1[0]\n",
    "\n",
    "    #Grab the distribution (getting even days 15 after)\n",
    "    selected_window = window_select_reforecast_by_realization(file=julian_file, time=single_date,window=window,add_window=day_to_grab)\n",
    "    selected_window = selected_window.load()\n",
    "    return(selected_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e73ed38b-19a8-49d1-a67f-6a1e51975752",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def return_selected_window_to_save_memory_ensemble_mean(non_completed_dates, anomaly_file, julian_file):\n",
    "    #first get the first date to only have to load all the files once\n",
    "    dates_init = non_completed_dates[0].split('-')\n",
    "    month_init = int(dates_init[0])\n",
    "    day_init = int(dates_init[1])\n",
    "\n",
    "    #Select the anomaly file dates just to have it to access later dates\n",
    "    run_dates = anomaly_file.sel(S=(anomaly_file['S.month'] == month_init) & (anomaly_file['S.day'] == day_init))\n",
    "    saved_dates1 = run_dates.S.values\n",
    "\n",
    "    #first choose a single date for the window\n",
    "    single_date = saved_dates1[0]\n",
    "\n",
    "    #Grab the distribution (getting even days 15 after)\n",
    "    selected_window2 = window_select_reforecast_by_realization(file=julian_file, time=single_date,window=window,add_window=day_to_grab)\n",
    "    selected_window2 = selected_window2.load()\n",
    "    return(selected_window2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "21639811-8cfc-4e7a-aea7-dc6a880e3b25",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Now loop through all the non-completed days\n",
    "def return_inputs(date_to_run,selected_window,anomaly_file):\n",
    "    dattt = date_to_run.split('-')\n",
    "    m = int(dattt[0])\n",
    "    d = int(dattt[1])\n",
    "\n",
    "    selected_window = xr.where(selected_window==0,np.nan,selected_window)\n",
    "    run_dates_final = anomaly_file.sel(S=(anomaly_file['S.month'] == m) & (anomaly_file['S.day'] == d))\n",
    "    run_dates_final = xr.where(run_dates_final == 0,np.nan,run_dates_final)\n",
    "    #Create the actual names of the files\n",
    "    saved_dates1 = run_dates_final.S.values\n",
    "    saved_dates2 = [f'{RZSM_name}_{pd.to_datetime(i).year}-{pd.to_datetime(i).month:02}-{pd.to_datetime(i).day:02}.nc' for i in saved_dates1]\n",
    "\n",
    "    print(f'Starting percentiles on:')\n",
    "    print(saved_dates2)\n",
    "    #Now get the julian dates to properly subset\n",
    "    julian_date_before,julian_date_after = return_julian_before_after(saved_dates1[0])\n",
    "\n",
    "    #grab subset of julian dates\n",
    "    if julian_date_before < julian_date_after:\n",
    "        julian_date_before_subset = selected_window.sel(L=(selected_window['L'] > julian_date_before) & (selected_window['L'] < julian_date_after))\n",
    "    else:\n",
    "        julian_date_before_subset = selected_window.sel(L=(selected_window['L'] > julian_date_before) | (selected_window['L'] < julian_date_after))\n",
    "\n",
    "    julian_date_before_subset = julian_date_before_subset.load()\n",
    "    return(julian_date_before_subset,selected_window,saved_dates2,saved_dates1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "42635750-6c81-465d-82e4-fe5d8e98b76c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def stack_models(anomaly_file):\n",
    "    stack = anomaly_file.stack(init_mod_lead = ['S','M','L'])\n",
    "        \n",
    "    all_leads1 = stack.RZSM.values\n",
    "    all_leads1 = np.where(all_leads1 == 0,np.nan,all_leads1)\n",
    "    all_leads1.shape #(48, 96, 57365) #This data is the anomaly files which we need to rank against the other distribution\n",
    "\n",
    "    return(stack,all_leads1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eadb14e9-eab9-4eaf-b109-5825f50db7fa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def stack_models_ensemble_mean(anom):\n",
    "    stack = anom.stack(init_mod_lead = ['S','L'])\n",
    "        \n",
    "    all_leads1 = stack.RZSM.values\n",
    "    all_leads1 = np.where(all_leads1 == 0,np.nan,all_leads1)\n",
    "    all_leads1.shape #(48, 96, 57365) #This data is the anomaly files which we need to rank against the other distribution\n",
    "\n",
    "    return(stack,all_leads1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d8552704-6abc-4723-8184-d3782a09ceb4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def pos_func(all_values1, all_leads1, mask_anom,out_array):\n",
    "    # X,Y=10,10\n",
    "    # print(\"all_values1 shape:\", all_values1.shape)\n",
    "    # print(\"all_leads1 shape:\", all_leads1.shape)\n",
    "    # print(\"mask_anom shape:\", mask_anom.shape)\n",
    "    # print(\"out_array shape:\", out_array.shape)\n",
    "    # assert all_values1.shape[3:5] == mask_anom.shape, \"Dimension mismatch between all_values1 and mask_anom\"\n",
    "    # assert all_leads1.shape[0:2] == mask_anom.shape, \"Dimension mismatch between all_leads1 and mask_anom\"\n",
    "    # assert out_array.shape[0:2] == mask_anom.shape, \"Dimension mismatch between out_array and mask_anom\"\n",
    "    \n",
    "    for Y in range(all_leads1.shape[0]):\n",
    "        for X in range(all_leads1.shape[1]):\n",
    "            if mask_anom[Y,X] == 1:\n",
    "                # print(f'Working on X: {X} and Y: {Y}')\n",
    "                a = all_values1[:, :, :, Y, X].flatten() #(1021097,)\n",
    "                a = a.flatten()\n",
    "                a = a[~np.isnan(a)]\n",
    "                a = a[a != 0]\n",
    "                a.shape\n",
    "                \n",
    "                score = all_leads1[Y, X, :] #(57365,)\n",
    "                good_index = ~np.isnan(score)\n",
    "                score = score[good_index]\n",
    "                # print(\"score shape:\", score.shape)\n",
    "                n = a.size #(1021097,)\n",
    "                score = score[..., None] #(57365, 1)\n",
    "                \n",
    "                def count(x):\n",
    "                    return np.count_nonzero(x, axis=-1)\n",
    "    \n",
    "                # print('Right before division')\n",
    "                # print(\"a:\", a.shape)\n",
    "                # print(\"score shape:\", score.shape)\n",
    "                div = a < score\n",
    "                # print(\"div shape:\", div.shape)\n",
    "                count_ = np.count_nonzero(div,axis=-1)\n",
    "                \n",
    "                # perct = count() * (100.0 / n) #Strict\n",
    "                # print('Right before the ranking')\n",
    "                \n",
    "                perct = count_ * (100.0 / n) #weak\n",
    "                out_array[Y,X,good_index] = perct\n",
    "                \n",
    "    print(\"Finished processing\")\n",
    "    return(out_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7e022fc6-5757-4d81-abfb-452fe019974c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def pos_func_ensemble_mean(all_values1, all_leads1, mask_anom,out_array):\n",
    "    # X,Y=10,10\n",
    "    # print(\"all_values1 shape:\", all_values1.shape)\n",
    "    # print(\"all_leads1 shape:\", all_leads1.shape)\n",
    "    # print(\"mask_anom shape:\", mask_anom.shape)\n",
    "    # print(\"out_array shape:\", out_array.shape)\n",
    "    # assert all_values1.shape[3:5] == mask_anom.shape, \"Dimension mismatch between all_values1 and mask_anom\"\n",
    "    # assert all_leads1.shape[0:2] == mask_anom.shape, \"Dimension mismatch between all_leads1 and mask_anom\"\n",
    "    # assert out_array.shape[0:2] == mask_anom.shape, \"Dimension mismatch between out_array and mask_anom\"\n",
    "    \n",
    "    for Y in range(all_leads1.shape[0]):\n",
    "        for X in range(all_leads1.shape[1]):\n",
    "            if mask_anom[Y,X] == 1:\n",
    "                # print(f'Working on X: {X} and Y: {Y}')\n",
    "                a = all_values1[:, :, Y, X].flatten() #(1021097,)\n",
    "                a = a.flatten()\n",
    "                a = a[~np.isnan(a)]\n",
    "                a = a[a != 0]\n",
    "                a.shape\n",
    "                \n",
    "                score = all_leads1[Y, X, :] #(57365,)\n",
    "                good_index = ~np.isnan(score)\n",
    "                score = score[good_index]\n",
    "                # print(\"score shape:\", score.shape)\n",
    "                n = a.size #(1021097,)\n",
    "                score = score[..., None] #(57365, 1)\n",
    "                \n",
    "                def count(x):\n",
    "                    return np.count_nonzero(x, axis=-1)\n",
    "    \n",
    "                # print('Right before division')\n",
    "                # print(\"a:\", a.shape)\n",
    "                # print(\"score shape:\", score.shape)\n",
    "                div = a < score\n",
    "                # print(\"div shape:\", div.shape)\n",
    "                count_ = np.count_nonzero(div,axis=-1)\n",
    "                \n",
    "                # perct = count() * (100.0 / n) #Strict\n",
    "                # print('Right before the ranking')\n",
    "\n",
    "                try:\n",
    "                    perct = count_ * (100.0 / n) #weak\n",
    "                except ZeroDivisionError:\n",
    "                    perct = np.nan\n",
    "                    \n",
    "                out_array[Y,X,good_index] = perct\n",
    "                \n",
    "    print(\"Finished processing\")\n",
    "    return(out_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "64051e5b-48de-42f0-86f0-424b2d1798c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def rank_data_func(all_values1, all_leads1, mask_anom,out_array):\n",
    "    # X,Y=10,10\n",
    "    # print(\"all_values1 shape:\", all_values1.shape)\n",
    "    # print(\"all_leads1 shape:\", all_leads1.shape)\n",
    "    # print(\"mask_anom shape:\", mask_anom.shape)\n",
    "    # print(\"out_array shape:\", out_array.shape)\n",
    "    # assert all_values1.shape[3:5] == mask_anom.shape, \"Dimension mismatch between all_values1 and mask_anom\"\n",
    "    # assert all_leads1.shape[0:2] == mask_anom.shape, \"Dimension mismatch between all_leads1 and mask_anom\"\n",
    "    # assert out_array.shape[0:2] == mask_anom.shape, \"Dimension mismatch between out_array and mask_anom\"\n",
    "    \n",
    "    for Y in range(all_leads1.shape[0]):\n",
    "        for X in range(all_leads1.shape[1]):\n",
    "            if mask_anom[Y,X] == 1:\n",
    "                # print(f'Working on X: {X} and Y: {Y}')\n",
    "                a = all_values1[:, :, :, Y, X].flatten() #(1021097,)\n",
    "                a = a.flatten()\n",
    "                a = a[~np.isnan(a)]\n",
    "                a = a[a != 0]\n",
    "                a.shape\n",
    "                \n",
    "                score = all_leads1[Y, X, :] #(57365,)\n",
    "                good_index = ~np.isnan(score)\n",
    "                score = score[good_index]\n",
    "                # print(\"score shape:\", score.shape)\n",
    "                n = a.size #(1021097,)\n",
    "                score = score[..., None] #(57365, 1)\n",
    "                \n",
    "                def count(x):\n",
    "                    return np.count_nonzero(x, axis=-1)\n",
    "    \n",
    "                # print('Right before division')\n",
    "                # print(\"a:\", a.shape)\n",
    "                # print(\"score shape:\", score.shape)\n",
    "                div = a < score\n",
    "                # print(\"div shape:\", div.shape)\n",
    "                count_ = np.count_nonzero(div,axis=-1)\n",
    "                \n",
    "                # perct = count() * (100.0 / n) #Strict\n",
    "                # print('Right before the ranking')\n",
    "                \n",
    "                perct = count_ * (100.0 / n) #weak\n",
    "                out_array[Y,X,good_index] = perct\n",
    "                \n",
    "    print(\"Finished processing\")\n",
    "    return(out_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "485f656d-fd4b-47dc-ae43-69366dd8c3fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Now loop through each of the non_completed dates\n",
    "def run_multi_dates(date_to_run,anomaly_file):\n",
    "    # break\n",
    "    # date_to_run = '12-31'\n",
    "    julian_date_before_subset,selected_window2,saved_dates2,saved_dates1 = return_inputs(date_to_run,selected_window,anomaly_file)\n",
    "    stack,all_leads1 = stack_models(anomaly_file)\n",
    "    all_leads1.shape #(48, 96, 57365)\n",
    "    \n",
    "    all_values1 = julian_date_before_subset.RZSM.values\n",
    "    all_values1.shape # (1043, 11, 89, 48, 96)\n",
    "    \n",
    "    # percentile_arr = pos_func(all_values1, all_leads1,mask_anom)\n",
    "    percentile_arr = np.empty_like(all_leads1)\n",
    "    percentile_arr.shape #(48, 96, 57365)\n",
    "    \n",
    "    percentile_arr = pos_func(all_values1, all_leads1,mask_anom,percentile_arr)\n",
    "    \n",
    "    stack_cp = stack.copy(deep=True)\n",
    "    stack_cp.RZSM[:,:,:] = percentile_arr\n",
    "    unstack = stack_cp.unstack('init_mod_lead').transpose('S','M','L','Y','X')\n",
    "    unstack.RZSM[:,0,0,10,10].values\n",
    "    \n",
    "    for idx,date_run in enumerate(saved_dates1):\n",
    "        unstack.sel(S=date_run).to_netcdf(f'{save_dir}/{saved_dates2[idx]}')\n",
    "    return(f'Completed {date_to_run}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7d3a97a5-9d8a-42af-9c49-5a7f70aa7c07",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_multi_dates_ensemble_mean(date_to_run,anomaly_file,selected_window,save_dir):\n",
    "    # break\n",
    "    # date_to_run = '01-01'\n",
    "    julian_date_before_subset,selected_window2,saved_dates2,saved_dates1 = return_inputs(date_to_run,selected_window,anomaly_file)\n",
    "    stack,all_leads1 = stack_models_ensemble_mean(anomaly_file)\n",
    "    all_leads1.shape #(48, 96, 5215)\n",
    "    \n",
    "    all_values1 = julian_date_before_subset.RZSM.values\n",
    "    all_values1.shape # (1043, 89, 48, 96)\n",
    "    \n",
    "    # percentile_arr = pos_func(all_values1, all_leads1,mask_anom)\n",
    "    percentile_arr = np.empty_like(all_leads1)\n",
    "    percentile_arr.shape #(48, 96, 5215)\n",
    "    \n",
    "    percentile_arr = pos_func_ensemble_mean(all_values1, all_leads1,mask_anom,percentile_arr)\n",
    "    \n",
    "    stack_cp = stack.copy(deep=True)\n",
    "    stack_cp.RZSM[:,:,:] = percentile_arr\n",
    "    unstack = stack_cp.unstack('init_mod_lead').transpose('S','L','Y','X')\n",
    "    unstack.RZSM[:,0,10,10].values\n",
    "    \n",
    "    for idx,date_run in enumerate(saved_dates1):\n",
    "        unstack.sel(S=date_run).to_netcdf(f'{save_dir}/{saved_dates2[idx]}')\n",
    "    return(f'Completed {date_to_run}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "350556dd-9650-4a06-8a87-9cf8a5d6fee1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_multi_dates_ensemble_mean_UNET_EXPERIMENT(date_to_run,anomaly_file,selected_window,save_dir):\n",
    "    # break\n",
    "    # date_to_run = '12-31'\n",
    "    anom = anomaly_file.mean(dim='M')\n",
    "    \n",
    "    julian_date_before_subset,selected_window2,saved_dates2,saved_dates1 = return_inputs(date_to_run,selected_window,anomaly_file)\n",
    "    stack,all_leads1 = stack_models_ensemble_mean(anom)\n",
    "    all_leads1.shape #(48, 96, 5215)\n",
    "    \n",
    "    all_values1 = julian_date_before_subset.RZSM.values\n",
    "    all_values1.shape # (1043, 89, 48, 96)\n",
    "    \n",
    "    # percentile_arr = pos_func(all_values1, all_leads1,mask_anom)\n",
    "    percentile_arr = np.empty_like(all_leads1)\n",
    "    percentile_arr.shape #(48, 96, 5215)\n",
    "    \n",
    "    percentile_arr = pos_func_ensemble_mean(all_values1, all_leads1,mask_anom,percentile_arr)\n",
    "    \n",
    "    stack_cp = stack.copy(deep=True)\n",
    "    stack_cp.RZSM[:,:,:] = percentile_arr\n",
    "    unstack = stack_cp.unstack('init_mod_lead').transpose('S','L','Y','X')\n",
    "    unstack.RZSM[:,0,10,10].values\n",
    "    \n",
    "    for idx,date_run in enumerate(saved_dates1):\n",
    "        unstack.sel(S=date_run).to_netcdf(f'{save_dir}/{saved_dates2[idx]}')\n",
    "    return(f'Completed {date_to_run}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524461d3-935c-4f38-98e4-d8632630758d",
   "metadata": {},
   "source": [
    "# GEFSv12 ensemble mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "aa2120d9-f3d8-42bd-98b3-5e75113369dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_GEFSv12_ensemble_mean(region_name):\n",
    "    # GEFSv12\n",
    "    # template\n",
    "    julian_file = per.create_data_julian_dates_GEFS_testing_distribution(template,region_name).mean(dim='M')\n",
    "    \n",
    "    anomaly_file = gefs_bias_corrected.mean(dim='M')\n",
    "    save_dir=f'{conf.gefsv12_data}/{region_name}/soilw_bgrnd/percentiles_MEM_testing_distribution'\n",
    "    MEM_or_by_model='model'\n",
    "\n",
    "    global RZSM_name\n",
    "    total_days, final_dates, RZSM_name = step_1_return_info(anomaly_file,julian_file,save_dir,MEM_or_by_model)\n",
    "    num_cpus_available = numba.config.NUMBA_NUM_THREADS\n",
    "    print(\"Number of CPUs available for Numba:\", num_cpus_available)\n",
    "    \n",
    "    \n",
    "    for date_index in range(0, total_days, day_to_grab):\n",
    "        # date_index=0\n",
    "        # break\n",
    "        non_completed_dates = per.get_non_completed_days(final_dates,date_index,day_to_grab, anomaly_file, RZSM_name, save_dir)\n",
    "        print(f'Non completed dates:')\n",
    "        print(non_completed_dates) \n",
    "        \n",
    "        #Run function\n",
    "        if len(non_completed_dates) > 0 :\n",
    "            # global selected_window\n",
    "            selected_window = return_selected_window_to_save_memory_ensemble_mean(non_completed_dates,anomaly_file,julian_file)                \n",
    "    \n",
    "            # p=Pool(n_processes)\n",
    "            # p.map(run_multi_dates_ensemble_mean,non_completed_dates)\n",
    "    \n",
    "            for date_to_run in non_completed_dates:\n",
    "                run_multi_dates_ensemble_mean(date_to_run,anomaly_file,selected_window,save_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f1381202-2066-4157-b150-567d4db92c94",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the julian day anomaly dataset\n",
      "Number of CPUs available for Numba: 1\n",
      "Non completed dates:\n",
      "['12-26', '12-25', '12-19', '12-18', '12-12', '12-11', '12-05', '12-04', '11-28', '11-27']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-12-26.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-12-25.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-12-19.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-12-18.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-12-12.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-12-11.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-12-05.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-12-04.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-11-28.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-11-27.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['11-21', '11-20', '11-14', '11-13', '11-07', '11-06', '10-31', '10-30', '10-24', '10-23']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-11-21.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-11-20.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-11-14.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-11-13.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-11-07.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-11-06.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-10-31.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-10-30.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-10-24.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-10-23.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['10-17', '10-16', '10-10', '10-09', '10-03', '10-02', '09-26', '09-25', '09-19', '09-18']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-10-17.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-10-16.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-10-10.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-10-09.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-10-03.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-10-02.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-09-26.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-09-25.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-09-19.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-09-18.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['09-12', '09-11', '09-05', '09-04', '08-29', '08-28', '08-22', '08-21', '08-15', '08-14']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-09-12.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-09-11.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-09-05.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-09-04.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-08-29.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-08-28.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-08-22.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-08-21.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-08-15.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-08-14.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['08-08', '08-07', '08-01', '07-31', '07-25', '07-24', '07-18', '07-17', '07-11', '07-10']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-08-08.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-08-07.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-08-01.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-07-31.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-07-25.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-07-24.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-07-18.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-07-17.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-07-11.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-07-10.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['07-04', '07-03', '06-27', '06-26', '06-20', '06-19', '06-13', '06-12', '06-06', '06-05']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-07-04.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-07-03.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-06-27.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-06-26.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-06-20.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-06-19.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-06-13.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-06-12.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-06-06.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-06-05.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['05-30', '05-29', '05-23', '05-22', '05-16', '05-15', '05-09', '05-08', '05-02', '05-01']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-05-30.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-05-29.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-05-23.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-05-22.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-05-16.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-05-15.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-05-09.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-05-08.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-05-02.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-05-01.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['04-25', '04-24', '04-18', '04-17', '04-11', '04-10', '04-04', '04-03', '03-28', '03-27']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-04-25.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-04-24.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-04-18.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-04-17.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-04-11.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-04-10.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-04-04.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-04-03.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-03-28.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-03-27.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['03-21', '03-20', '03-14', '03-13', '03-07', '03-06', '02-28', '02-27', '02-21', '02-20']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-03-21.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-03-20.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-03-14.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-03-13.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-03-07.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-03-06.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-02-28.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-02-27.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-02-21.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-02-20.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['02-14', '02-13', '02-07', '02-06', '01-31', '01-30', '01-24', '01-23', '01-17', '01-16']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-02-14.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-02-13.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-02-07.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-02-06.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-01-31.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-01-30.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-01-24.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-01-23.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-01-17.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-01-16.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['01-10', '01-09', '01-03', '01-02']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-01-10.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-01-09.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-01-03.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-01-02.nc']\n",
      "Finished processing\n"
     ]
    }
   ],
   "source": [
    "run_GEFSv12_ensemble_mean(region_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a412a491-ae7d-479e-8ff0-01bf70bad5ca",
   "metadata": {},
   "source": [
    "# UNET experiment ensemble mean create percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8c093272-2f7b-4387-99f6-8a2060f554e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the julian day anomaly dataset\n",
      "Number of CPUs available for Numba: 1\n",
      "Non completed dates:\n",
      "['12-26', '12-25', '12-19', '12-18', '12-12', '12-11', '12-05', '12-04', '11-28', '11-27']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-12-26.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-12-25.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-12-19.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-12-18.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-12-12.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-12-11.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-12-05.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-12-04.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-11-28.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-11-27.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['11-21', '11-20', '11-14', '11-13', '11-07', '11-06', '10-31', '10-30', '10-24', '10-23']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-11-21.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-11-20.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-11-14.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-11-13.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-11-07.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-11-06.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-10-31.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-10-30.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-10-24.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-10-23.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['10-17', '10-16', '10-10', '10-09', '10-03', '10-02', '09-26', '09-25', '09-19', '09-18']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-10-17.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-10-16.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-10-10.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-10-09.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-10-03.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-10-02.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-09-26.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-09-25.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-09-19.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-09-18.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['09-12', '09-11', '09-05', '09-04', '08-29', '08-28', '08-22', '08-21', '08-15', '08-14']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-09-12.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-09-11.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-09-05.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-09-04.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-08-29.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-08-28.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-08-22.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-08-21.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-08-15.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-08-14.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['08-08', '08-07', '08-01', '07-31', '07-25', '07-24', '07-18', '07-17', '07-11', '07-10']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-08-08.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-08-07.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-08-01.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-07-31.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-07-25.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-07-24.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-07-18.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-07-17.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-07-11.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-07-10.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['07-04', '07-03', '06-27', '06-26', '06-20', '06-19', '06-13', '06-12', '06-06', '06-05']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-07-04.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-07-03.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-06-27.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-06-26.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-06-20.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-06-19.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-06-13.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-06-12.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-06-06.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-06-05.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['05-30', '05-29', '05-23', '05-22', '05-16', '05-15', '05-09', '05-08', '05-02', '05-01']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-05-30.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-05-29.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-05-23.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-05-22.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-05-16.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-05-15.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-05-09.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-05-08.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-05-02.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-05-01.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['04-25', '04-24', '04-18', '04-17', '04-11', '04-10', '04-04', '04-03', '03-28', '03-27']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-04-25.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-04-24.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-04-18.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-04-17.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-04-11.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-04-10.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-04-04.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-04-03.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-03-28.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-03-27.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['03-21', '03-20', '03-14', '03-13', '03-07', '03-06', '02-28', '02-27', '02-21', '02-20']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-03-21.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-03-20.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-03-14.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-03-13.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-03-07.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-03-06.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-02-28.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-02-27.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-02-21.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-02-20.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['02-14', '02-13', '02-07', '02-06', '01-31', '01-30', '01-24', '01-23', '01-17', '01-16']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-02-14.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-02-13.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-02-07.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-02-06.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-01-31.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-01-30.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-01-24.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-01-23.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-01-17.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-01-16.nc']\n",
      "Finished processing\n",
      "Non completed dates:\n",
      "['01-10', '01-09', '01-03', '01-02']\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-01-10.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-01-09.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2018-01-03.nc']\n",
      "Finished processing\n",
      "Starting percentiles on:\n",
      "['soilw_bgrnd_percentiles_2019-01-02.nc']\n",
      "Finished processing\n"
     ]
    }
   ],
   "source": [
    "anomaly_file = create_reforecast_with_predictions(experiment_name = 'EX29_regular_RZSM').load()\n",
    "julian_file = per.create_data_julian_dates_UNET_experiment_testing_distribution(anomaly_file, 'EX29', region_name).mean(dim='M').load()\n",
    "\n",
    "save_dir=f'predictions/{region_name}/UNET_percentiles_MEM_testing_distribution/EX29'\n",
    "MEM_or_by_model='model'\n",
    "\n",
    "total_days, final_dates, RZSM_name = step_1_return_info_UNET_experiment(anomaly_file,julian_file,save_dir,MEM_or_by_model)\n",
    "num_cpus_available = numba.config.NUMBA_NUM_THREADS\n",
    "print(\"Number of CPUs available for Numba:\", num_cpus_available)\n",
    "\n",
    "\n",
    "for date_index in range(0, total_days, day_to_grab):\n",
    "    # date_index=0\n",
    "    # break\n",
    "    non_completed_dates = per.get_non_completed_days(final_dates,date_index,day_to_grab, anomaly_file, RZSM_name, save_dir)\n",
    "    print(f'Non completed dates:')\n",
    "    print(non_completed_dates) \n",
    "    \n",
    "    #Run function\n",
    "    if len(non_completed_dates) > 0:\n",
    "        # global selected_window\n",
    "        selected_window = return_selected_window_to_save_memory_ensemble_mean(non_completed_dates,anomaly_file,julian_file)                \n",
    "\n",
    "        # p=Pool(n_processes)\n",
    "        # p.map(run_multi_dates_ensemble_mean_UNET_EXPERIMENT,non_completed_dates)\n",
    "\n",
    "        for date_to_run in non_completed_dates:\n",
    "            run_multi_dates_ensemble_mean_UNET_EXPERIMENT(date_to_run, anomaly_file,selected_window,save_dir)\n",
    "\n",
    "# stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb5637-6ddc-4127-bf8c-9be72aa0244a",
   "metadata": {},
   "source": [
    "# GEFSv12 indivdual forecast realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7d8a4-2a7f-469e-b1ad-5b77bb120b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GEFSv12\n",
    "# template\n",
    "# reforecast_anomaly_julian = per.create_data_julian_dates_GEFS(template)\n",
    "\n",
    "# anomaly_file = template\n",
    "# julian_file = reforecast_anomaly_julian\n",
    "# save_dir=f'Data/GEFSv12_reforecast/soilw_bgrnd/percentiles'\n",
    "# MEM_or_by_model='model'\n",
    "\n",
    "\n",
    "# total_days, final_dates, RZSM_name = step_1_return_info(anomaly_file,julian_file,save_dir,MEM_or_by_model)\n",
    "# num_cpus_available = numba.config.NUMBA_NUM_THREADS\n",
    "# print(\"Number of CPUs available for Numba:\", num_cpus_available)\n",
    "\n",
    "\n",
    "# for date_index in range(0, total_days, day_to_grab):\n",
    "#     # date_index=0\n",
    "#     # break\n",
    "#     non_completed_dates = per.get_non_completed_days(final_dates,date_index,day_to_grab, anomaly_file, RZSM_name, save_dir)\n",
    "#     print(f'Non completed dates:')\n",
    "#     print(non_completed_dates) \n",
    "    \n",
    "#     #Run function\n",
    "#     if len(non_completed_dates) > 0 :\n",
    "#         global selected_window\n",
    "#         selected_window = return_selected_window_to_save_memory(non_completed_dates)\n",
    "        \n",
    "#         p=Pool(n_processes)\n",
    "#         p.map(run_multi_dates,non_completed_dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdccfb1-cfd4-46f5-a5e6-a479d68d251c",
   "metadata": {},
   "source": [
    "# ECMWF ensemble mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "37ecb567-5435-4299-89cf-96924a0cec10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the ECMWF julian day anomaly dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'template_ECM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m ecmwf_bias_corrected\n\u001b[1;32m      3\u001b[0m julian_file \u001b[38;5;241m=\u001b[39m create_data_julian_dates_ECMWF(ecmwf_bias_corrected)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m anomaly_file \u001b[38;5;241m=\u001b[39m \u001b[43mtemplate_ECM\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconf\u001b[38;5;241m.\u001b[39mecmwf_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/soilw_bgrnd/percentiles_MEM_testing_distribution\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m MEM_or_by_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'template_ECM' is not defined"
     ]
    }
   ],
   "source": [
    "# ECMWF\n",
    "ecmwf_bias_corrected\n",
    "julian_file = create_data_julian_dates_ECMWF(ecmwf_bias_corrected).mean(dim='M')\n",
    "\n",
    "anomaly_file = ecmwf_bias_corrected.mean(dim='M')\n",
    "save_dir=f'{conf.ecmwf_data}/{region_name}/soilw_bgrnd/percentiles_MEM_testing_distribution'\n",
    "MEM_or_by_model='model'\n",
    "\n",
    "\n",
    "total_days, final_dates, RZSM_name = step_1_return_info(anomaly_file,julian_file,save_dir,MEM_or_by_model)\n",
    "num_cpus_available = numba.config.NUMBA_NUM_THREADS\n",
    "print(\"Number of CPUs available for Numba:\", num_cpus_available)\n",
    "\n",
    "days_to_grab = n_processes #this should help with not needing so much memory\n",
    "\n",
    "for date_index in range(0, total_days, day_to_grab):\n",
    "    # date_index=0\n",
    "    # break\n",
    "    non_completed_dates = per.get_non_completed_days(final_dates,date_index,day_to_grab, anomaly_file, RZSM_name, save_dir)\n",
    "    print(f'Non completed dates:')\n",
    "    print(non_completed_dates) \n",
    "    \n",
    "    #Run function\n",
    "    if len(non_completed_dates) > 0 :\n",
    "        # global selected_window\n",
    "        selected_window = return_selected_window_to_save_memory_ensemble_mean(non_completed_dates,anomaly_file,julian_file)\n",
    "\n",
    "        #Multi-process (split selected window in next function)\n",
    "        # p=Pool(n_processes)\n",
    "        # p.map(run_multi_dates_ensemble_mean,non_completed_dates)\n",
    "\n",
    "        for date_to_run in non_completed_dates:\n",
    "            run_multi_dates_ensemble_mean(date_to_run,anomaly_file,selected_window,save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd531d7-8434-4f7d-8fd6-85939812c05d",
   "metadata": {},
   "source": [
    "# ECMWF individual realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de15107-b09e-45e2-bd41-1da3b2a13ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ECMWF\n",
    "# template_ECM\n",
    "# reforecast_anomaly_julian = create_data_julian_dates_ECMWF(template_ECM)\n",
    "\n",
    "# anomaly_file = template_ECM\n",
    "# julian_file = reforecast_anomaly_julian\n",
    "# save_dir=f'Data/ECMWF/soilw_bgrnd_processed/CONUS/percentiles'\n",
    "# MEM_or_by_model='model'\n",
    "\n",
    "\n",
    "# total_days, final_dates, RZSM_name = step_1_return_info(anomaly_file,julian_file,save_dir,MEM_or_by_model)\n",
    "# num_cpus_available = numba.config.NUMBA_NUM_THREADS\n",
    "# print(\"Number of CPUs available for Numba:\", num_cpus_available)\n",
    "\n",
    "# days_to_grab = n_processes #this should help with not needing so much memory\n",
    "\n",
    "# for date_index in range(0, total_days, day_to_grab):\n",
    "#     # date_index=0\n",
    "#     # break\n",
    "#     non_completed_dates = per.get_non_completed_days(final_dates,date_index,day_to_grab, anomaly_file, RZSM_name, save_dir)\n",
    "#     print(f'Non completed dates:')\n",
    "#     print(non_completed_dates) \n",
    "    \n",
    "#     #Run function\n",
    "#     if len(non_completed_dates) > 0 :\n",
    "#         global selected_window\n",
    "#         selected_window = return_selected_window_to_save_memory_ensemble_mean(non_completed_dates)\n",
    "\n",
    "#         #Multi-process (split selected window in next function)\n",
    "#         p=Pool(n_processes)\n",
    "#         p.map(run_multi_dates,non_completed_dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc02fe6-44e6-4701-91ab-1ab3342c1943",
   "metadata": {},
   "source": [
    "# Testing code beneath this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082b20d-57a9-45e3-9c4e-e0b4488a4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the UNET prediction dataset to do other functions\n",
    "# experiment_list = ['EX10_RZSM','EX20_RZSM','EX24_regular_RZSM','EX20_RZSM','EX20_RZSM','EX26_RZSM'] #Best model outputs (multi-week)\n",
    "\n",
    "#Set up experiment list as week lead: experiment name\n",
    "experiment_list = {'3':'EX24_regular_RZSM'} #Best model outputs (single_week)\n",
    "\n",
    "\n",
    "create_data_julian_dates(experiment_name=experiment_name)\n",
    "\n",
    "stop\n",
    "\n",
    "#Load the new experiment anomalies that have been reformatted\n",
    "prediction_reforecast_anomaly = xr.open_mfdataset(f'predictions/no_julian_dates_bias_corrected/{experiment_name}*.nc').load()\n",
    "reforecast_anomaly_julian = xr.open_mfdataset(f'predictions/experiment_RZSM_julian_dates_bias_corrected/RZSM_{experiment_name}*.nc',combine='nested',concat_dim=['S'])\n",
    "percentile_output = prediction_reforecast_anomaly.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad59cf5-24a4-4caf-9c9d-ffd906aef296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create model realization percentiles 1.) For each individual model\n",
    "experiment_name = 'EX29'\n",
    "percentile_of_score_by_realization(anomaly_file=prediction_reforecast_anomaly, julian_file=reforecast_anomaly_julian,\n",
    "                                   percentile_output=percentile_output,save_dir=f'predictions/UNET/{region_name}/percentiles/{experiment_name}',MEM_or_by_model='model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d9326-faf4-4ec8-8090-5b43d334fb01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create model ensemble mean percentiles (MEM)\n",
    "percentile_of_score_by_realization(anomaly_file=prediction_reforecast_anomaly, julian_file=reforecast_anomaly_julian,\n",
    "                                   percentile_output=percentile_output,save_dir=f'predictions/UNET/percentiles/{experiment_name}',MEM_or_by_model='MEM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884d8c0-b14d-440d-92d9-eb12bdd53864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_of_score_by_realization(anomaly_file, julian_file, percentile_output,save_dir,MEM_or_by_model):\n",
    "     \n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    if MEM_or_by_model == 'MEM':\n",
    "        RZSM_name = 'RZSM_percentiles_MEM'\n",
    "        anomaly_file = anomaly_file.mean(dim='M')\n",
    "        julian_file = julian_file.mean(dim='M')\n",
    "        percentile_output = percentile_output.mean(dim='M')\n",
    "    else:\n",
    "        RZSM_name = 'RZSM_percentiles'\n",
    "    \n",
    "\n",
    "    #Test \n",
    "    # anomaly_file = base_reforecast_anomaly\n",
    "    # julian_file = base_reforecast_anomaly_julian\n",
    "    # save_dir = 'Data/GEFSv12_reforecast/soilw_bgrnd/percentiles_baseline'\n",
    "    \n",
    "    #first find out what the unique dates in the file\n",
    "    all_dates = anomaly_file.S.values\n",
    "    \n",
    "    month_day = sorted([f'{pd.to_datetime(i).month:02}-{pd.to_datetime(i).day:02}' for i in all_dates])\n",
    "    final_dates = create_final_day_list(month_day)\n",
    "    final_dates.reverse()\n",
    "    \n",
    "    #run through several days at a time because it's a slow process to keep re-loading the same file when they are only 1 julian day apart\n",
    "    total_days = len(final_dates)\n",
    "    day_to_grab = 15\n",
    "    \n",
    "    for date_index in range(0, total_days, day_to_grab):\n",
    "        # break\n",
    "        selected_days = final_dates[date_index:date_index + day_to_grab]\n",
    "        dates_init = selected_days[0].split('-')\n",
    "        month_init = int(dates_init[0])\n",
    "        day_init = int(dates_init[1])\n",
    "        \n",
    "        non_completed_dates = []\n",
    "\n",
    "        \n",
    "        for j in selected_days:\n",
    "            dates =j.split('-')\n",
    "            month = int(dates[0])\n",
    "            day = int(dates[1])\n",
    "            # break\n",
    "            \n",
    "            #Now loop through each of the month_days and create the distribution\n",
    "            run_dates = anomaly_file.sel(S=(anomaly_file['S.month'] == month) & (anomaly_file['S.day'] == day ))\n",
    "\n",
    "            #Now check if files exists (don't run otherwise)\n",
    "            saved_dates1 = run_dates.S.values\n",
    "            saved_dates2 = [f'{RZSM_name}_{pd.to_datetime(i).year}-{pd.to_datetime(i).month:02}-{pd.to_datetime(i).day:02}.nc' for i in saved_dates1]\n",
    "\n",
    "            completed_or_not = []\n",
    "            for i in saved_dates2:\n",
    "                if os.path.exists(f'{save_dir}/{i}'):\n",
    "                    completed_or_not.append(True)\n",
    "                else:\n",
    "                    completed_or_not.append(False)\n",
    "        \n",
    "            if len(completed_or_not) == sum(completed_or_not):\n",
    "                #All files are already completed\n",
    "                pass\n",
    "            else:\n",
    "                non_completed_dates.append(j)\n",
    "                \n",
    "        print(f'Non completed dates:')\n",
    "        print(non_completed_dates) \n",
    "           \n",
    "        #Now loop through all the non-completed days\n",
    "        def run_subset_of_selected_window(date_to_run,selected_window):\n",
    "            # break\n",
    "            dattt = date_to_run.split('-')\n",
    "            m = int(dattt[0])\n",
    "            d = int(dattt[1])\n",
    "\n",
    "            run_dates_final = anomaly_file.sel(S=(anomaly_file['S.month'] == m) & (anomaly_file['S.day'] == d))\n",
    "\n",
    "            #Create the actual names of the files\n",
    "            saved_dates1 = run_dates_final.S.values\n",
    "            saved_dates2 = [f'{RZSM_name}_{pd.to_datetime(i).year}-{pd.to_datetime(i).month:02}-{pd.to_datetime(i).day:02}.nc' for i in saved_dates1]\n",
    "\n",
    "            print(f'Starting percentiles on:')\n",
    "            print(saved_dates2)\n",
    "            #Now get the julian dates to properly subset\n",
    "            julian_date_before,julian_date_after = return_julian_before_after(saved_dates1[0])\n",
    "\n",
    "            #grab subset of julian dates\n",
    "            if julian_date_before < julian_date_after:\n",
    "                julian_date_before_subset = selected_window.sel(L=(selected_window['L'] > julian_date_before) & (selected_window['L'] < julian_date_after))\n",
    "            else:\n",
    "                julian_date_before_subset = selected_window.sel(L=(selected_window['L'] > julian_date_before) | (selected_window['L'] < julian_date_after))\n",
    "\n",
    "            #Now loop through each of the actual files and create the percentile distribution\n",
    "\n",
    "            if MEM_or_by_model == 'model':\n",
    "\n",
    "                for idx,date_run in enumerate(saved_dates1):\n",
    "                    # break\n",
    "                    file_run  = anomaly_file.sel(S=date_run)\n",
    "\n",
    "                    for X in range(anomaly_file.X.shape[0]):\n",
    "                        for Y in range(anomaly_file.Y.shape[0]):\n",
    "                            for model in range(anomaly_file.M.shape[0]):\n",
    "\n",
    "                                all_leads = anomaly_file.isel(X=X,Y=Y,M=model).sel(S=date_run).RZSM.values\n",
    "                                all_values = julian_date_before_subset.isel(X=X,Y=Y,M=model).RZSM.values.flatten()\n",
    "                                all_values = all_values[~np.isnan(all_values)]\n",
    "                                all_values = all_values[all_values != 0]\n",
    "                                #percentile of score\n",
    "                                percentile = pos(all_values, all_leads)\n",
    "\n",
    "                                #Now add back to the dataset\n",
    "                                #get index\n",
    "                                index = int(np.where(anomaly_file['S'] == pd.to_datetime(date_run))[0])\n",
    "                                percentile_output.RZSM[index,model,:, Y, X] = percentile\n",
    "\n",
    "                    percentile_output.sel(S=date_run).to_netcdf(f'{save_dir}/{saved_dates2[idx]}')\n",
    "\n",
    "            elif MEM_or_by_model == 'MEM':\n",
    "                for idx,date_run in enumerate(saved_dates1):\n",
    "                    # break\n",
    "                    file_run  = anomaly_file.sel(S=date_run)\n",
    "\n",
    "                    for X in range(anomaly_file.X.shape[0]):\n",
    "                        for Y in range(anomaly_file.Y.shape[0]):\n",
    "\n",
    "                                all_leads = anomaly_file.isel(X=X,Y=Y).sel(S=date_run).RZSM.values\n",
    "                                all_values = julian_date_before_subset.isel(X=X,Y=Y).RZSM.values.flatten()\n",
    "                                all_values = all_values[~np.isnan(all_values)]\n",
    "                                all_values = all_values[all_values != 0]\n",
    "                                #percentile of score\n",
    "                                percentile = pos(all_values, all_leads)\n",
    "\n",
    "                                #Now add back to the dataset\n",
    "                                #get index\n",
    "                                index = int(np.where(anomaly_file['S'] == pd.to_datetime(date_run))[0])\n",
    "                                percentile_output.RZSM[index,:, Y, X] = percentile\n",
    "\n",
    "                    percentile_output.sel(S=date_run).to_netcdf(f'{save_dir}/{saved_dates2[idx]}')\n",
    "\n",
    "            del julian_date_before_subset\n",
    "\n",
    "            return(0)\n",
    "\n",
    "        def run_selected_window_to_save_memory(non_completed_dates):\n",
    "            #first get the first date to only have to load all the files once\n",
    "            dates_init = non_completed_dates[0].split('-')\n",
    "            month_init = int(dates_init[0])\n",
    "            day_init = int(dates_init[1])\n",
    "\n",
    "            #Select the anomaly file dates just to have it to access later dates\n",
    "            run_dates = anomaly_file.sel(S=(anomaly_file['S.month'] == month_init) & (anomaly_file['S.day'] == day_init))\n",
    "            saved_dates1 = run_dates.S.values\n",
    "\n",
    "            #first choose a single date for the window\n",
    "            single_date = saved_dates1[0]\n",
    "\n",
    "            #Grab the distribution (getting even days 15 after)\n",
    "            selected_window = window_select_reforecast_by_realization(file=julian_file, time=single_date,window=window,add_window=day_to_grab)\n",
    "\n",
    "            #Now loop through each of the non_completed dates\n",
    "            for date_to_run in non_completed_dates:\n",
    "                run_subset_of_selected_window(date_to_run,selected_window)\n",
    "\n",
    "            del selected_window\n",
    "            return(0)\n",
    "\n",
    "        #Run function\n",
    "        if len(non_completed_dates) > 0 :\n",
    "            run_selected_window_to_save_memory(non_completed_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a91803-d8f0-4bf2-83f9-f8ea686224a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @njit(parallel=True)\n",
    "# def pos_func_numba(all_values1, all_leads1, mask_anom,out_array):\n",
    "#     # X,Y=10,10\n",
    "#     print(\"all_values1 shape:\", all_values1.shape)\n",
    "#     print(\"all_leads1 shape:\", all_leads1.shape)\n",
    "#     print(\"mask_anom shape:\", mask_anom.shape)\n",
    "#     print(\"out_array shape:\", out_array.shape)\n",
    "#     assert all_values1.shape[3:5] == mask_anom.shape, \"Dimension mismatch between all_values1 and mask_anom\"\n",
    "#     assert all_leads1.shape[0:2] == mask_anom.shape, \"Dimension mismatch between all_leads1 and mask_anom\"\n",
    "#     assert out_array.shape[0:2] == mask_anom.shape, \"Dimension mismatch between out_array and mask_anom\"\n",
    "    \n",
    "#     for Y in prange(all_leads1.shape[0]):\n",
    "#         for X in prange(all_leads1.shape[1]):\n",
    "#             if mask_anom[Y,X] == 1:\n",
    "#                 print(f'Working on X: {X} and Y: {Y}')\n",
    "#                 a = all_values1[:, :, :, Y, X].flatten() #(1021097,)\n",
    "#                 a = a.flatten()\n",
    "#                 a = a[~np.isnan(a)]\n",
    "#                 a = a[a != 0]\n",
    "#                 a.shape\n",
    "                \n",
    "#                 score = all_leads1[Y, X, :] #(57365,)\n",
    "#                 good_index = ~np.isnan(score)\n",
    "#                 score = score[good_index]\n",
    "#                 print(\"score shape:\", score.shape)\n",
    "#                 n = a.size #(1021097,)\n",
    "#                 score = score[..., None] #(57365, 1)\n",
    "                \n",
    "#                 def count(x):\n",
    "#                     return np.count_nonzero(x, axis=-1)\n",
    "    \n",
    "#                 print('Right before division')\n",
    "#                 print(\"a:\", a.shape)\n",
    "#                 print(\"score shape:\", score.shape)\n",
    "#                 div = a < score\n",
    "#                 print(\"div shape:\", div.shape)\n",
    "#                 count_ = np.count_nonzero(div,axis=-1)\n",
    "                \n",
    "#                 # perct = count() * (100.0 / n) #Strict\n",
    "#                 print('Right before the ranking')\n",
    "                \n",
    "#                 perct = count_ * (100.0 / n) #weak\n",
    "#                 out_array[Y,X,good_index] = perct\n",
    "#     print(\"Finished processing\")\n",
    "#     return(out_array)\n",
    "    \n",
    "# out_array = np.empty_like(all_leads1)\n",
    "# out_array.shape #(48, 96, 57365)\n",
    "# o_array = pos_func_numba(all_values1, all_leads1,mask_anom,out_array)\n",
    "\n",
    "\n",
    "# np.nanmax(o_array)\n",
    "# o_array[Y, X, :]\n",
    "\n",
    "#             stack_cp = stack.copy(deep=True)\n",
    "#             stack_cp.RZSM[:,:,:] = percentile_arr\n",
    "#             unstack = stack_cp.unstack('init_mod_lead').transpose('S','M','L','Y','X')\n",
    "#             unstack.RZSM[:,0,0,10,10].values\n",
    "\n",
    "# # stack_cp = stack.copy(deep=True)\n",
    "# # stack_cp.RZSM[:,:,:] = percentile_arr\n",
    "\n",
    "# # unstack = stack_cp.unstack('init_mod_lead').transpose('S','M','L','Y','X')\n",
    "# # unstack.RZSM[:,0,0,10,10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c430810-7fdf-4eed-b0eb-5ebc84d5f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf212gpu]",
   "language": "python",
   "name": "conda-env-tf212gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
