{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ba6510-3860-42d4-93d5-a2c4b22d4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e1608-8564-4b92-8631-f5aafcc9eb2d",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## This data is very messy. The d10 and d35 leads are split up and the first lead of d35 is actually the last lead of d10. Also, there are several realizations where they data is not complete throughout the entire file. So some days for specific realizations have <80 steps when they all should have 80 steps. \n",
    "\n",
    "## I'm not sure if the NCL operators messed this up, but I've deleted and re-processed the bad files once and nothing changed. So all in all, I'd say 99% of files are fine, but we are replacing the bad realizations by the control realization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9c969e-4941-4201-a478-6703598fe852",
   "metadata": {},
   "outputs": [],
   "source": [
    "global region_names, var_names\n",
    "region_names = ['china']\n",
    "var_names = ['soilw_bgrnd', 'hgt_pres', 'hgt_pres', 'pwat_eatm', 'tmin_2m', 'tmax_2m', 'spfh_2m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e3c27c-7cda-45e4-9ce9-7748bd739ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "global start_date, end_date, dates\n",
    "#dates\n",
    "#GEFS long-term (multi-ensemble) forecasts are only initialized on Wednesdays\n",
    "start_date = dt.date(2000, 1, 1)\n",
    "\n",
    "#Actual end date (only data through that period for this website https://noaa-gefs-retrospective.s3.amazonaws.com/index.html#GEFSv12/reforecast/)\n",
    "end_date = dt.date(2019, 12, 31)\n",
    "\n",
    "dates = [start_date + dt.timedelta(days=d) for d in range(0, end_date.toordinal() - start_date.toordinal() + 1)]\n",
    "\n",
    "#from date time, Wednesday is a 2. (Monday is a 0) https://docs.python.org/3/library/datetime.html#datetime.datetime.weekday\n",
    "\n",
    "dates = [i for i in dates if i.weekday() ==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "856781d5-cbda-45dc-9ad6-89d0ac6460ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_average_of_ensembles(var,var_name,open_d10,open_d35,template_GEFS_initial,ensemble_number):\n",
    "\n",
    "    #take the average of the different lead dates, but not for tmax and tmin\n",
    "    if (var == 'tmax_2m') or (var =='tmin_2m'):\n",
    "     #for tmax and tmin\n",
    "     #Only find the maximum and minimum of each day. Don't take the averages\n",
    "     start_ = 0\n",
    "     steps = {}\n",
    "     \n",
    "     #they ordered the days terribly. There are 6 timesteps for each day for d10 files. Except the last timestep which actually belongs to the first timestep of d35\n",
    "     for i in range(len_leads):\n",
    "         if i ==0:\n",
    "             steps[i] = open_d10[f'{var_name}'][start_:start_+7,:,:].max(dim=step_name).values\n",
    "             start_+=8 #needed to begin the next index to keep up with proper dates\n",
    "         elif i<10:\n",
    "             steps[i] = open_d10[f'{var_name}'][start_:start_+7,:,:].max(dim=step_name).values #eight total possible values until last time step\n",
    "             start_+= 8 #Need to add one because we don't want to re-index the same day\n",
    "         elif i == 10:\n",
    "             # break\n",
    "             try:\n",
    "                 #Need to take from the first file (time 00:00:00), and combine with d35 files\n",
    "                 #find the max of 4 different dates\n",
    "                 #make a new array\n",
    "                 new_array = np.empty(shape=(4,open_d10[f'{var_name}'].shape[1],open_d10[f'{var_name}'].shape[2]))\n",
    "                 new_array[0,:,:] = open_d10[f'{var_name}'][-1,:,:]\n",
    "                 new_array[1,:,:] = open_d35[f'{var_name}'][0,:,:]\n",
    "                 new_array[2,:,:] = open_d35[f'{var_name}'][1,:,:] \n",
    "                 new_array[3,:,:] = open_d35[f'{var_name}'][2,:,:] \n",
    "                     \n",
    "                 s1 = np.nanmax(new_array,axis=0)\n",
    "                 steps[i] = s1\n",
    "                 start_ = 3 #start count over, 4th file is the new date in d35 files\n",
    "             except IndexError:\n",
    "                 pass\n",
    "                 #Some ensembles have broken members\n",
    "         elif i <=34:\n",
    "             steps[i] = open_d35[f'{var_name}'][start_:start_+4,:,:].max(dim=step_name).values\n",
    "             start_+=4\n",
    "     #Add to file\n",
    "     for step,lead_day in enumerate(steps.keys()):\n",
    "         template_GEFS_initial[:,ensemble_number,step,:,:] = steps[lead_day]\n",
    "         \n",
    "    else: \n",
    "        start_ = 0\n",
    "        steps = {}\n",
    "        for i in range(len_leads):\n",
    "            # break\n",
    "            if i ==0:\n",
    "                steps[i] = open_d10[f'{var_name}'][start_:start_+7,:,:].mean(dim=step_name).values\n",
    "                start_+=8 #needed to begin the next index to keep up with proper dates\n",
    "            elif i<10:\n",
    "                steps[i] = open_d10[f'{var_name}'][start_:start_+7,:,:].mean(dim=step_name).values #eight total possible values until last time step\n",
    "                start_+= 8 #Need to add one because we don't want to re-index the same day\n",
    "            elif i == 10:\n",
    "                #Need to take from the first file (time 00:00:00), and combine with d35 files\n",
    "                s1 = (open_d10[f'{var_name}'][-1,:,:] + open_d35[f'{var_name}'][0,:,:] + \\\n",
    "                    open_d35[f'{var_name}'][1,:,:] + open_d35[f'{var_name}'][2,:,:]) /4\n",
    "                steps[i] = s1\n",
    "                start_ = 3 #start count over, 4th file is the new date in d35 files\n",
    "            elif i <=34:\n",
    "                steps[i] = open_d35[f'{var_name}'][start_:start_+4,:,:].mean(dim=step_name).values\n",
    "                start_+=4\n",
    "        #Add to file\n",
    "        for step,lead_day in enumerate(steps.keys()):\n",
    "            template_GEFS_initial[:,ensemble_number,step,:,:] = steps[lead_day]         \n",
    "            \n",
    "      \n",
    "    return(template_GEFS_initial[:,:,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee71110-d5e7-4859-a95a-ad4853750234",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def return_xarray_file(var, template_GEFS_initial, julian_list, _date, open_d10):\n",
    "    if 'dlwrf' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                dlwrf = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Downwelling longwave radiation GEFSv12. Daily average already computed. All ensembles and Ls in one file'),\n",
    "        )  \n",
    "        \n",
    "    if 'pwat_eatm' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                dlwrf = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Precipitable water GEFSv12. Daily average already computed. All ensembles and Ls in one file'),\n",
    "        )  \n",
    "        \n",
    "    elif 'dswrf' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                dswrf = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Downwelling shortwave radiation GEFSv12. Daily average already computed. All ensembles and Ls in one file'),\n",
    "        )  \n",
    "    elif 'soil' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                RZSM = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "    \n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Volumetric soil moisture content 0-100cm: 0.0-0.1, 0.1-0.4, 0.4-1.0 and 1.-2. m depth \\\n",
    "    (fraction between wilting and saturation) GEFSv12. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'tmp' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                tmean = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Average temperature GEFSv12. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'ulwrf' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                ulwrf = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Longwave upwelling radiation. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'uswrf' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                uswrf = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Shortwave upwelling radiation. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'spfh' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                spfh = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Specific humidity. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'tmax' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                tasmax = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Maximum Temperature. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'tmin' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                tasmin = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Minimum Temperature. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'uflx' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                uas = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'U component of wind, N/m2 momentum flux. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'vflx' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                vas = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'V component of wind, N/m2 momentum flux. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif var == 'hgt_pres':\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                z = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Geopotential height. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    \n",
    "        # GEFS_out = GEFS_out.sel()\n",
    "    elif 'apcp_sfc' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                pr = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Precipitation. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "        GEFS_out2 = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                pr = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = np.arange(35),\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Precipitation. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'ugrd' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                uas = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'U component of wind, m/2. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'vgrd' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                vas = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'V component of wind, m/2. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "        \n",
    "    elif 'lhtfl' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                latentHeat = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Surface Latent heat, W/m^2. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "        \n",
    "    elif 'shtfl' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                sensibleHeat = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Surface Sensible heat, W/m^2. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "\n",
    "    return(GEFS_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4b0437f-aef4-476a-9dcf-41373c96c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ensemble_members(_date):\n",
    "    #testing\n",
    "    # _date = pd.to_datetime('2004-03-03')\n",
    "    # region_name='CONUS'\n",
    "    # var = 'hgt_pres'\n",
    "    \n",
    "    \n",
    "    global len_leads, step_name\n",
    "    len_leads = 35\n",
    "    \n",
    "    #After manually inspecting the files after using NCL operators, the \"step\" has been replaced by \"forecast_time0\"\n",
    "    step_name = 'forecast_time0'\n",
    "            \n",
    "    \n",
    "    for region_name in region_names:\n",
    "        for var in var_names:\n",
    "            \n",
    "            if region_name == 'CONUS':\n",
    "                save_dir = '/glade/work/klesinger/FD_RZSM_deep_learning/Data/GEFSv12_reforecast'\n",
    "                source_dir = f'/glade/derecho/scratch/klesinger/GEFSv12_raw/{var}/{var}_processed'\n",
    "                lat_ = 200 #you must manually know the size of the input file\n",
    "                lon_ = 380\n",
    "            elif region_name == 'australia':\n",
    "                save_dir = '/glade/work/klesinger/FD_RZSM_deep_learning/Data_australia/GEFSv12_reforecast'\n",
    "                source_dir = f'/glade/derecho/scratch/klesinger/GEFSv12_raw/{var}/{var}_processed_australia'\n",
    "                lat_ = 48 #you must manually know the size of the input file\n",
    "                lon_ = 96\n",
    "            elif region_name == 'china':\n",
    "                save_dir = '/glade/work/klesinger/FD_RZSM_deep_learning/Data_china/GEFSv12_reforecast'\n",
    "                source_dir = f'/glade/derecho/scratch/klesinger/GEFSv12_raw/{var}/{var}_processed_china'\n",
    "                lat_ = 48 #you must manually know the size of the input file\n",
    "                lon_ = 96        \n",
    "            \n",
    "            \n",
    "            os.system(f'mkdir -p {save_dir}/{var}')\n",
    "            os.chdir(f'{source_dir}')\n",
    "            \n",
    "            #For soil layer depth\n",
    "            global soil_layer_depth, weighted_RZSM\n",
    "            soil_layer_depth=3 #0-100cm. Can do 0-2m if number =4\n",
    "            \n",
    "            weighted_RZSM=True #weighted sum of the individual layers (we only have it set to 3 layers, you must modify the code if soil_layer_depth != 3)\n",
    "\n",
    "            os.system(f'mkdir -p {save_dir}')\n",
    "        \n",
    "            def name(file):\n",
    "                return(list(file.keys())[0])\n",
    "        \n",
    "            len_steps_d10 = 80 #I checked the good files and this is how many steps it should have\n",
    "            len_steps_d35 = 100 #I checked the good files and this is how many steps it should have\n",
    "            \n",
    "\n",
    "            '''Need to create a new xarray template with 11 ensemble members, 35 lead days, lat/lon same as GMAO \n",
    "            (because file is already regridded to be in the same format as all other observations and GMAO '''\n",
    "        \n",
    "            #This template was created from the code above\n",
    "            template_GEFS_initial = np.empty(shape=(1,11,len_leads,lat_,lon_))\n",
    "            lead_splices = ['d10','d35']\n",
    "        \n",
    "            all_possible_ensemble_members = ['c00', 'p01', 'p02', 'p03', 'p04', 'p05', 'p06', 'p07', 'p08', 'p09', 'p10']\n",
    "            #testing with one at a time\n",
    "            # vars_to_process = ['soilw_bgrnd']\n",
    "        \n",
    "            #Get the dates of the files\n",
    "        \n",
    "            # _date=dates[0]\n",
    "    \n",
    "            #The date of the file is when it was intialized\n",
    "            out_date_create = pd.to_datetime(_date) \n",
    "            out_date = f'{out_date_create.year}-{out_date_create.month:02}-{out_date_create.day:02}'\n",
    "                \n",
    "            final_out_name = f'{var}_EMC_{out_date}.nc'\n",
    "    \n",
    "            if os.path.exists(f\"{save_dir}/{var}/{final_out_name}\"):\n",
    "                pass\n",
    "            else:\n",
    "                            \n",
    "                print(f'Working on variable {var} to merge ensemble members for date {_date}.')\n",
    "    \n",
    "                '''Now combine the files, because there are some missing ensemble members (not sure why)\n",
    "                we need to account for files with 1.) ALL members, 2.) some members\n",
    "        \n",
    "                It gets more complicated when the missing ensemble members are different between each\n",
    "                d10 (first 10 days) and d35 (last 25 days), but I have figured it out'''\n",
    "                \n",
    "                template_GEFS_initial[:,:,:,:,:] = np.nan\n",
    "                # lead_day=0 #keeps up with which index is correct in template_GEFS_initial (resets with each date)\n",
    "    \n",
    "                all_files_d10 = sorted(glob(f'{lead_splices[0]}_{var}_{_date.year}{_date.month:02}{_date.day:02}*.n*'))\n",
    "                all_files_d35 = sorted(glob(f'{lead_splices[1]}_{var}_{_date.year}{_date.month:02}{_date.day:02}*.n*'))\n",
    "    \n",
    "                good_files_d10 = []\n",
    "                good_files_d35 = []\n",
    "\n",
    "                #Save name template for later\n",
    "                \n",
    "                \n",
    "                #Some files don't have all the steps, so just remove the bad files from processing (they are going to be replaced later by a different realization)\n",
    "                for i in all_files_d10:\n",
    "                    op = xr.open_dataset(i)\n",
    "                    op.close()\n",
    "                    if len(op[step_name].values) == len_steps_d10: \n",
    "                        good_files_d10.append(i)\n",
    "                        \n",
    "                for i in all_files_d35:\n",
    "                    op = xr.open_dataset(i)\n",
    "                    op.close()\n",
    "                    if len(op[step_name].values) == len_steps_d35: \n",
    "                        good_files_d35.append(i)            \n",
    "\n",
    "                tem = good_files_d10[0].split('_')\n",
    "                \n",
    "                #some files have doubles (rsync error or from HPC when converting)\n",
    "                file_len = [len(i) for i in good_files_d10]\n",
    "                \n",
    "                mode = max(set(file_len), key=file_len.count)\n",
    "                #Replace files\n",
    "                good_files_d10 = [i for i in good_files_d10 if len(i) == mode]\n",
    "                good_files_d35 = [i for i in good_files_d35 if len(i) == mode]\n",
    "                  \n",
    "                    \n",
    "                #TODO:If all realizations are present\n",
    "                if (len(good_files_d10) == 11) and (len(good_files_d35) == 11):\n",
    "                    #If all possible files are there, then this is the easy code processing to add data to single file\n",
    "                    for ensemble_number,files in enumerate(zip(good_files_d10,good_files_d35)):\n",
    "                         # break\n",
    "                        if var != 'soilw_bgrnd' and var != 'hgt_pres':\n",
    "                            open_d10 = xr.open_dataset(files[0])\n",
    "                            open_d35 = xr.open_dataset(files[1])\n",
    "                            try:\n",
    "                                var_name = [i for i in list(open_d10.keys()) if 'step' not in i][0]\n",
    "                            except IndexError:\n",
    "                                pass\n",
    "                                \n",
    "                        elif var == 'hgt_pres':\n",
    "                        \n",
    "                            open_d10 = xr.open_dataset(files[0])\n",
    "                            open_d35 = xr.open_dataset(files[1])\n",
    "    \n",
    "                            #If you have all the values in a file, you can subset here. \n",
    "                            # open_d10 = xr.open_dataset(files[0]).sel(isobaricInhPa=hgt_pressure_level)\n",
    "                            # open_d35 = xr.open_dataset(files[1]).sel(isobaricInhPa=hgt_pressure_level)\n",
    "    \n",
    "                            var_name = [i for i in list(open_d10.keys()) if 'step' not in i][0]\n",
    "                            \n",
    "        \n",
    "                                \n",
    "                        elif var == 'soilw_bgrnd':\n",
    "                            #Take the sum of the columns\n",
    "                            open_d10= xr.open_dataset(files[0])\n",
    "                            open_d35 = xr.open_dataset(files[1])\n",
    "                            var_name = [i for i in list(open_d10.keys()) if 'step' not in i][0]\n",
    "                            #TODO: Take the summation of the first 3 soil layers (0-100cm)\n",
    "                            if weighted_RZSM == False:\n",
    "                                open_d10 = open_d10[f'{var_name}'][:,0:soil_layer_depth,:,:].sum(dim=['SOILW_P1_2L106_GLL0']).to_dataset()\n",
    "                                open_d35 = open_d35[f'{var_name}'][:,0:soil_layer_depth,:,:].sum(dim=['SOILW_P1_2L106_GLL0']).to_dataset()\n",
    "                            else:\n",
    "                                # break\n",
    "                                #take weighted mean by layer\n",
    "                                open_d10 = (np.multiply(open_d10[f'{var_name}'][:,0,:,:],0.1) + np.multiply(open_d10[f'{var_name}'][:,1,:,:],0.3) + np.multiply(open_d10[f'{var_name}'][:,1,:,:],0.6)).to_dataset()\n",
    "                                open_d35 = (np.multiply(open_d35[f'{var_name}'][:,0,:,:],0.1) + np.multiply(open_d35[f'{var_name}'][:,1,:,:],0.3) + np.multiply(open_d35[f'{var_name}'][:,1,:,:],0.6)).to_dataset()\n",
    "    \n",
    "                        '''EMC has some broken files where there is only 1 time spot when it should have 35'''\n",
    "                        try:\n",
    "                            open_d10[f'{step_name}'].values\n",
    "                            open_d35[f'{step_name}'].values\n",
    "                            \n",
    "                            if len(open_d10[f'{step_name}'].values) == 0 or len(open_d35[f'{step_name}'].values) == 0:\n",
    "                                pass_=True\n",
    "                            else:\n",
    "                                pass_=False\n",
    "                                \n",
    "                        except AttributeError:\n",
    "                            #no steps in file, just a single file (equals a bad file)\n",
    "                            pass_=True\n",
    "                        ##########################################################\n",
    "                        if pass_==True:\n",
    "                            pass\n",
    "                        elif len(open_d10[f'{step_name}'].values)==1 or len(open_d35[f'{step_name}'].values)==1:\n",
    "                            pass\n",
    "                        else:\n",
    "                        #First get the dates of the files\n",
    "                            '''Take average of first 7 timesteps if d10 file. I have verified\n",
    "                            this is correct when looking at HPC'''\n",
    "                            template_GEFS_initial[:,:,:,:,:] = return_average_of_ensembles(var=var,var_name=var_name,open_d10=open_d10,open_d35=open_d35,template_GEFS_initial=template_GEFS_initial[:,:,:,:,:],ensemble_number=ensemble_number)    \n",
    "                            \n",
    "                            \n",
    "                #If all ensembles are missing, do nothing\n",
    "                elif (len(good_files_d10) == 0 )and (len(good_files_d35) == 0):\n",
    "                    # print(_date)\n",
    "                    pass\n",
    "                \n",
    "                #If there are a differnet number of ensembles between leads\n",
    "                elif (len(good_files_d10) != 11) or (len(good_files_d35) != 11):\n",
    "    \n",
    "                    #Some ensembles are missing, split to get the name of ensemble members\n",
    "                    avail_ensemble_members_d10 = [i.split('_')[-1].split('.')[0] for i in good_files_d10]\n",
    "                    avail_ensemble_members_d35 = [i.split('_')[-1].split('.')[0] for i in good_files_d35]\n",
    "\n",
    "                    missing_d10_ = sorted(list(set(all_possible_ensemble_members).difference(set(avail_ensemble_members_d10))))\n",
    "                    missing_d35_ = sorted(list(set(all_possible_ensemble_members).difference(set(avail_ensemble_members_d35))))\n",
    "                    \n",
    "                    #if missing only the exact same realizations                \n",
    "                    if (len(missing_d35_) ==  len(missing_d10_)) and (missing_d35_==missing_d10_):\n",
    "                        #Find a way to append the missing ensemble files with np.nan\n",
    "                        \n",
    "                        for idx,ensemble in enumerate(all_possible_ensemble_members):\n",
    "                            \n",
    "                            if ensemble not in avail_ensemble_members_d10:\n",
    "                                pass\n",
    "                                #just keep the data as np.nan\n",
    "                            else:\n",
    "                                idx_num = avail_ensemble_members_d10.index(ensemble)\n",
    "                                open_d10=xr.open_dataset(good_files_d10[idx_num])\n",
    "                                open_d35=xr.open_dataset(good_files_d35[idx_num])\n",
    "                                \n",
    "                                if var == 'hgt_pres':\n",
    "                                    open_d10 = xr.open_dataset(files[0])\n",
    "                                    open_d35 = xr.open_dataset(files[1])\n",
    "            \n",
    "                                    #If you have all the values in a file, you can subset here. \n",
    "                                    # open_d10 = xr.open_dataset(files[0]).sel(isobaricInhPa=hgt_pressure_level)\n",
    "                                    # open_d35 = xr.open_dataset(files[1]).sel(isobaricInhPa=hgt_pressure_level)\n",
    "                                elif var == 'soilw_bgrnd':\n",
    "                                    if weighted_RZSM == False:\n",
    "                                        open_d10 = open_d10[f'{var_name}'][:,0:soil_layer_depth,:,:].sum(dim=['SOILW_P1_2L106_GLL0']).to_dataset()\n",
    "                                        open_d35 = open_d35[f'{var_name}'][:,0:soil_layer_depth,:,:].sum(dim=['SOILW_P1_2L106_GLL0']).to_dataset()\n",
    "                                    else:\n",
    "                                        open_d10 = (np.multiply(open_d10[f'{var_name}'][:,0,:,:],0.1) + np.multiply(open_d10[f'{var_name}'][:,1,:,:],0.3) + np.multiply(open_d10[f'{var_name}'][:,1,:,:],0.6)).to_dataset()\n",
    "                                        open_d35 = (np.multiply(open_d35[f'{var_name}'][:,0,:,:],0.1) + np.multiply(open_d35[f'{var_name}'][:,1,:,:],0.3) + np.multiply(open_d35[f'{var_name}'][:,1,:,:],0.6)).to_dataset()\n",
    "    \n",
    "                                else:\n",
    "                                    open_d10=xr.open_dataset(good_files_d10[idx_num])\n",
    "                                    open_d35=xr.open_dataset(good_files_d35[idx_num]) \n",
    "                                \n",
    "                                var_name = [i for i in list(open_d10.keys()) if 'step' not in i][0]\n",
    "                                \n",
    "    \n",
    "                                '''Take average of first 7 timesteps if d10 file. I have verified\n",
    "                                this is correct when looking at HPC'''\n",
    "                                template_GEFS_initial = return_average_of_ensembles(var=var,var_name=var_name,open_d10=open_d10,open_d35=open_d35,template_GEFS_initial=template_GEFS_initial,ensemble_number=idx)    \n",
    "    \n",
    "                    # missing different realizations \n",
    "                    else:\n",
    "                         #Because there are missing ensemble members that are supposed to be aligned,\n",
    "                         #we must delete those ensemble members\n",
    "            \n",
    "                         # #remove missing members\n",
    "                         # out_10 = [i for i in all_possible_ensemble_members if i not in missing_d10_]\n",
    "                         # out_35 = [i for i in all_possible_ensemble_members if i not in missing_d35_]\n",
    "                        \n",
    "                         #replace empty file with the control file. Deep learning doesn't like np.nan values\n",
    "                         #there are so few of these missing files that it should be fine\n",
    "                         for i in good_files_d10:\n",
    "                             #for each set of files\n",
    "                             for m in all_possible_ensemble_members:\n",
    "                                 name_out = f'd10_{var}_{tem[3]}_{m}.nc'\n",
    "                                 \n",
    "                                 if name_out in good_files_d10:\n",
    "                                     pass\n",
    "                                 #for each set of members\n",
    "                                 #we need to see if the file exists, if not create a blank one\n",
    "                                 else:\n",
    "                                     # break\n",
    "                                     temp_10=xr.open_dataset(good_files_d10[0]) #make a temporary file as the blank file\n",
    "                                     # temp_10[name(temp_10)][:,:,:] = np.nan\n",
    "                                     temp_10.to_netcdf(f'd10_{i[4:24]}{m}.nc')\n",
    "                                 \n",
    "                         #replace empty file with the control file. Deep learning doesn't like np.nan values\n",
    "                         #there are so few of these missing files that it should be fine\n",
    "                         for i in good_files_d35:\n",
    "                             # print(i)\n",
    "                             #for each set of files\n",
    "                             for m in all_possible_ensemble_members:\n",
    "                                 # print(m)\n",
    "                                 name_out = f'd35_{var}_{tem[3]}_{m}.nc'\n",
    "                                 if name_out in good_files_d35:\n",
    "                                     pass\n",
    "                                 #for each set of members\n",
    "                                 #we need to see if the file exists, if not create a blank one\n",
    "                                 else:\n",
    "                                     temp_10=xr.open_dataset(all_files_d35[0]) #make a temporary file as the blank file\n",
    "                                     # temp_10[name(temp_10)][:,:,:] = np.nan\n",
    "                                     temp_10.to_netcdf(f'd35_{i[4:24]}{m}.nc')\n",
    "                         #'''Take average of first 7 timesteps if d10 file. I have verified\n",
    "                         #this is correct when looking at HPC'''\n",
    "                         \n",
    "                         for idx,ensemble in enumerate(all_possible_ensemble_members):\n",
    "                             name_out_d10 = f'd10_{var}_{tem[3]}_{ensemble}.nc'\n",
    "                             name_out_d35 = f'd35_{var}_{tem[3]}_{ensemble}.nc'\n",
    "                             \n",
    "                             # idx_num = avail_ensemble_members_d10.index(ensemble)\n",
    "                             open_d10=xr.open_dataset(name_out_d10)\n",
    "                             open_d35=xr.open_dataset(name_out_d35)\n",
    "                             var_name = [i for i in list(open_d10.keys()) if 'step' not in i][0]\n",
    "                             \n",
    "                             template_GEFS_initial[:,:,:,:,:]  = return_average_of_ensembles(var=var,var_name=var_name,open_d10=open_d10,open_d35=open_d35,template_GEFS_initial=template_GEFS_initial,ensemble_number=idx)    \n",
    "               \n",
    "    \n",
    "                def julian_date(_date,template_GEFS_initial):\n",
    "                    #Return julian date for anomaly calculation\n",
    "                    a_date_in= template_GEFS_initial.shape[2]\n",
    "                    #get the start date\n",
    "                    a_start_date = pd.to_datetime(_date) \n",
    "        \n",
    "                    a_date_out=[]\n",
    "                    for a_i in range(a_date_in):\n",
    "                        a_date_out.append((a_start_date + np.timedelta64(a_i,'D')).timetuple().tm_yday)\n",
    "            \n",
    "                    return(a_date_out)\n",
    "    \n",
    "                #Can specifically add the julian date if you want.\n",
    "                # julian_list = julian_date(_date,template_GEFS_initial)\n",
    "                #Instead of replacing the below lines, lets just make it a 35 day lead\n",
    "    \n",
    "                #This is just the number of leads\n",
    "                julian_list=np.arange(len_leads)\n",
    "    \n",
    "                GEFS_out = return_xarray_file(var, template_GEFS_initial, julian_list, _date, open_d10)\n",
    "                GEFS_out = GEFS_out.astype(np.float32)\n",
    "                \n",
    "                GEFS_out.to_netcdf(path = f\"{save_dir}/{var}/{final_out_name}\")\n",
    "                GEFS_out.close()\n",
    "\n",
    "    return(0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e492bed-470a-4f95-a557-a98301efe1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on variable soilw_bgrnd to merge ensemble members for date 2003-02-12.\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2004-02-25.Working on variable soilw_bgrnd to merge ensemble members for date 2002-08-07.Working on variable soilw_bgrnd to merge ensemble members for date 2000-01-05.Working on variable soilw_bgrnd to merge ensemble members for date 2001-01-17.Working on variable soilw_bgrnd to merge ensemble members for date 2004-09-01.Working on variable soilw_bgrnd to merge ensemble members for date 2001-07-25.\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2003-08-20.\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2002-01-30.Working on variable soilw_bgrnd to merge ensemble members for date 2000-07-12.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Working on variable hgt_pres to merge ensemble members for date 2001-01-17.Working on variable hgt_pres to merge ensemble members for date 2004-02-25.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2002-08-07.Working on variable hgt_pres to merge ensemble members for date 2003-02-12.\n",
      "\n",
      "\n",
      "Working on variable hgt_pres to merge ensemble members for date 2003-08-20.Working on variable hgt_pres to merge ensemble members for date 2002-01-30.\n",
      "\n",
      "Working on variable hgt_pres to merge ensemble members for date 2000-07-12.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2004-09-01.Working on variable hgt_pres to merge ensemble members for date 2001-07-25.\n",
      "\n",
      "Working on variable hgt_pres to merge ensemble members for date 2000-01-05.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2001-01-17.Working on variable pwat_eatm to merge ensemble members for date 2003-02-12.\n",
      "\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2002-08-07.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2004-02-25.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2003-08-20.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2002-01-30.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2001-07-25.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2000-01-05.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2000-07-12.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2004-09-01.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2003-02-12.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2001-01-17.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2004-02-25.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2002-08-07.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2003-08-20.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2002-01-30.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2000-01-05.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2000-07-12.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2004-09-01.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2001-07-25.\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2005-03-09.Working on variable soilw_bgrnd to merge ensemble members for date 2005-09-14.Working on variable soilw_bgrnd to merge ensemble members for date 2006-09-27.\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2006-03-22.\n",
      "\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2007-04-04.\n",
      "\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2007-10-10.\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2008-10-22.Working on variable soilw_bgrnd to merge ensemble members for date 2008-04-16.Working on variable soilw_bgrnd to merge ensemble members for date 2009-04-29.\n",
      "\n",
      "\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2009-11-04.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2006-09-27.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2005-09-14.Working on variable hgt_pres to merge ensemble members for date 2007-04-04.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2005-03-09.\n",
      "\n",
      "Working on variable hgt_pres to merge ensemble members for date 2007-10-10.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2006-03-22.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2009-04-29.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2008-10-22.Working on variable hgt_pres to merge ensemble members for date 2008-04-16.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    p = Pool(10)\n",
    "    p.map(merge_ensemble_members,dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460be60-7e3d-4b77-a948-21bcc8a1adf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671128d-1c4c-4c6f-89ab-3a33a6a4e849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf210gpu]",
   "language": "python",
   "name": "conda-env-tf210gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
