{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ba6510-3860-42d4-93d5-a2c4b22d4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9c969e-4941-4201-a478-6703598fe852",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '/glade/derecho/scratch/klesinger/FD_RZSM_deep_learning/Data/reforecast/GEFSv12/CONUS/soilw_bgrnd/soilw_bgrnd_EMC_2007-04-04.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b054688-e96b-4a06-882b-566d8d84c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f= xr.open_dataset(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c448ecf-74f4-4b18-acdf-fa69a8793c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.43720004, 0.4236429 , ..., 0.40442857,\n",
       "           0.4028143 , 0.40088573],\n",
       "          [0.43700004, 0.43400005, 0.43534288, ..., 0.40695718,\n",
       "           0.40325716, 0.402     ],\n",
       "          [0.43800002, 0.4313    , 0.4257429 , ..., 0.40385717,\n",
       "           0.38892862, 0.38334292]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.43745714, 0.4247286 , ..., 0.4082857 ,\n",
       "           0.4050286 , 0.40322858],\n",
       "          [0.43700004, 0.43400005, 0.4356572 , ..., 0.40802863,\n",
       "           0.4035857 , 0.40261433],\n",
       "          [0.43800002, 0.4314429 , 0.42602867, ..., 0.40448576,\n",
       "           0.39094287, 0.38407144]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.43900007, 0.43527147, ..., 0.4121429 ,\n",
       "           0.40718576, 0.40512863],\n",
       "          [0.43700004, 0.43400005, 0.4383572 , ..., 0.40957147,\n",
       "           0.40441427, 0.40315717],\n",
       "          [0.43784288, 0.43260008, 0.42702863, ..., 0.40510005,\n",
       "           0.39280003, 0.38395718]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.38947505, 0.355725  , 0.326775  , ..., 0.41110003,\n",
       "           0.41110003, 0.41130003],\n",
       "          [0.388     , 0.39097503, 0.35342506, ..., 0.412975  ,\n",
       "           0.41355005, 0.41400003],\n",
       "          [0.38350004, 0.37860003, 0.38497505, ..., 0.42047507,\n",
       "           0.42210007, 0.41615006]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.37735003, 0.34282506, 0.31400004, ..., 0.41120005,\n",
       "           0.41107503, 0.41105002],\n",
       "          [0.38072503, 0.38502502, 0.34265   , ..., 0.41270006,\n",
       "           0.4131    , 0.41345003],\n",
       "          [0.37672505, 0.37005007, 0.377225  , ..., 0.4208001 ,\n",
       "           0.42170006, 0.41605002]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.36520004, 0.33112502, 0.30297503, ..., 0.41115004,\n",
       "           0.41095006, 0.41060004],\n",
       "          [0.37307504, 0.37892503, 0.33250004, ..., 0.41217503,\n",
       "           0.41240004, 0.41257507],\n",
       "          [0.36997503, 0.36245   , 0.36910003, ..., 0.42050007,\n",
       "           0.41990006, 0.41525006]]],\n",
       "\n",
       "\n",
       "        [[[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.43720004, 0.42338577, ..., 0.40404287,\n",
       "           0.40197143, 0.39962864],\n",
       "          [0.43700004, 0.43400005, 0.43531436, ..., 0.40568572,\n",
       "           0.4011001 , 0.4002429 ],\n",
       "          [0.43800002, 0.4322572 , 0.42674288, ..., 0.4025001 ,\n",
       "           0.38711432, 0.38217145]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.4373286 , 0.4245286 , ..., 0.40725714,\n",
       "           0.4026857 , 0.3999286 ],\n",
       "          [0.43700004, 0.43400005, 0.4364286 , ..., 0.4056572 ,\n",
       "           0.40104294, 0.40027148],\n",
       "          [0.43800002, 0.43277147, 0.42638573, ..., 0.4027143 ,\n",
       "           0.38728574, 0.38210005]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.43900007, 0.4339857 , ..., 0.41019997,\n",
       "           0.4038    , 0.4005572 ],\n",
       "          [0.43700004, 0.43400005, 0.4383572 , ..., 0.40564293,\n",
       "           0.40185717, 0.40092868],\n",
       "          [0.43787146, 0.43350005, 0.42781428, ..., 0.40315717,\n",
       "           0.38795716, 0.3826572 ]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.39802504, 0.38010004, 0.35610002, ..., 0.41630006,\n",
       "           0.41612506, 0.41565004],\n",
       "          [0.40495002, 0.40402502, 0.37457502, ..., 0.41767505,\n",
       "           0.41765004, 0.41737506],\n",
       "          [0.39947504, 0.40505004, 0.41115004, ..., 0.42452508,\n",
       "           0.42505005, 0.418725  ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.39307502, 0.37245005, 0.34467506, ..., 0.41482502,\n",
       "           0.41462505, 0.41382504],\n",
       "          [0.40117505, 0.4009    , 0.36745003, ..., 0.41562504,\n",
       "           0.41540003, 0.41515005],\n",
       "          [0.39687502, 0.40245003, 0.408275  , ..., 0.42195004,\n",
       "           0.42180005, 0.41617507]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.386325  , 0.36397505, 0.33310005, ..., 0.41430002,\n",
       "           0.41365004, 0.412925  ],\n",
       "          [0.39630002, 0.39625004, 0.358925  , ..., 0.414775  ,\n",
       "           0.41432503, 0.41380003],\n",
       "          [0.39217502, 0.39690006, 0.40207505, ..., 0.42092505,\n",
       "           0.42107505, 0.41482502]]],\n",
       "\n",
       "\n",
       "        [[[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.43720004, 0.42278573, ..., 0.4027286 ,\n",
       "           0.40141433, 0.39860004],\n",
       "          [0.4369429 , 0.43385723, 0.43438575, ..., 0.4057286 ,\n",
       "           0.4020286 , 0.4005572 ],\n",
       "          [0.4379429 , 0.43108574, 0.4252286 , ..., 0.40265718,\n",
       "           0.38732862, 0.3832715 ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.43758577, 0.42384288, ..., 0.40597147,\n",
       "           0.40275714, 0.40011433],\n",
       "          [0.43700004, 0.43394288, 0.4355286 , ..., 0.40634292,\n",
       "           0.40217146, 0.40117148],\n",
       "          [0.43800002, 0.43131432, 0.42560002, ..., 0.4035857 ,\n",
       "           0.3890572 , 0.3831429 ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.43900007, 0.4341143 , ..., 0.40951428,\n",
       "           0.4047286 , 0.40188578],\n",
       "          [0.43700004, 0.43400005, 0.4383572 , ..., 0.40771434,\n",
       "           0.40265718, 0.40108576],\n",
       "          [0.43788576, 0.4321715 , 0.42607147, ..., 0.4041143 ,\n",
       "           0.39180002, 0.38401434]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.38270003, 0.35700002, 0.32567504, ..., 0.43162504,\n",
       "           0.43035004, 0.42907503],\n",
       "          [0.37307507, 0.37967503, 0.35650003, ..., 0.43375003,\n",
       "           0.43340003, 0.4332251 ],\n",
       "          [0.35770005, 0.35780004, 0.36900002, ..., 0.4374751 ,\n",
       "           0.43795007, 0.43377504]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.37897503, 0.35245   , 0.31885004, ..., 0.42907503,\n",
       "           0.42737505, 0.42635006],\n",
       "          [0.36995006, 0.37732506, 0.35140002, ..., 0.43312508,\n",
       "           0.43260002, 0.43277505],\n",
       "          [0.35382503, 0.35432503, 0.36662504, ..., 0.43680006,\n",
       "           0.43775004, 0.43310004]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.37422502, 0.346775  , 0.31262505, ..., 0.42695004,\n",
       "           0.42542505, 0.42452505],\n",
       "          [0.36707503, 0.37492502, 0.346125  , ..., 0.43250006,\n",
       "           0.43185002, 0.43235004],\n",
       "          [0.35052502, 0.35122505, 0.36447504, ..., 0.43630007,\n",
       "           0.43732506, 0.43332505]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.43681434, 0.42210004, ..., 0.4043143 ,\n",
       "           0.40295717, 0.40121433],\n",
       "          [0.43690008, 0.4337143 , 0.43411437, ..., 0.40695718,\n",
       "           0.4032857 , 0.4025001 ],\n",
       "          [0.43800002, 0.43142864, 0.42597148, ..., 0.40375718,\n",
       "           0.38828573, 0.38338572]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.4373286 , 0.42407146, ..., 0.40854287,\n",
       "           0.4049286 , 0.40327147],\n",
       "          [0.4369429 , 0.43382862, 0.43540004, ..., 0.40802863,\n",
       "           0.4035857 , 0.40265718],\n",
       "          [0.43800002, 0.43150005, 0.42580006, ..., 0.40427145,\n",
       "           0.3898143 , 0.38330004]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.43900007, 0.43462855, ..., 0.4109857 ,\n",
       "           0.40657148, 0.40514293],\n",
       "          [0.43700004, 0.43400005, 0.43758577, ..., 0.40868574,\n",
       "           0.4048    , 0.40331432],\n",
       "          [0.43782863, 0.4329    , 0.42724293, ..., 0.40415713,\n",
       "           0.39097145, 0.38400003]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.30380005, 0.29525003, 0.28302503, ..., 0.41075003,\n",
       "           0.41227502, 0.41127503],\n",
       "          [0.33335   , 0.34012502, 0.30455002, ..., 0.41680005,\n",
       "           0.41572505, 0.41597503],\n",
       "          [0.32860002, 0.30707502, 0.31307504, ..., 0.42767504,\n",
       "           0.41967505, 0.42092505]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.29485002, 0.28615004, 0.27655   , ..., 0.40957505,\n",
       "           0.410775  , 0.41035005],\n",
       "          [0.32717502, 0.33382505, 0.297475  , ..., 0.41380006,\n",
       "           0.41322505, 0.41342503],\n",
       "          [0.321875  , 0.29922503, 0.30645   , ..., 0.42217505,\n",
       "           0.41752505, 0.41647506]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.28857505, 0.27942502, 0.271025  , ..., 0.40785003,\n",
       "           0.40915003, 0.40935007],\n",
       "          [0.322025  , 0.32895005, 0.29142505, ..., 0.41157502,\n",
       "           0.41125005, 0.41142505],\n",
       "          [0.31607503, 0.29337502, 0.30085003, ..., 0.41905004,\n",
       "           0.41537505, 0.41362503]]],\n",
       "\n",
       "\n",
       "        [[[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.4377429 , 0.43578577, 0.4227143 , ..., 0.40442857,\n",
       "           0.4035286 , 0.40134287],\n",
       "          [0.43695718, 0.4338858 , 0.4352572 , ..., 0.40718576,\n",
       "           0.4037857 , 0.40262863],\n",
       "          [0.4379572 , 0.43111435, 0.4255286 , ..., 0.40357146,\n",
       "           0.38847145, 0.38362864]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.43630004, 0.4233572 , ..., 0.40815714,\n",
       "           0.40581435, 0.40388575],\n",
       "          [0.43700004, 0.43398574, 0.4355286 , ..., 0.40885714,\n",
       "           0.4054143 , 0.40355715],\n",
       "          [0.43800002, 0.43131432, 0.42587146, ..., 0.40418577,\n",
       "           0.3908572 , 0.38428575]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.4377143 , 0.4292286 , ..., 0.41108575,\n",
       "           0.40727144, 0.4053429 ],\n",
       "          [0.43700004, 0.43400005, 0.43758577, ..., 0.4095572 ,\n",
       "           0.40605715, 0.40441433],\n",
       "          [0.43798575, 0.43191433, 0.42630005, ..., 0.40441433,\n",
       "           0.39285716, 0.38474292]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.32712504, 0.30965003, 0.29102504, ..., 0.41127503,\n",
       "           0.41195005, 0.41307503],\n",
       "          [0.34742504, 0.35797504, 0.32235003, ..., 0.41270006,\n",
       "           0.41290003, 0.41392505],\n",
       "          [0.34505004, 0.33167505, 0.33915   , ..., 0.41867504,\n",
       "           0.41975006, 0.41277504]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.3233    , 0.30670005, 0.29435003, ..., 0.41212505,\n",
       "           0.41252506, 0.41352504],\n",
       "          [0.34235004, 0.35282502, 0.32105005, ..., 0.4135    ,\n",
       "           0.41357502, 0.41482505],\n",
       "          [0.33920002, 0.32510003, 0.33370003, ..., 0.41935006,\n",
       "           0.42102504, 0.41340005]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.31812504, 0.30172503, 0.292625  , ..., 0.4124    ,\n",
       "           0.41375005, 0.41565004],\n",
       "          [0.33615   , 0.34730005, 0.314875  , ..., 0.41500002,\n",
       "           0.41492504, 0.41745004],\n",
       "          [0.33217502, 0.31750005, 0.32645005, ..., 0.41925007,\n",
       "           0.42092508, 0.414375  ]]],\n",
       "\n",
       "\n",
       "        [[[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43787146, 0.43630004, 0.42378575, ..., 0.4043    ,\n",
       "           0.4025    , 0.40005723],\n",
       "          [0.43700004, 0.43400005, 0.43538573, ..., 0.40484288,\n",
       "           0.40118578, 0.4003572 ],\n",
       "          [0.43800002, 0.43244293, 0.42710003, ..., 0.40242863,\n",
       "           0.38717148, 0.38231435]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.4364286 , 0.42460003, ..., 0.40764287,\n",
       "           0.40365714, 0.40121433],\n",
       "          [0.43700004, 0.43400005, 0.43617147, ..., 0.40575716,\n",
       "           0.40115717, 0.40032864],\n",
       "          [0.43800002, 0.43258575, 0.4273715 , ..., 0.40260002,\n",
       "           0.38801432, 0.38302866]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.43800002, 0.43900007, 0.4341143 , ..., 0.4094286 ,\n",
       "           0.40420005, 0.4025286 ],\n",
       "          [0.43700004, 0.43400005, 0.4383572 , ..., 0.40585715,\n",
       "           0.40112862, 0.40100002],\n",
       "          [0.43785718, 0.4333572 , 0.42838573, ..., 0.40335718,\n",
       "           0.39001432, 0.3830286 ]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.40245003, 0.3934    , 0.37677503, ..., 0.40912503,\n",
       "           0.40790004, 0.40375006],\n",
       "          [0.40307504, 0.40682507, 0.39787507, ..., 0.40737504,\n",
       "           0.40462506, 0.40537503],\n",
       "          [0.41155005, 0.40892506, 0.41272503, ..., 0.40607503,\n",
       "           0.40435004, 0.404875  ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.40025002, 0.38940004, 0.36957502, ..., 0.40507507,\n",
       "           0.40467504, 0.40067506],\n",
       "          [0.40030003, 0.4045    , 0.39255   , ..., 0.40372503,\n",
       "           0.4013    , 0.40260005],\n",
       "          [0.40930003, 0.40612504, 0.41067502, ..., 0.40095004,\n",
       "           0.398425  , 0.40142506]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "           1.        , 1.        ],\n",
       "          ...,\n",
       "          [0.39715004, 0.38402504, 0.36132503, ..., 0.40027505,\n",
       "           0.40015003, 0.39557505],\n",
       "          [0.39685005, 0.40227503, 0.38610002, ..., 0.39995003,\n",
       "           0.39692503, 0.39747503],\n",
       "          [0.40614998, 0.40240002, 0.40712503, ..., 0.39492503,\n",
       "           0.39100006, 0.39662504]]]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.RZSM.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e3c27c-7cda-45e4-9ce9-7748bd739ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "global start_date, end_date, dates\n",
    "#dates\n",
    "#GEFS long-term (multi-ensemble) forecasts are only initialized on Wednesdays\n",
    "start_date = dt.date(2000, 1, 1)\n",
    "\n",
    "#Actual end date (only data through that period for this website https://noaa-gefs-retrospective.s3.amazonaws.com/index.html#GEFSv12/reforecast/)\n",
    "end_date = dt.date(2019, 12, 31)\n",
    "\n",
    "dates = [start_date + dt.timedelta(days=d) for d in range(0, end_date.toordinal() - start_date.toordinal() + 1)]\n",
    "\n",
    "#from date time, Wednesday is a 2. (Monday is a 0) https://docs.python.org/3/library/datetime.html#datetime.datetime.weekday\n",
    "\n",
    "dates = [i for i in dates if i.weekday() ==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "856781d5-cbda-45dc-9ad6-89d0ac6460ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_average_of_ensembles(var,var_name,open_d10,open_d35,template_GEFS_initial,ensemble_number):\n",
    "\n",
    "    #take the average of the different lead dates, but not for tmax and tmin\n",
    "    if (var == 'tmax_2m') or (var =='tmin_2m'):\n",
    "     #for tmax and tmin\n",
    "     #Only find the maximum and minimum of each day. Don't take the averages\n",
    "     start_ = 0\n",
    "     steps = {}\n",
    "     \n",
    "     #they ordered the days terribly. There are 6 timesteps for each day for d10 files. Except the last timestep which actually belongs to the first timestep of d35\n",
    "     for i in range(len_leads):\n",
    "         if i ==0:\n",
    "             steps[i] = open_d10[f'{var_name}'][start_:start_+7,:,:].max(dim=step_name).values\n",
    "             start_+=8 #needed to begin the next index to keep up with proper dates\n",
    "         elif i<10:\n",
    "             steps[i] = open_d10[f'{var_name}'][start_:start_+7,:,:].max(dim=step_name).values #eight total possible values until last time step\n",
    "             start_+= 8 #Need to add one because we don't want to re-index the same day\n",
    "         elif i == 10:\n",
    "             # break\n",
    "             try:\n",
    "                 #Need to take from the first file (time 00:00:00), and combine with d35 files\n",
    "                 #find the max of 4 different dates\n",
    "                 #make a new array\n",
    "                 new_array = np.empty(shape=(4,open_d10[f'{var_name}'].shape[1],open_d10[f'{var_name}'].shape[2]))\n",
    "                 new_array[0,:,:] = open_d10[f'{var_name}'][-1,:,:]\n",
    "                 new_array[1,:,:] = open_d35[f'{var_name}'][0,:,:]\n",
    "                 new_array[2,:,:] = open_d35[f'{var_name}'][1,:,:] \n",
    "                 new_array[3,:,:] = open_d35[f'{var_name}'][2,:,:] \n",
    "                     \n",
    "                 s1 = np.nanmax(new_array,axis=0)\n",
    "                 steps[i] = s1\n",
    "                 start_ = 3 #start count over, 4th file is the new date in d35 files\n",
    "             except IndexError:\n",
    "                 pass\n",
    "                 #Some ensembles have broken members\n",
    "         elif i <=34:\n",
    "             steps[i] = open_d35[f'{var_name}'][start_:start_+4,:,:].max(dim=step_name).values\n",
    "             start_+=4\n",
    "     #Add to file\n",
    "     for step,lead_day in enumerate(steps.keys()):\n",
    "         template_GEFS_initial[:,ensemble_number,step,:,:] = steps[lead_day]\n",
    "         \n",
    "    else: \n",
    "        start_ = 0\n",
    "        steps = {}\n",
    "        for i in range(len_leads):\n",
    "            # break\n",
    "            if i ==0:\n",
    "                steps[i] = open_d10[f'{var_name}'][start_:start_+7,:,:].mean(dim=step_name).values\n",
    "                start_+=8 #needed to begin the next index to keep up with proper dates\n",
    "            elif i<10:\n",
    "                steps[i] = open_d10[f'{var_name}'][start_:start_+7,:,:].mean(dim=step_name).values #eight total possible values until last time step\n",
    "                start_+= 8 #Need to add one because we don't want to re-index the same day\n",
    "            elif i == 10:\n",
    "                #Need to take from the first file (time 00:00:00), and combine with d35 files\n",
    "                s1 = (open_d10[f'{var_name}'][-1,:,:] + open_d35[f'{var_name}'][0,:,:] + \\\n",
    "                    open_d35[f'{var_name}'][1,:,:] + open_d35[f'{var_name}'][2,:,:]) /4\n",
    "                steps[i] = s1\n",
    "                start_ = 3 #start count over, 4th file is the new date in d35 files\n",
    "            elif i <=34:\n",
    "                steps[i] = open_d35[f'{var_name}'][start_:start_+4,:,:].mean(dim=step_name).values\n",
    "                start_+=4\n",
    "        #Add to file\n",
    "        for step,lead_day in enumerate(steps.keys()):\n",
    "            template_GEFS_initial[:,ensemble_number,step,:,:] = steps[lead_day]         \n",
    "            \n",
    "      \n",
    "    return(template_GEFS_initial[:,:,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee71110-d5e7-4859-a95a-ad4853750234",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def return_xarray_file(var, template_GEFS_initial, julian_list, _date, open_d10):\n",
    "    if 'dlwrf' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                dlwrf = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Downwelling longwave radiation GEFSv12. Daily average already computed. All ensembles and Ls in one file'),\n",
    "        )  \n",
    "        \n",
    "    if 'pwat_eatm' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                dlwrf = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Precipitable water GEFSv12. Daily average already computed. All ensembles and Ls in one file'),\n",
    "        )  \n",
    "        \n",
    "    elif 'dswrf' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                dswrf = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Downwelling shortwave radiation GEFSv12. Daily average already computed. All ensembles and Ls in one file'),\n",
    "        )  \n",
    "    elif 'soil' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                RZSM = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "    \n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Volumetric soil moisture content 0-100cm: 0.0-0.1, 0.1-0.4, 0.4-1.0 and 1.-2. m depth \\\n",
    "    (fraction between wilting and saturation) GEFSv12. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'tmp' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                tmean = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Average temperature GEFSv12. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'ulwrf' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                ulwrf = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Longwave upwelling radiation. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'uswrf' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                uswrf = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Shortwave upwelling radiation. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'spfh' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                spfh = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Specific humidity. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'tmax' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                tasmax = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Maximum Temperature. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'tmin' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                tasmin = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Minimum Temperature. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'uflx' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                uas = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'U component of wind, N/m2 momentum flux. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'vflx' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                vas = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'V component of wind, N/m2 momentum flux. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif var == 'hgt_pres':\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                z = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Geopotential height. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    \n",
    "        # GEFS_out = GEFS_out.sel()\n",
    "    elif 'apcp_sfc' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                pr = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Precipitation. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "        GEFS_out2 = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                pr = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = np.arange(35),\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Precipitation. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'ugrd' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                uas = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'U component of wind, m/2. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "    elif 'vgrd' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                vas = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'V component of wind, m/2. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "        \n",
    "    elif 'lhtfl' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                latentHeat = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Surface Latent heat, W/m^2. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "        \n",
    "    elif 'shtfl' in var:\n",
    "        GEFS_out = xr.Dataset(\n",
    "            data_vars = dict(\n",
    "                sensibleHeat = (['S','M','L','Y','X'], template_GEFS_initial[:,:,:,:,:]),\n",
    "            ),\n",
    "            coords = dict(\n",
    "              \n",
    "                X = open_d10.longitude.values,\n",
    "                Y = open_d10.latitude.values,\n",
    "                L = julian_list,\n",
    "                M = range(template_GEFS_initial.shape[1]),\n",
    "                S = np.atleast_1d(pd.to_datetime(_date)),\n",
    "            ),\n",
    "            attrs = dict(\n",
    "                Description = 'Surface Sensible heat, W/m^2. Daily average already computed. All ensembles and Ls in one file')\n",
    "        )\n",
    "\n",
    "    return(GEFS_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4b0437f-aef4-476a-9dcf-41373c96c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ensemble_members(_date):\n",
    "    #testing\n",
    "    # _date = pd.to_datetime('2004-03-03')\n",
    "    # region_name='CONUS'\n",
    "    # var = 'hgt_pres'\n",
    "    \n",
    "    \n",
    "    global len_leads, step_name\n",
    "    len_leads = 35\n",
    "    \n",
    "    #After manually inspecting the files after using NCL operators, the \"step\" has been replaced by \"forecast_time0\"\n",
    "    step_name = 'forecast_time0'\n",
    "            \n",
    "    \n",
    "    for region_name in region_names:\n",
    "        for var in var_names:\n",
    "            \n",
    "            if region_name == 'CONUS':\n",
    "                save_dir = '/glade/work/klesinger/FD_RZSM_deep_learning/Data/GEFSv12_reforecast'\n",
    "                source_dir = f'/glade/derecho/scratch/klesinger/GEFSv12_raw/{var}/{var}_processed'\n",
    "                lat_ = 200 #you must manually know the size of the input file\n",
    "                lon_ = 380\n",
    "            elif region_name == 'australia':\n",
    "                save_dir = '/glade/work/klesinger/FD_RZSM_deep_learning/Data_australia/GEFSv12_reforecast'\n",
    "                source_dir = f'/glade/derecho/scratch/klesinger/GEFSv12_raw/{var}/{var}_processed_australia'\n",
    "                lat_ = 48 #you must manually know the size of the input file\n",
    "                lon_ = 96\n",
    "            elif region_name == 'china':\n",
    "                save_dir = '/glade/work/klesinger/FD_RZSM_deep_learning/Data_china/GEFSv12_reforecast'\n",
    "                source_dir = f'/glade/derecho/scratch/klesinger/GEFSv12_raw/{var}/{var}_processed_china'\n",
    "                lat_ = 48 #you must manually know the size of the input file\n",
    "                lon_ = 96        \n",
    "            \n",
    "            \n",
    "            os.system(f'mkdir -p {save_dir}/{var}')\n",
    "            os.chdir(f'{source_dir}')\n",
    "            \n",
    "            #For soil layer depth\n",
    "            global soil_layer_depth, weighted_RZSM\n",
    "            soil_layer_depth=3 #0-100cm. Can do 0-2m if number =4\n",
    "            \n",
    "            weighted_RZSM=True #weighted sum of the individual layers (we only have it set to 3 layers, you must modify the code if soil_layer_depth != 3)\n",
    "\n",
    "            os.system(f'mkdir -p {save_dir}')\n",
    "        \n",
    "            def name(file):\n",
    "                return(list(file.keys())[0])\n",
    "        \n",
    "            len_steps_d10 = 80 #I checked the good files and this is how many steps it should have\n",
    "            len_steps_d35 = 100 #I checked the good files and this is how many steps it should have\n",
    "            \n",
    "\n",
    "            '''Need to create a new xarray template with 11 ensemble members, 35 lead days, lat/lon same as GMAO \n",
    "            (because file is already regridded to be in the same format as all other observations and GMAO '''\n",
    "        \n",
    "            #This template was created from the code above\n",
    "            template_GEFS_initial = np.empty(shape=(1,11,len_leads,lat_,lon_))\n",
    "            lead_splices = ['d10','d35']\n",
    "        \n",
    "            all_possible_ensemble_members = ['c00', 'p01', 'p02', 'p03', 'p04', 'p05', 'p06', 'p07', 'p08', 'p09', 'p10']\n",
    "            #testing with one at a time\n",
    "            # vars_to_process = ['soilw_bgrnd']\n",
    "        \n",
    "            #Get the dates of the files\n",
    "        \n",
    "            # _date=dates[0]\n",
    "    \n",
    "            #The date of the file is when it was intialized\n",
    "            out_date_create = pd.to_datetime(_date) \n",
    "            out_date = f'{out_date_create.year}-{out_date_create.month:02}-{out_date_create.day:02}'\n",
    "                \n",
    "            final_out_name = f'{var}_EMC_{out_date}.nc'\n",
    "    \n",
    "            if os.path.exists(f\"{save_dir}/{var}/{final_out_name}\"):\n",
    "                pass\n",
    "            else:\n",
    "                            \n",
    "                print(f'Working on variable {var} to merge ensemble members for date {_date}.')\n",
    "    \n",
    "                '''Now combine the files, because there are some missing ensemble members (not sure why)\n",
    "                we need to account for files with 1.) ALL members, 2.) some members\n",
    "        \n",
    "                It gets more complicated when the missing ensemble members are different between each\n",
    "                d10 (first 10 days) and d35 (last 25 days), but I have figured it out'''\n",
    "                \n",
    "                template_GEFS_initial[:,:,:,:,:] = np.nan\n",
    "                # lead_day=0 #keeps up with which index is correct in template_GEFS_initial (resets with each date)\n",
    "    \n",
    "                all_files_d10 = sorted(glob(f'{lead_splices[0]}_{var}_{_date.year}{_date.month:02}{_date.day:02}*.n*'))\n",
    "                all_files_d35 = sorted(glob(f'{lead_splices[1]}_{var}_{_date.year}{_date.month:02}{_date.day:02}*.n*'))\n",
    "    \n",
    "                good_files_d10 = []\n",
    "                good_files_d35 = []\n",
    "\n",
    "                #Save name template for later\n",
    "                \n",
    "                \n",
    "                #Some files don't have all the steps, so just remove the bad files from processing (they are going to be replaced later by a different realization)\n",
    "                for i in all_files_d10:\n",
    "                    op = xr.open_dataset(i)\n",
    "                    op.close()\n",
    "                    if len(op[step_name].values) == len_steps_d10: \n",
    "                        good_files_d10.append(i)\n",
    "                        \n",
    "                for i in all_files_d35:\n",
    "                    op = xr.open_dataset(i)\n",
    "                    op.close()\n",
    "                    if len(op[step_name].values) == len_steps_d35: \n",
    "                        good_files_d35.append(i)            \n",
    "\n",
    "                tem = good_files_d10[0].split('_')\n",
    "                \n",
    "                #some files have doubles (rsync error or from HPC when converting)\n",
    "                file_len = [len(i) for i in good_files_d10]\n",
    "                \n",
    "                mode = max(set(file_len), key=file_len.count)\n",
    "                #Replace files\n",
    "                good_files_d10 = [i for i in good_files_d10 if len(i) == mode]\n",
    "                good_files_d35 = [i for i in good_files_d35 if len(i) == mode]\n",
    "                  \n",
    "                    \n",
    "                #TODO:If all realizations are present\n",
    "                if (len(good_files_d10) == 11) and (len(good_files_d35) == 11):\n",
    "                    #If all possible files are there, then this is the easy code processing to add data to single file\n",
    "                    for ensemble_number,files in enumerate(zip(good_files_d10,good_files_d35)):\n",
    "                         # break\n",
    "                        if var != 'soilw_bgrnd' and var != 'hgt_pres':\n",
    "                            open_d10 = xr.open_dataset(files[0])\n",
    "                            open_d35 = xr.open_dataset(files[1])\n",
    "                            try:\n",
    "                                var_name = [i for i in list(open_d10.keys()) if 'step' not in i][0]\n",
    "                            except IndexError:\n",
    "                                pass\n",
    "                                \n",
    "                        elif var == 'hgt_pres':\n",
    "                        \n",
    "                            open_d10 = xr.open_dataset(files[0])\n",
    "                            open_d35 = xr.open_dataset(files[1])\n",
    "    \n",
    "                            #If you have all the values in a file, you can subset here. \n",
    "                            # open_d10 = xr.open_dataset(files[0]).sel(isobaricInhPa=hgt_pressure_level)\n",
    "                            # open_d35 = xr.open_dataset(files[1]).sel(isobaricInhPa=hgt_pressure_level)\n",
    "    \n",
    "                            var_name = [i for i in list(open_d10.keys()) if 'step' not in i][0]\n",
    "                            \n",
    "        \n",
    "                                \n",
    "                        elif var == 'soilw_bgrnd':\n",
    "                            #Take the sum of the columns\n",
    "                            open_d10= xr.open_dataset(files[0])\n",
    "                            open_d35 = xr.open_dataset(files[1])\n",
    "                            var_name = [i for i in list(open_d10.keys()) if 'step' not in i][0]\n",
    "                            #TODO: Take the summation of the first 3 soil layers (0-100cm)\n",
    "                            if weighted_RZSM == False:\n",
    "                                open_d10 = open_d10[f'{var_name}'][:,0:soil_layer_depth,:,:].sum(dim=['SOILW_P1_2L106_GLL0']).to_dataset()\n",
    "                                open_d35 = open_d35[f'{var_name}'][:,0:soil_layer_depth,:,:].sum(dim=['SOILW_P1_2L106_GLL0']).to_dataset()\n",
    "                            else:\n",
    "                                # break\n",
    "                                #take weighted mean by layer\n",
    "                                open_d10 = (np.multiply(open_d10[f'{var_name}'][:,0,:,:],0.1) + np.multiply(open_d10[f'{var_name}'][:,1,:,:],0.3) + np.multiply(open_d10[f'{var_name}'][:,1,:,:],0.6)).to_dataset()\n",
    "                                open_d35 = (np.multiply(open_d35[f'{var_name}'][:,0,:,:],0.1) + np.multiply(open_d35[f'{var_name}'][:,1,:,:],0.3) + np.multiply(open_d35[f'{var_name}'][:,1,:,:],0.6)).to_dataset()\n",
    "    \n",
    "                        '''EMC has some broken files where there is only 1 time spot when it should have 35'''\n",
    "                        try:\n",
    "                            open_d10[f'{step_name}'].values\n",
    "                            open_d35[f'{step_name}'].values\n",
    "                            \n",
    "                            if len(open_d10[f'{step_name}'].values) == 0 or len(open_d35[f'{step_name}'].values) == 0:\n",
    "                                pass_=True\n",
    "                            else:\n",
    "                                pass_=False\n",
    "                                \n",
    "                        except AttributeError:\n",
    "                            #no steps in file, just a single file (equals a bad file)\n",
    "                            pass_=True\n",
    "                        ##########################################################\n",
    "                        if pass_==True:\n",
    "                            pass\n",
    "                        elif len(open_d10[f'{step_name}'].values)==1 or len(open_d35[f'{step_name}'].values)==1:\n",
    "                            pass\n",
    "                        else:\n",
    "                        #First get the dates of the files\n",
    "                            '''Take average of first 7 timesteps if d10 file. I have verified\n",
    "                            this is correct when looking at HPC'''\n",
    "                            template_GEFS_initial[:,:,:,:,:] = return_average_of_ensembles(var=var,var_name=var_name,open_d10=open_d10,open_d35=open_d35,template_GEFS_initial=template_GEFS_initial[:,:,:,:,:],ensemble_number=ensemble_number)    \n",
    "                            \n",
    "                            \n",
    "                #If all ensembles are missing, do nothing\n",
    "                elif (len(good_files_d10) == 0 )and (len(good_files_d35) == 0):\n",
    "                    # print(_date)\n",
    "                    pass\n",
    "                \n",
    "                #If there are a differnet number of ensembles between leads\n",
    "                elif (len(good_files_d10) != 11) or (len(good_files_d35) != 11):\n",
    "    \n",
    "                    #Some ensembles are missing, split to get the name of ensemble members\n",
    "                    avail_ensemble_members_d10 = [i.split('_')[-1].split('.')[0] for i in good_files_d10]\n",
    "                    avail_ensemble_members_d35 = [i.split('_')[-1].split('.')[0] for i in good_files_d35]\n",
    "\n",
    "                    missing_d10_ = sorted(list(set(all_possible_ensemble_members).difference(set(avail_ensemble_members_d10))))\n",
    "                    missing_d35_ = sorted(list(set(all_possible_ensemble_members).difference(set(avail_ensemble_members_d35))))\n",
    "                    \n",
    "                    #if missing only the exact same realizations                \n",
    "                    if (len(missing_d35_) ==  len(missing_d10_)) and (missing_d35_==missing_d10_):\n",
    "                        #Find a way to append the missing ensemble files with np.nan\n",
    "                        \n",
    "                        for idx,ensemble in enumerate(all_possible_ensemble_members):\n",
    "                            \n",
    "                            if ensemble not in avail_ensemble_members_d10:\n",
    "                                pass\n",
    "                                #just keep the data as np.nan\n",
    "                            else:\n",
    "                                idx_num = avail_ensemble_members_d10.index(ensemble)\n",
    "                                open_d10=xr.open_dataset(good_files_d10[idx_num])\n",
    "                                open_d35=xr.open_dataset(good_files_d35[idx_num])\n",
    "                                \n",
    "                                if var == 'hgt_pres':\n",
    "                                    open_d10 = xr.open_dataset(files[0])\n",
    "                                    open_d35 = xr.open_dataset(files[1])\n",
    "            \n",
    "                                    #If you have all the values in a file, you can subset here. \n",
    "                                    # open_d10 = xr.open_dataset(files[0]).sel(isobaricInhPa=hgt_pressure_level)\n",
    "                                    # open_d35 = xr.open_dataset(files[1]).sel(isobaricInhPa=hgt_pressure_level)\n",
    "                                elif var == 'soilw_bgrnd':\n",
    "                                    if weighted_RZSM == False:\n",
    "                                        open_d10 = open_d10[f'{var_name}'][:,0:soil_layer_depth,:,:].sum(dim=['SOILW_P1_2L106_GLL0']).to_dataset()\n",
    "                                        open_d35 = open_d35[f'{var_name}'][:,0:soil_layer_depth,:,:].sum(dim=['SOILW_P1_2L106_GLL0']).to_dataset()\n",
    "                                    else:\n",
    "                                        open_d10 = (np.multiply(open_d10[f'{var_name}'][:,0,:,:],0.1) + np.multiply(open_d10[f'{var_name}'][:,1,:,:],0.3) + np.multiply(open_d10[f'{var_name}'][:,1,:,:],0.6)).to_dataset()\n",
    "                                        open_d35 = (np.multiply(open_d35[f'{var_name}'][:,0,:,:],0.1) + np.multiply(open_d35[f'{var_name}'][:,1,:,:],0.3) + np.multiply(open_d35[f'{var_name}'][:,1,:,:],0.6)).to_dataset()\n",
    "    \n",
    "                                else:\n",
    "                                    open_d10=xr.open_dataset(good_files_d10[idx_num])\n",
    "                                    open_d35=xr.open_dataset(good_files_d35[idx_num]) \n",
    "                                \n",
    "                                var_name = [i for i in list(open_d10.keys()) if 'step' not in i][0]\n",
    "                                \n",
    "    \n",
    "                                '''Take average of first 7 timesteps if d10 file. I have verified\n",
    "                                this is correct when looking at HPC'''\n",
    "                                template_GEFS_initial = return_average_of_ensembles(var=var,var_name=var_name,open_d10=open_d10,open_d35=open_d35,template_GEFS_initial=template_GEFS_initial,ensemble_number=idx)    \n",
    "    \n",
    "                    # missing different realizations \n",
    "                    else:\n",
    "                         #Because there are missing ensemble members that are supposed to be aligned,\n",
    "                         #we must delete those ensemble members\n",
    "            \n",
    "                         # #remove missing members\n",
    "                         # out_10 = [i for i in all_possible_ensemble_members if i not in missing_d10_]\n",
    "                         # out_35 = [i for i in all_possible_ensemble_members if i not in missing_d35_]\n",
    "                        \n",
    "                         #replace empty file with the control file. Deep learning doesn't like np.nan values\n",
    "                         #there are so few of these missing files that it should be fine\n",
    "                         for i in good_files_d10:\n",
    "                             #for each set of files\n",
    "                             for m in all_possible_ensemble_members:\n",
    "                                 name_out = f'd10_{var}_{tem[3]}_{m}.nc'\n",
    "                                 \n",
    "                                 if name_out in good_files_d10:\n",
    "                                     pass\n",
    "                                 #for each set of members\n",
    "                                 #we need to see if the file exists, if not create a blank one\n",
    "                                 else:\n",
    "                                     # break\n",
    "                                     temp_10=xr.open_dataset(good_files_d10[0]) #make a temporary file as the blank file\n",
    "                                     # temp_10[name(temp_10)][:,:,:] = np.nan\n",
    "                                     temp_10.to_netcdf(f'd10_{i[4:24]}{m}.nc')\n",
    "                                 \n",
    "                         #replace empty file with the control file. Deep learning doesn't like np.nan values\n",
    "                         #there are so few of these missing files that it should be fine\n",
    "                         for i in good_files_d35:\n",
    "                             # print(i)\n",
    "                             #for each set of files\n",
    "                             for m in all_possible_ensemble_members:\n",
    "                                 # print(m)\n",
    "                                 name_out = f'd35_{var}_{tem[3]}_{m}.nc'\n",
    "                                 if name_out in good_files_d35:\n",
    "                                     pass\n",
    "                                 #for each set of members\n",
    "                                 #we need to see if the file exists, if not create a blank one\n",
    "                                 else:\n",
    "                                     temp_10=xr.open_dataset(all_files_d35[0]) #make a temporary file as the blank file\n",
    "                                     # temp_10[name(temp_10)][:,:,:] = np.nan\n",
    "                                     temp_10.to_netcdf(f'd35_{i[4:24]}{m}.nc')\n",
    "                         #'''Take average of first 7 timesteps if d10 file. I have verified\n",
    "                         #this is correct when looking at HPC'''\n",
    "                         \n",
    "                         for idx,ensemble in enumerate(all_possible_ensemble_members):\n",
    "                             name_out_d10 = f'd10_{var}_{tem[3]}_{ensemble}.nc'\n",
    "                             name_out_d35 = f'd35_{var}_{tem[3]}_{ensemble}.nc'\n",
    "                             \n",
    "                             # idx_num = avail_ensemble_members_d10.index(ensemble)\n",
    "                             open_d10=xr.open_dataset(name_out_d10)\n",
    "                             open_d35=xr.open_dataset(name_out_d35)\n",
    "                             var_name = [i for i in list(open_d10.keys()) if 'step' not in i][0]\n",
    "                             \n",
    "                             template_GEFS_initial[:,:,:,:,:]  = return_average_of_ensembles(var=var,var_name=var_name,open_d10=open_d10,open_d35=open_d35,template_GEFS_initial=template_GEFS_initial,ensemble_number=idx)    \n",
    "               \n",
    "    \n",
    "                def julian_date(_date,template_GEFS_initial):\n",
    "                    #Return julian date for anomaly calculation\n",
    "                    a_date_in= template_GEFS_initial.shape[2]\n",
    "                    #get the start date\n",
    "                    a_start_date = pd.to_datetime(_date) \n",
    "        \n",
    "                    a_date_out=[]\n",
    "                    for a_i in range(a_date_in):\n",
    "                        a_date_out.append((a_start_date + np.timedelta64(a_i,'D')).timetuple().tm_yday)\n",
    "            \n",
    "                    return(a_date_out)\n",
    "    \n",
    "                #Can specifically add the julian date if you want.\n",
    "                # julian_list = julian_date(_date,template_GEFS_initial)\n",
    "                #Instead of replacing the below lines, lets just make it a 35 day lead\n",
    "    \n",
    "                #This is just the number of leads\n",
    "                julian_list=np.arange(len_leads)\n",
    "    \n",
    "                GEFS_out = return_xarray_file(var, template_GEFS_initial, julian_list, _date, open_d10)\n",
    "                GEFS_out = GEFS_out.astype(np.float32)\n",
    "                \n",
    "                GEFS_out.to_netcdf(path = f\"{save_dir}/{var}/{final_out_name}\")\n",
    "                GEFS_out.close()\n",
    "\n",
    "    return(0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e492bed-470a-4f95-a557-a98301efe1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on variable soilw_bgrnd to merge ensemble members for date 2003-02-12.\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2004-02-25.Working on variable soilw_bgrnd to merge ensemble members for date 2002-08-07.Working on variable soilw_bgrnd to merge ensemble members for date 2000-01-05.Working on variable soilw_bgrnd to merge ensemble members for date 2001-01-17.Working on variable soilw_bgrnd to merge ensemble members for date 2004-09-01.Working on variable soilw_bgrnd to merge ensemble members for date 2001-07-25.\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2003-08-20.\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2002-01-30.Working on variable soilw_bgrnd to merge ensemble members for date 2000-07-12.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Working on variable hgt_pres to merge ensemble members for date 2001-01-17.Working on variable hgt_pres to merge ensemble members for date 2004-02-25.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2002-08-07.Working on variable hgt_pres to merge ensemble members for date 2003-02-12.\n",
      "\n",
      "\n",
      "Working on variable hgt_pres to merge ensemble members for date 2003-08-20.Working on variable hgt_pres to merge ensemble members for date 2002-01-30.\n",
      "\n",
      "Working on variable hgt_pres to merge ensemble members for date 2000-07-12.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2004-09-01.Working on variable hgt_pres to merge ensemble members for date 2001-07-25.\n",
      "\n",
      "Working on variable hgt_pres to merge ensemble members for date 2000-01-05.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2001-01-17.Working on variable pwat_eatm to merge ensemble members for date 2003-02-12.\n",
      "\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2002-08-07.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2004-02-25.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2003-08-20.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2002-01-30.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2001-07-25.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2000-01-05.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2000-07-12.\n",
      "Working on variable pwat_eatm to merge ensemble members for date 2004-09-01.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2003-02-12.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2001-01-17.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2004-02-25.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2002-08-07.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2003-08-20.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2002-01-30.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2000-01-05.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2000-07-12.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2004-09-01.\n",
      "Working on variable tmin_2m to merge ensemble members for date 2001-07-25.\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2005-03-09.Working on variable soilw_bgrnd to merge ensemble members for date 2005-09-14.Working on variable soilw_bgrnd to merge ensemble members for date 2006-09-27.\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2006-03-22.\n",
      "\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2007-04-04.\n",
      "\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2007-10-10.\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2008-10-22.Working on variable soilw_bgrnd to merge ensemble members for date 2008-04-16.Working on variable soilw_bgrnd to merge ensemble members for date 2009-04-29.\n",
      "\n",
      "\n",
      "Working on variable soilw_bgrnd to merge ensemble members for date 2009-11-04.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2006-09-27.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2005-09-14.Working on variable hgt_pres to merge ensemble members for date 2007-04-04.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2005-03-09.\n",
      "\n",
      "Working on variable hgt_pres to merge ensemble members for date 2007-10-10.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2006-03-22.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2009-04-29.\n",
      "Working on variable hgt_pres to merge ensemble members for date 2008-10-22.Working on variable hgt_pres to merge ensemble members for date 2008-04-16.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    p = Pool(10)\n",
    "    p.map(merge_ensemble_members,dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460be60-7e3d-4b77-a948-21bcc8a1adf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671128d-1c4c-4c6f-89ab-3a33a6a4e849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf210gpu]",
   "language": "python",
   "name": "conda-env-tf210gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
