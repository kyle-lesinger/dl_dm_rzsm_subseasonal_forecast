{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ee5de7-0e18-43c3-aac9-056533042c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from glob import glob\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask\n",
    "from functools import partial\n",
    "\n",
    "#Must do this to ensure we can find the library\n",
    "os.system(f'export ESMFMKFILE=/glade/work/klesinger/conda-envs/tf212gpu/lib/esmf.mk')\n",
    "\n",
    "import xesmf as xe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3bd727a-16aa-4228-b705-2221c4c827ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xarray_varname(file: xr.DataArray) -> str:\n",
    "    #grabs the first xarray variable, just easier with a function. But only works if you have 1 varaible in the xarray file\n",
    "    return(list(file.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3b17f54-9a50-408e-9345-2f22863b14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "global dir_base,save_dir_base\n",
    "dir_base = f'/glade/derecho/scratch/klesinger/FD_RZSM_deep_learning/Data/raw_downloads/ECMWF'\n",
    "save_dir_base = f'/glade/derecho/scratch/klesinger/FD_RZSM_deep_learning/Data/reforecast/ECMWF'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b294f-8801-40f2-b258-6d6e1ac863da",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## We need to open each file using xarray and cfgrib\n",
    "\n",
    "## Then we need to save the file so that CDO operators can regrid --- then regrid the saved file\n",
    "\n",
    "## Then we need to convert RZSM from kg/m3 to m3/m3 by dividing by 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e85873-65d8-4188-be7d-afb9628de3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_var_info(var,region_name):\n",
    "    \n",
    "    #First create the daily data that we want\n",
    "    file_source = f'{dir_base}/{var}'\n",
    "    \n",
    "    global control_,perturbed_\n",
    "    control_ = sorted(glob(f'{file_source}/*control*nc'))\n",
    "    perturbed_ = sorted(glob(f'{file_source}/*perturbed*nc'))\n",
    "    \n",
    "    date_list = [i.split('/')[-1].split('_')[2] for i in control_]\n",
    "    \n",
    "    global save_dir\n",
    "    save_dir = f'{save_dir_base}/{region_name}/{var}\"\n",
    "    \n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    return(control_,perturbed_,date_list, save_dir)\n",
    "\n",
    "def return_control_info(var):\n",
    "    \n",
    "    #First create the daily data that we want\n",
    "    file_source = f'{dir_base}/{var}'\n",
    "    \n",
    "    global control_\n",
    "    control_ = sorted(glob(f'{file_source}/*control*nc'))\n",
    "\n",
    "    return(control_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d918657-9626-419a-9309-7159bfabe05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_control_info(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4885a90d-9f79-45e4-aa7f-16f5e854934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our GEFSv12 initialized dates\n",
    "\n",
    "'''We need to wait until GEFSv12 data has been processed to get the correct coordinates '''\n",
    "def return_init_info(region_name):\n",
    "\n",
    "    global gefs_init\n",
    "    \n",
    "    gefs_init = sorted(glob(f'/glade/derecho/scratch/klesinger/FD_RZSM_deep_learning/Data/reforecast/GEFSv12/{region_name}/soilw_bgrnd'))\n",
    "    \n",
    "    if region_name == 'CONUS':\n",
    "        gefs_init = sorted(glob('/glade/work/klesinger/FD_RZSM_deep_learning/Data/GEFSv12_reforecast/soilw_bgrnd/*'))\n",
    "    elif region_name == 'australia':\n",
    "        gefs_init = sorted(glob('/glade/work/klesinger/FD_RZSM_deep_learning/Data_australia/GEFSv12_reforecast/soilw_bgrnd/*'))\n",
    "    elif region_name == 'china':\n",
    "        gefs_init = sorted(glob('/glade/work/klesinger/FD_RZSM_deep_learning/Data_china/GEFSv12_reforecast/soilw_bgrnd/*'))\n",
    "    \n",
    "    gefs_dates = [i.split('/')[-1].split('_')[-1].split('.')[0] for i in gefs_init]\n",
    "    \n",
    "    #first \"date\" is a folder\n",
    "    gefs_dates = gefs_dates[1:]\n",
    "    \n",
    "    global template, lon, lat\n",
    "    template = xr.open_dataset(gefs_init[10]).load()\n",
    "    lon, lat = template.X.values, template.Y.values\n",
    "\n",
    "    return(template, lon, lat, gefs_dates)\n",
    "# gefs_dates.reverse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3705907-6d76-4d4e-8f82-c00e06eeca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have the proper init dates to re-create a new dataset for ECMWF (we are going to select the lead dates and make a new file exactly the same shape as GEFSv12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ba9531-a4a3-4bf7-812f-4dd7f445054a",
   "metadata": {},
   "source": [
    "# Need to make masks for bilinear interpolation or conservative to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7a453a-cd74-4208-9d85-0acc3f7eab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resave_regrid_merge_multiprocessing_added_mask_RZSM(file):\n",
    "    \n",
    "    for region_name in ['CONUS', 'australia', 'china']:\n",
    "        \n",
    "        template, lon, lat, gefs_dates = return_init_info(region_name)\n",
    "        \n",
    "        # file = control_[0]\n",
    "        len_of_lead = 45 #this is specific to ECMWF, i downloaded 45 days\n",
    "        \n",
    "        #Test \n",
    "        # file = control_[-30]\n",
    "        #file comes from the control_ file list\n",
    "        control_,perturbed_,date_list, save_dir = return_var_info(var,region_name)\n",
    "        \n",
    "        # break\n",
    "        save_name = f\"{save_dir}/{file.split('/')[-1]}\"\n",
    "        save_name = file.split('/')[-1].split('_control')\n",
    "        save_name = f'{save_dir}/{save_name[0]}{save_name[1]}'\n",
    "        \n",
    "        if os.path.exists(save_name):\n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            fill_this_file = template.copy(deep = True)\n",
    "            #First open the control file (divide by 1000 to convert to m3/m3)\n",
    "            ctl = xr.open_dataset(file,engine='cfgrib').astype(np.float32) / 1000\n",
    "    \n",
    "            \n",
    "    \n",
    "            ctl[\"mask\"] = xr.where(~np.isnan(ctl[\"sm100\"].isel(step=0)), 1, 0)\n",
    "            \n",
    "             # #Check ctl file \n",
    "            ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "            ctl.isel(step=0)['sm100'].plot.pcolormesh(ax=ax, vmin=0, vmax=1)\n",
    "            ax.coastlines()\n",
    "    \n",
    "            ctl[\"mask\"].plot(cmap=\"binary_r\")\n",
    "        \n",
    "        \n",
    "            #make a date and find perturbed file\n",
    "            find_date = file.split('/')[-1].split('_')[2]\n",
    "            perturbed_date =[i for i in perturbed_ if find_date in i]\n",
    "            \n",
    "            if len(perturbed_date) == 1:\n",
    "    \n",
    "                process_perturbed = xr.open_dataset(perturbed_date[0],engine='cfgrib')/ 1000\n",
    "    \n",
    "                # #Check perturbed file \n",
    "                # ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "                # process_perturbed.isel(step=0).isel(number=1)['sm100'].plot.pcolormesh(ax=ax, vmin=0, vmax=1)\n",
    "                # process_perturbed.isel(step=0)['sm100'].plot.pcolormesh(ax=ax, vmin=0, vmax=1)\n",
    "                # ax.coastlines()\n",
    "                \n",
    "                #keep all longitude values that are less than 90 (it's a weird way of doing it but our current mask for other data GEFSv12 is like this)\n",
    "                if region_name == 'CONUS':\n",
    "                    new_lon_values = [i if i < 90 else i-360 for i in ctl.longitude.values]\n",
    "                    high_res_grid = xr.open_dataset('masks/NH_grid.nc') #I manually created this. Didn't know how to structure it until I ran the code once then came back and added it.\n",
    "                    # high_res_grid = xr.open_dataset('masks/RZSM_weighted_mean_0_100cm.nc4')\n",
    "                    high_res_grid.close()\n",
    "                    # high_res_grid\n",
    "                elif region_name == 'australia':\n",
    "                    new_lon_values = ctl.longitude.values\n",
    "                    high_res_grid = xr.open_dataset('masks/australia_mask.nc4')\n",
    "                elif region_name == 'china':\n",
    "                    new_lon_values = ctl.longitude.values\n",
    "                    high_res_grid = xr.open_dataset('masks/china_mask.nc4')\n",
    "                \n",
    "                ctl = ctl.assign_coords({'longitude':new_lon_values})\n",
    "                process_perturbed = process_perturbed.assign_coords({'longitude':new_lon_values})\n",
    "    \n",
    "                if region_name == 'CONUS':\n",
    "                    ds_out = xr.Dataset(\n",
    "                        {\n",
    "                            \"latitude\": ([\"latitude\"], np.arange(template.Y.values[0], template.Y.values[-1]-0.5, -0.5), {\"units\": \"degrees_north\"}),\n",
    "                            \"longitude\": ([\"longitude\"], np.arange(template.X.values[0], template.X.values[-1]+0.5, 0.5), {\"units\": \"degrees_east\"}),\n",
    "                        }\n",
    "                    )\n",
    "                elif region_name == 'australia':\n",
    "                    ds_out = xr.Dataset(\n",
    "                        {\n",
    "                            \"latitude\": ([\"latitude\"], np.arange(template.Y.values[0], template.Y.values[-1]-0.5, -0.5), {\"units\": \"degrees_north\"}),\n",
    "                            \"longitude\": ([\"longitude\"], np.arange(template.X.values[0], template.X.values[-1]+0.5, 0.5), {\"units\": \"degrees_east\"}),\n",
    "                        }\n",
    "                    )\n",
    "                elif region_name == 'china':\n",
    "                    ds_out = xr.Dataset(\n",
    "                        {\n",
    "                            \"latitude\": ([\"latitude\"], np.arange(template.Y.values[0], template.Y.values[-1]-0.5, -0.5), {\"units\": \"degrees_north\"}),\n",
    "                            \"longitude\": ([\"longitude\"], np.arange(template.X.values[0], template.X.values[-1]+0.5, 0.5), {\"units\": \"degrees_east\"}),\n",
    "                        }\n",
    "                    )               \n",
    "                ds_out\n",
    "    \n",
    "                name_ = list(high_res_grid.keys())[0]\n",
    "    \n",
    "                if region_name == 'CONUS':\n",
    "                    high_res_grid['mask'] = xr.where(high_res_grid[name_] == 1,0,1)\n",
    "                    high_res_grid = high_res_grid.rename({'X':'longitude','Y':'latitude'})\n",
    "                elif region_name == 'australia':\n",
    "                    high_res_grid['mask'] = xr.where(np.isnan(high_res_grid[name_]),0,1)\n",
    "                    high_res_grid = high_res_grid.isel(time=0)\n",
    "                elif region_name == 'china':\n",
    "                    high_res_grid['mask'] = xr.where(np.isnan(high_res_grid[name_]),0,1)\n",
    "                    high_res_grid = high_res_grid.isel(time=0)\n",
    "                \n",
    "                high_res_grid[\"mask\"].plot(cmap=\"binary_r\")\n",
    "                \n",
    "                \n",
    "                #Cannot do bilinear, it produces no values along the coasts of the data for RZSM. Conservative works better\n",
    "                # regridder = xe.Regridder(ctl, high_res_grid, \"patch\")\n",
    "                # regridder = xe.Regridder(ctl, high_res_grid, \"bilinear\")\n",
    "                # conservative is the only way that we can get smooth edges around coasts\n",
    "                regridder = xe.Regridder(ctl, high_res_grid, \"conservative\")\n",
    "                \n",
    "                ctl_out = regridder(ctl, keep_attrs=True)\n",
    "                perturbed_out = regridder(process_perturbed, keep_attrs=True)\n",
    "    \n",
    "                # #Check perturbed file \n",
    "                ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "                perturbed_out.isel(step=0).isel(number=1)['sm100'].plot.pcolormesh(ax=ax, vmin=0, vmax=1)\n",
    "                # perturbed_out.isel(step=0)['sm100'].plot.pcolormesh(ax=ax, vmin=0, vmax=1)\n",
    "                ax.coastlines()\n",
    "    \n",
    "                #Now replace the template data with control and perturbed\n",
    "                fill_this_file = fill_this_file.assign_coords({'S':np.atleast_1d(pd.to_datetime(find_date))}).reindex(L= np.arange(len(ctl_out.step.values)), fill_value=np.nan) \n",
    "                \n",
    "                \n",
    "                fill_this_file.RZSM[:,0,0:len(ctl_out.step.values),:,:] = ctl_out[var].values \n",
    "                fill_this_file.RZSM[:,1:,0:len(perturbed_out.step.values),:,:] = perturbed_out[var].values\n",
    "    \n",
    "                #Now add a mask for the values that are a \"1\" which are the ocean/water bodies\n",
    "                fill_this_file = xr.where(np.isnan(fill_this_file), 1, fill_this_file)\n",
    "    \n",
    "                #Check final file \n",
    "                ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "                fill_this_file.isel(L=0).isel(M=1).isel(S=0)['RZSM'].plot.pcolormesh(ax=ax, vmin=0, vmax=1)\n",
    "                ax.coastlines()\n",
    "                #Save file\n",
    "                fill_this_file.to_netcdf(save_name)\n",
    "    \n",
    "                #Now regrid using cdo\n",
    "            \n",
    "            else:\n",
    "                print(f'Could not find a perturbed file for {find_date}')\n",
    "                pass\n",
    "    \n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "#Then save as a netcdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f694fe-6255-458d-9c66-a7f3e08d47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'soilw_bgrnd'\n",
    "file_list = return_control_info(var)\n",
    "# file=file_list[0] # for testing\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p=Pool(20)\n",
    "    p.map(resave_regrid_merge_multiprocessing_added_mask_RZSM, file_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af7ece-1940-4c48-b6b8-14277108c48c",
   "metadata": {},
   "source": [
    "# NOw we need to make a new function for the other variables (tmax, tmin, specific humidity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195f14f-d2b3-4922-bc38-09f5da253e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global var_full\n",
    "var_full = 'temp_pwat_dewpoint'\n",
    "other_var_file_list = return_control_info(var_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5653db2-d79f-42d1-8b92-970056f3f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = xr.open_dataset(other_var_file_list[0],engine='cfgrib')\n",
    "test_\n",
    "\n",
    "global vars_downloaded\n",
    "vars_downloaded = ['tcw','t2m', 'd2m'] #total water content, 2m temperature, dewpoint temp 2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22fed9b-bc5f-4db4-a75c-4028c9fc1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = other_var_file_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29a1bf-f11a-41ba-b4bb-244dbe4bcbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resave_regrid_merge_multiprocessing_other_vars(file):\n",
    "# for file in other_var_file_list:\n",
    "    for region_name in ['CONUS', 'australia', 'china']:\n",
    "        for var in vars_downloaded:\n",
    "            # break\n",
    "            template, lon, lat, gefs_dates = return_init_info(region_name)\n",
    "            \n",
    "            # file = control_[0]\n",
    "            len_of_lead = 45 #this is specific to ECMWF, i downloaded 45 days\n",
    "            \n",
    "            #Test \n",
    "            # file = control_[-30]\n",
    "            #file comes from the control_ file list\n",
    "            control_,perturbed_,date_list, save_dir = return_var_info(var_full,region_name)\n",
    "            \n",
    "            # break\n",
    "            save_name = f\"{save_dir}/{file.split('/')[-1]}\"\n",
    "            save_name = file.split('/')[-1].split('_control')\n",
    "            date = f\"{save_name[0].split('_')[-1]}\"\n",
    "            save_name = f'{save_dir}/{var}_{date}.nc'\n",
    "            \n",
    "            if os.path.exists(save_name):\n",
    "                pass\n",
    "            else:\n",
    "                \n",
    "                fill_this_file = template.copy(deep = True)\n",
    "                #First open the control file \n",
    "                ctl = xr.open_dataset(file,engine='cfgrib').astype(np.float32) \n",
    "\n",
    "                # find_date = file.split('/')[-1].split('_')[2]\n",
    "                perturbed_date =[i for i in perturbed_ if date in i]\n",
    "                \n",
    "                if len(perturbed_date) == 1:\n",
    "        \n",
    "                    process_perturbed = xr.open_dataset(perturbed_date[0],engine='cfgrib')\n",
    "        \n",
    "                    # #Check perturbed file \n",
    "                    # ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "                    # process_perturbed.isel(step=0).isel(number=1)['sm100'].plot.pcolormesh(ax=ax, vmin=0, vmax=1)\n",
    "                    # process_perturbed.isel(step=0)['sm100'].plot.pcolormesh(ax=ax, vmin=0, vmax=1)\n",
    "                    # ax.coastlines()\n",
    "                    \n",
    "                    #keep all longitude values that are less than 90 (it's a weird way of doing it but our current mask for other data GEFSv12 is like this)\n",
    "                    if region_name == 'CONUS':\n",
    "                        new_lon_values =  [i if i < 90 else i-360 for i in ctl.longitude.values]\n",
    "                        high_res_grid = xr.open_dataset('masks/NH_grid.nc') #I manually created this. Didn't know how to structure it until I ran the code once then came back and added it.\n",
    "                        # high_res_grid = xr.open_dataset('masks/RZSM_weighted_mean_0_100cm.nc4')\n",
    "                        high_res_grid.close()\n",
    "                        high_res_grid = high_res_grid.rename({'X':'longitude','Y':'latitude'})\n",
    "                        # high_res_grid\n",
    "                    elif region_name == 'australia':\n",
    "                        new_lon_values = ctl.longitude.values\n",
    "                        high_res_grid = xr.open_dataset('masks/australia_mask.nc4')\n",
    "                    elif region_name == 'china':\n",
    "                        new_lon_values = ctl.longitude.values\n",
    "                        high_res_grid = xr.open_dataset('masks/china_mask.nc4')\n",
    "                    \n",
    "                    ctl = ctl.assign_coords({'longitude':new_lon_values})\n",
    "                    process_perturbed = process_perturbed.assign_coords({'longitude':new_lon_values})\n",
    "        \n",
    "                    if region_name == 'CONUS':\n",
    "                        ds_out = xr.Dataset(\n",
    "                            {\n",
    "                                \"latitude\": ([\"latitude\"], np.arange(template.Y.values[0], template.Y.values[-1]-0.5, -0.5), {\"units\": \"degrees_north\"}),\n",
    "                                \"longitude\": ([\"longitude\"], np.arange(template.X.values[0], template.X.values[-1]+0.5, 0.5), {\"units\": \"degrees_east\"}),\n",
    "                            }\n",
    "                        )\n",
    "                    elif region_name == 'australia':\n",
    "                        ds_out = xr.Dataset(\n",
    "                            {\n",
    "                                \"latitude\": ([\"latitude\"], np.arange(template.Y.values[0], template.Y.values[-1]-0.5, -0.5), {\"units\": \"degrees_north\"}),\n",
    "                                \"longitude\": ([\"longitude\"], np.arange(template.X.values[0], template.X.values[-1]+0.5, 0.5), {\"units\": \"degrees_east\"}),\n",
    "                            }\n",
    "                        )\n",
    "                    elif region_name == 'china':\n",
    "                        ds_out = xr.Dataset(\n",
    "                            {\n",
    "                                \"latitude\": ([\"latitude\"], np.arange(template.Y.values[0], template.Y.values[-1]-0.5, -0.5), {\"units\": \"degrees_north\"}),\n",
    "                                \"longitude\": ([\"longitude\"], np.arange(template.X.values[0], template.X.values[-1]+0.5, 0.5), {\"units\": \"degrees_east\"}),\n",
    "                            }\n",
    "                        )               \n",
    "                    ds_out\n",
    "        \n",
    "\n",
    "                    #Cannot do bilinear, it produces no values along the coasts of the data for RZSM. Conservative works better\n",
    "                    # regridder = xe.Regridder(ctl, high_res_grid, \"patch\")\n",
    "                    regridder = xe.Regridder(ctl, high_res_grid, \"bilinear\")\n",
    "                    # conservative is the only way that we can get smooth edges around coasts\n",
    "                    # regridder = xe.Regridder(ctl, high_res_grid, \"conservative\")\n",
    "                    \n",
    "                    ctl_out = regridder(ctl, keep_attrs=True)\n",
    "                    perturbed_out = regridder(process_perturbed, keep_attrs=True)\n",
    "        \n",
    "                    # #Check perturbed file \n",
    "                    # ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "                    # perturbed_out.isel(step=0).isel(number=1)[var].plot.pcolormesh(ax=ax, vmin=0, vmax=perturbed_out[var].max().values)\n",
    "                    # perturbed_out.isel(step=0)['sm100'].plot.pcolormesh(ax=ax, vmin=0, vmax=1)\n",
    "                    # ax.coastlines()\n",
    "        \n",
    "                    #Now replace the template data with control and perturbed\n",
    "                    fill_this_file = fill_this_file.assign_coords({'S':np.atleast_1d(pd.to_datetime(date))}).reindex(L= np.arange(len(ctl_out.step.values)), fill_value=np.nan) \n",
    "                    \n",
    "                    \n",
    "                    fill_this_file[xarray_varname(fill_this_file)][:,0,0:len(ctl_out.step.values),:,:] = ctl_out[xarray_varname(ctl_out)].values\n",
    "                    name_old = xarray_varname(fill_this_file)\n",
    "                    name_new = var\n",
    "                    fill_this_file =fill_this_file.rename({name_old:name_new})\n",
    "                    fill_this_file[xarray_varname(fill_this_file)][:,1:,0:len(perturbed_out.step.values),:,:] = perturbed_out[xarray_varname(perturbed_out)].values\n",
    "        \n",
    "                    #Now add a mask for the values that are a \"1\" which are the ocean/water bodies\n",
    "                    fill_this_file = xr.where(np.isnan(fill_this_file), 1, fill_this_file)\n",
    "        \n",
    "                    #Check final file \n",
    "                    # ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "                    # fill_this_file.isel(L=0).isel(M=1).isel(S=0)[var].plot.pcolormesh(ax=ax, vmin=0, vmax=perturbed_out[var].max().values)\n",
    "                    # ax.coastlines()\n",
    "                    #Save file\n",
    "                    fill_this_file.to_netcdf(save_name)\n",
    "        \n",
    "                    #Now regrid using cdo\n",
    "                \n",
    "                else:\n",
    "                    print(f'Could not find a perturbed file for {find_date}')\n",
    "                    pass\n",
    "        \n",
    "            \n",
    "                \n",
    "        \n",
    "\n",
    "#Then save as a netcdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16302dac-972c-42e9-8a73-770b0487f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_var_file_list.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07abbe1f-3e86-4075-ae31-60ea0bb0869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for region in ['CONUS','australia','china']:\n",
    "        p=Pool(20)\n",
    "        p.map(resave_regrid_merge_multiprocessing_other_vars, other_var_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670034d-7cd1-4ecc-a5b0-350bf6108363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf212gpu]",
   "language": "python",
   "name": "conda-env-tf212gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
